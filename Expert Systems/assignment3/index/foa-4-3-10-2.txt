

FOA: 4.3.10.2 Operating characteristic curves





FOA Home

 | UP: Other measures



Operating characteristic curves

Swets [Swets63] enumerated a number
of abstract desiderata (and quoted by \vanR{155}) that we might wish for
any assessment measure. According to these, IR's standard Re/Pre plot
leaves much to be desired, in particular because this two-dimensional
assessment makes direct comparision impossible. Swets therefore
recommends an analysis from the perspective of signal detection, based
on several key assumptions: \item[(A1)] There is a ``relevant'' signal
we wish to distinguish from background noise. We can consider the worst
case to be comparison against an ``irrelevant'' signal, with both
signals imposed over the data collection. We can imagine that this
signal is generated by the presence or absence of some keywords;
\item[(A2)] These two signals are to be discriminated according to only
a single dimension; and \item[(A3)] These signals are both distributed
{\em normally} across the corpus.


 In this idealized case, we get a
picture something like Figure (figure) . Then, since our corpus
has been ordered by the ranking, the goal becomes to select a value
${\bf \mathname{Rank}}_\tau$ that best separates these two modal
distributions.


Using a simple retrieval rule that retrieves a document
just in case its value is above the threshold ${\mathname{Rank}}_\tau$,
wherever we place this threshold we are bound to make two types of
errors. There will be some \Rel documents which fall below our threshold
(cross-hatched in \blue in Figure (FOAref) ) and some
irrelevant documents which fall above it (cross-hatched in \red).
Following signal detection theory we can call the first set ``FALSE-''
errors, and the second ``FALSE+'' errors. (These are often called Type 1
and Type 2 errors, resp.) Note that the ratio of the right tail of the
\Rel curve (that area {\em not} cross-hatched in \blue in Figure
(FOAref) ) to the total area under the \Rel curve corresponds
exactly to the Recall measure defined earlier (Equation
(FOAref) ), while the ratio of the right tail of the NRel curve
(\red cross-hatched in Figure (FOAref) ) to the total area
under the NRel curve corresponds exactly to Fallout (Equation
(FOAref) ).


The paremetric curve defined by the percentage of
\mathname{Rel} \) versus \( \overline{\mathname{NRel}} \) documents
retrieved as \( \tau \) is varied is called the OPERATING
CHARACTERISTIC CURVE . Obviously, if these two distributions are
identical, this curve will be exactly a diagonal line, from (0,0) to
(1,1). If the mean value of the \( \mathname{Rel} \) distribution is
greater than that of the \( \overline{\mathname{NRel}} \), the OC curve
is moved closer to the upper-left corner, as shown in Figure
(figure) .


While Swets (and subsequently others [<A
HREF="bibrefs.html#Robertson69">Robertson69] [<A
HREF="bibrefs.html#Bookstein77">Bookstein77] ) then considered
fairly elaborate tests to discriminate the relative performance of
retrieval systems with respect to such curves, it is fair to say that
\vanR{154} assessment of 1979 still stands: ``...although Swets' model
is theoretically attractive and links IR measurements to readymade and
well-developed statistical theory, it has not found general acceptance
among workers in the field.'' Optimal selection of ${\bf
\mathname{Rank}}_\tau$ depends on specification of the COSTS
(losses) of making FALSE+ or FALSE- errors. For example, if you are an
over-worked and underpaid law clerk and you read an irrelevant document
(FALSE+) you've wasted precision attention, but that's all; if you miss
a reference you should have found (FALSE-) the cost might be huge. But
if you're a partying undergraduate with one more term paper between you
and summer vacation, your assessments might be quite different. Section
&sect;5.5.6 gives an example of how
explicit models of these various costs can be incorporated within a
Bayesian decision-making framework.




Top of Page

 | UP: Other measures

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21



