

FOA: 1.1.1 Working within the IR Tradition





FOA Home

 | UP: Finding Out About - a cognitive activity



Working within the IR Tradition

If it seems to you that the last section has side-stepped many of the
most difficult issues underlying FOA, you're right! Later chapters will
return to redress some of these omissions, but the immediate goal of
Chapters 2-4 is to ``operationalize'' FOA to resemble a well-studied
problem within computer science, typically referred to as INFORMATION
RETRIEVAL (IR). IR is a field that has existed since computers were
first used to count words [Belkin87]
. Even earlier, the related discipline of Library Science had developed
many automated techniques for efficiently storing, cataloging and
retrieving the {\em physical} materials so that browsing patrons could
find them; many of these methods can be applied to the digital documents
held within computers. IR has also borrowed heavily from the field of
linguistics, especially computational linguistics.


The primary journals
in the field and most important conferences {Information Processing \&
Management, the ACM's Transactions on Information Systems and the
Journal of the American Society for Information Science (JASIS) are some
of the central journals; meetings of the American Society for
Information Science, the ACM's Special Interest Group in Information
Retrieval (SIGIR), and the Symposium on Document Analysis and
Information Retrieval (DAIR) are the most important meetings, producing
consistently valuable proceedings.} in IR have continued to publish and
meet since the 1960s, but the field has taken on new momentum within the
last decade. Computers capable of searching and retrieving from the
entire biomedical literature, across an entire nation's judicial system,
or from all of the major newspaper and magazine articles, have created
new markets among doctors, lawyers, journalists, students, ... everyone!
And of course, the Internet, within just a few years, has generated
many, many other examples of textual collections and people interested
in searching through them.


The long tradition of IR is therefore the
primary tradition from which we will approach FOA. Of course, every
tradition also brings with it tacit assumptions and preconceived notions
that can hinder progress. In some ways, an elementary school student
using the Internet to FOA class materials is related to the original
problem considered by library science and IR, but in many ways it
couldn't be more different (cf. Section <A
HREF="foa-8-1.html">&sect;8.1 ). In this text, ``FOA'' will be used
to refer to the broadest characterization of the cognitive process and
``IR'' to this subdiscipline of computer science and its traditional
techniques. When we talk of the ``search engine,'' this is not meant to
refer to any particular implementation, but to an idealized system most
typical of the many different generations and varieties of actual search
engines now in use. If you are using this text as part of a course, you
may build one simple example of a search engine.


Using Figure
(figure) as a guide, we'll return to each of the three phases
above and be a bit more specific about each component of our search
engine. Here, finally, the human question-answerer has been replaced by
an algorithm, the search engine, that will attempt to accomplish the
same purpose. A second thing this figure makes clear is that the
fundamental operation performed by a search engine is a {\em match},
between descriptive features mentioned by users in their queries, and
documents sharing those same features. By far the most important kind of
features are keywords.




Top of Page

 | UP: Finding Out About - a cognitive activity

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.1 Finding Out About - a cognitive activity





FOA Home

 | UP: Overview



Finding Out About - a cognitive activity

We are all forced to make decisions regularly, sometimes at the spur of
the moment. But often we have enough warning that it becomes possible to
collect our thoughts and even do some research that makes our decision
as sound as it can be. This book is a closer look at the process of
FINDING OUT ABOUT (FOA), research activities that allow a
decision-maker to draw on others' knowledge. It is written from a
technical perspective, in terms of computational tools that speed the
FOA activity in the modern era of the distributed networks of knowledge
collectively known as the World Wide Web (WWW). It shows you how to
build many of the tools that are useful for searching collections of
text and other media. The primary argument advanced is that progress
requires that we appreciate the cognitive foundation we bring
to this task as academics, as language users, and even as adaptive
organisms.


As organisms, we have evolved a wide range of strategies for
seeking useful information about our environment. We use the term
``cognitive'' to highlight the use of internal representations
that help even the simplest organisms perceive and respond to their
world; as the organisms get less simple, their cognitive structures
increase in complexity. Whether done by simple or complex organisms,
however, the process of {\em finding out about} (FOA) is a very active
one - making initial guesses about good paths, using complex sets of
features to decide if we seem to be on the right path, and proceeding
forward.


As humans, we are especially expert at searching through one of
the most complex environments of all - {Language}. Its system of
linguistic features is not derived from the natural world, at least not
directly. It is a constructed, cultural system that has worked well
since (by definition!) prehistoric times. In part languages remain
useful because they are capable of change when necessary. New features
and new objects are noticed, and it becomes necessary for us to express
new things about them, form our reactions to them, and express
these reactions to one another.


Our first experience of language, as
children and as a species, was oral - we spoke and listened. As children
we learn { Sprachspiele}(word or language games) [<A
HREF="bibrefs.html#REF640">REF640] - how to use language to get what
we want. A baby saying ``Juice!'' is using the exclamation as a
tool to make adults move; that's what a word \means. Such a
functional notion of language, in terms of the jobs it accomplishes,
will prove central to our conception of what keywords in documents and
queries mean as part of the FOA task.


Beyond the oral uses of
language, as a species we have also learned the advantages of {writing
down} important facts which we might otherwise forget. Writing down a
list of things to do that you might forget tomorrow extends our limited
memory. Some of these advantages accrue to even a single individual: We
use language personally, to organize our thoughts and to conceive
strategies.


Even more important, we use writing to say things to others.
Writing down important, memorable facts in a consistent, {conventional}
manner, so that others can understand what we mean and vice
versa, further amplifies the linguistic advantage. As a society, we
value reading and writing skills because they let us interpret shared
symbols and coordinate our actions. In advanced cultures scholarship,
entire curricula can be defined in terms of what Robert McHenry (editor
in chief of Encyclopedia Britannica) calls <A
HREF="http://www.howtoknow.com/htktitle.html">``Knowing How To Know''
.


It is easiest to think of the organism's or human's search as being for
a valuable object, sweet pieces of fruit in the jungle, or (in modern
times!) a grocer that sells it. But as language has played an
increasingly important role in our society, searching for valuable
written passages becomes an end unto itself. Especially as members of
the academic community, we are likely to go to libraries seeking others'
writings as part of our search. Here we find rows upon rows of books,
each full of facts the author thought important, and endorsed by a
librarian who has selected it. The authors are typically people far from
our own time and place, using language similar but not identical to our
own.


And the library contains books on many, many topics. We must Find
Out About a topic of special interest, looking only for those things
that are RELEVANT to our search. This basic skill is a
fundamental part of the academic's job: \item we look for references in
order to write a term paper; \item we read a textbook, looking for help
in answering an exercise; \item we comb the scientific journals to see
if a question has already been answered. We know that if we find the
right reference, the right paper, the right paragraph, ..., our job will
be made much easier. Language has become not only the means of our
search, but its object as well.


 Today we can also search the WORLD
WIDE WEB (WWW) for others' opinions of music, movies, or software.
Of course these examples are much less of an ``academic exercise'' -
Finding Out About these information commodities, and doing it
consistently and well, is a skill that the modern information society
values. But while the infra-structure forming the modern WWW is quite
recent, the promise offered by truly connecting all the world's
knowledge has been anticipated for some time, for example by H. G. Wells
[Wells38] .


Many of the FOA searching
techniques we will discuss in this text have been designed to operate on
vast collections of apparently ``dead'' linguistic objects: files full
of old email messages, CDROMs full of manuals or literature, Web servers
full of technical reports, etc. But at their core each of these
collections is evidence of real, vital attempts to communicate.
Typically an AUTHOR (explicitly or implicitly) anticipates the
interests of some imagined AUDIENCE and produces text that is a
balance between what the author wants to say and what they think the
audience wants to hear. A textual CORPUS will contain many such
documents, written by many different authors, in many styles and for
many different purposes. A person searching through such a corpus comes
with their own purposes, and may well use language in a different way
than any of the authors. But each of the individual linguistic
expressions -- the authors' attempts to write, the searchers' attempts
to express their questions and then read the authors' documents -- must
be appreciated for the WORD GAMES [<A
HREF="bibrefs.html#REF640">REF640] that they are. FOA is centrally
concerned with \rikmeaning: the semantics of the words, sentences,
questions and documents involved. We cannot tell if a document is
about a topic unless we understand (at least something of) the
semantics of the document and the topic. This is the notion of
\about-ness most typical within the tradition of library science [<A
HREF="bibrefs.html#Hutchins78">Hutchins78] .


This means that our
attempts to engineer good technical solutions must be informed by, and
can contribute to, a broader philosophy of language. For example, it
will turn out that FOA's concern with the semantics of entire documents
is well-complemented by techniques from computational linguistics which
have tended to focus on syntactic analysis of individual sentences. But
even more exciting is the fact that the recent availability of new types
of ELECTRONIC ARTIFACTS -- from email messages and WWW corpora to
the browsing behaviors of millions of users all trying to FOA -- brings
an {\em empirical} grounding for new theories of language that may well
be revolutionary.


At its core, the FOA process of browsing readers can be
imagined to involve three phases: \item Asking a question; \item
Constructing an answer; \item Assessing the answer.


 This conversational
loop is sketched in Figure (figure) .


*{Step1. Asking a
Question}


The first step is initiated by people who (anticipating our
interest in building a search engine) we'll call ``users,'' and their
questions. We don't know a lot about these people, but we do know
they are in a particular frame of mind, a special cognitive state - they
may be aware knowing what it is that you don't know. Any use of the term
``awareness'' in this context, therefore, should be interpreted
loosely.} of a specific gap in their knowledge (or they be only vaguely
puzzled), and they're motivated to fill it. They want to FOA ... some
topic.


Supposing for a moment that we were there to ask, the users may
not even be able to characterize the topic, i.e., their knowledge gap.
More precisely, they may not be able to fully define characteristics of
the ``answer'' they seek. A paradoxical feature of the FOA problem is
that if users knew their question, precisely, they might not even need
the search engine we are designing - forming a clearly posed question is
often the hardest part of answering it! In any case, we'll call this
somewhat befuddled but not uncommon cognitive state the users'
INFORMATION NEED .


While a bit confused about their particular
question, the users are not without resources. First, they can typically
take their ill-defined, {internal} cognitive state and turn it into an
{\em external} expression of their question, in some language. We'll
call their expression the QUERY , and the language from which it
is constructed the QUERY LANGUAGE .


*{Step2. Constructing an
Answer}


So much for the source of the question; whence the answer? If the
question is being asked of a person, we must worry about equally complex
characteristics of the answerer's cognitive state:


 \item Can
they translate the user's ill-formed question into a better one? \item
Do they know the answer themselves? \item Are they able to verbalize
this answer? \item Can they give the answer in terms that the user will
understand? \item Can they provide the necessary background knowledge
for the user to understand the answer itself?


 We will refer to the
question-answerer as the SEARCH ENGINE , a computer program that
algorithmically performs this task. Immediately each of the concerns
(just listed) regarding the {\em human} answerer's cognitive state
translate into extremely ambitious demands we might make of our {\em
computer} system.


Throughout most of this book, we will avoid such
ambitious issues and instead consider a very restricted form of the FOA
problem: We will assume that the search engine has available to it only
a set of pre-existing, ``canned'' passages of text, and that its
response is limited to identifying one or more of these passages and
presenting them to the users; see Figure (figure) . We will
call each of these passages a DOCUMENT , and the entire set of
documents the CORPUS . Especially when the corpus is very large
(e.g., assume it contains millions or even billions of documents),
selecting a very small set (say 10-20) of these as potentially good
answers to be RETRIEVED will prove sufficiently difficult (and
practically important) that we will focus on it for the first few
chapters of this book. In the final chapter consider how this basic
functionality can be extended towards tools for ``Searching for an
education'' (cf. Section &sect;8.3.4 ).


*{Step3.
Assessing the answer}


Imagine a special instance of the FOA problem: You
are the user, waiting in line to ask a question of a professor. You're
confused about a topic that is sure to be on the final exam. When you
finally get your chance to ask your question, we'll assume that the
professor does nothing but select the three or four preformed pearls of
wisdom he or she thinks comes closest to your need, delivers these
``documents,'' and sends you on your way. ``But wait!'', you want to
say. ``That isn't what I Or, ``Let me ask it another way.'' Or, ``That
helps, but I still have this problem.''


The third and equally important
phase of the FOA process is a ``closing of the loop'' between asker and
answerer, whereby the user(asker) provides an assessment of just how
RELEVANT they find the answer provided. If after your first
question and the professor's initial answer you are summarily ushered
out of the office, you have a perfect right to be angry because the FOA
process has been violated. FOA is a dialog between
question-asker and -answerer; it does not end with the search engine's
first delivery of an answer. This initial exchange is only the first
iteration of an ongoing conversation by which asker and answerer
mutually negotiate a satisfactory exchange. In the process, the asker
may {\em recognize} elements of the answer they seek and be able to
re-express their information need in terms of threads taken from
previous answers.


Since the question-answerer has been restricted to a
simple set of documents, the asker's RELEVANCE FEEDBACK must be
similarly constrained - for each of the documents retrieved by the
search engine, the asker reacts by saying whether they find the document
relevant or not. Returning to the student/professor scenario, we can
imagine this as the student saying ``Thanks, that helps.'' after those
pearls that do, and remaining silent, or saying ``Huh?'' or ``What does
that have to do with anything?!'' or ``No, that's not what I meant!''
otherwise. More precisely, relevance feedback gives the asker the
opportunity to provide more information with their reaction to each
retrieved document - whether it is relevant (\Pos), irrelevant (\Neg),
or neutral (\DCare). This is shown as a Venn diagram-like labeling of
the set of retrieved documents in (figure) . We'll worry about
just how to solicit and make use of relevance feedback judgements in
Chapter 3.

Subsections

	 1.1.1 Working within the IR Tradition




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.2.1 Elements of the query language





FOA Home

 | UP: Keywords



Elements of the query language

If the query comes from a student during office hours, or from a patron
at a reference librarian's desk, the QUERY LANGUAGE they'll use
to frame their question is all of NATURAL LANGUAGE, that most
expressive ``mother tongue'' familiar to both question-asker and
-answerer. But for the software search engines we will consider, we must
assume a much more constrained, ``artificial'' query language. Like
other languages, ours will have both a meaningful VOCABULARY --
the set of important keywords any users are allowed to mention in any of
their queries -- and a {\em syntax} that allows us to construct more
elaborate query structures.




Top of Page

 | UP: Keywords

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.2.2 Topical scope





FOA Home

 | UP: Keywords



Topical scope

The first constraint we can apply to the set of keywords we will allow
in our vocabulary is to define a DOMAIN OF DISCOURSE - the
subject area within which each and every user of our search engine is
assumed to be searching. While we might imagine building a truly
encyclopedic reference work, one capable of answering questions
about any topic whatsoever, it is much more common to build a
search engine with more limited goals, capable of answering questions
about some particular subject. We will choose the simpler path
(it will prove enough of a challenge!), and focus on a particular topic.
To be concrete, throughout this text we will assume that the domain of
discourse is {\tt ARTIFICIAL INTELLIGENCE} (AI). Briefly, AI can be
defined as a sub-discipline of computer science, especially concerned
with algorithms that mimic inferences which, had they been made by a
human, would be considered ``intelligent.'' It typically includes such
topics as {\tt KNOWLEDGE REPRESENTATION, MACHINE LEARNING, ROBOTICS},
etc.


Thus is a BROADER TERM than \term{ARTIFICIAL INTELLIGENCE}.
This HYPERNYM relationship between the two phrases is something
we will return to later (cf. Section <A
HREF="foa-6-3.html">&sect;6.3 ). For example, our task becomes more
difficult if we assume that the corpus of documents contains materials
on the broader topic of \term{COMPUTER SCIENCE}, rather than just (!)
\term{ARTIFICIAL INTELLIGENCE}. Conversely, the topics \term{KNOWLEDGE
REPRESENTATION}, \term{MACHINE LEARNING}, \term{ROBOTICS} are all
NARROWER TERMS , and our task would, {\em caeteris
paribus\/}\footnote{(Assuming) all other things being equal.}, be made
easier if we only had to help users FOA one of them.


Constraining the
vocabulary so that it is EXHAUSTIVE enough that any imaginable
topic is expressible within the language, while remaining
SPECIFIC enough that any particular subjects a user is likely to
investigate can be distinguished from others, will become a central goal
of our design. \term{ROBOTICS}, for example, would seem a descriptive
keyword because it identifies a relatively small sub-area of
\term{ARTIFICIAL INTELLIGENCE}. \term{COMPUTER SCIENCE} would be silly
as a keyword (for this corpus), as we are assuming it would apply to
each and every document and hence does nothing to discriminate them - it
is too exhaustive. At the other extreme, \term{ROBOTIC VACUUM CLEANERS
FOR 747 AIRLINERS} is almost certainly too specific.


The VOCABULARY
SIZE -- the total number of keywords -- depends on many factors,
including the scope of the domain of discourse. A typical language user
has a reading vocabulary of approximately 50,000 words. Web search
engines and large test corpora formed from the union of many document
types may require vocabularies ten times this large. It is unlikely that
such a large lexicon of keywords is required for restricted corpora, but
it is also true that even a narrow field can develop an extensive,
specialized JARGON or TERMS OF ART. In practice, search
engines typically have difficulty reducing the number of usable keywords
much below 10,000.




Top of Page

 | UP: Keywords

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.2.3 Document descriptors





FOA Home

 | UP: Keywords



Document descriptors

We've introduced keywords as features mentioned by users as part of
their queries, but the other face of keywords is as descriptive features
of documents. That is, we might naturally say that a document is
about \term{ROBOTICS}. Users mentioning \term{ROBOTICS} in their
query should expect to get those documents that are about this
topic. Keywords must therefore also function as the {\em documents}'
description language. The same vocabulary of words used in queries must
be used to describe the topical content of each and every document.
Keywords become our characterization of what each document is \about.
INDEXING is the process of associating one or more keywords with
each document.


The vocabulary used can either be CONTROLLED or
UNCONTROLLED (a.k.a., CLOSED- or OPEN-VOCABULARIES
). Suppose we decide to have all the documents in our corpus manually
indexed by their authors; this is quite common in many conference
proceedings, for example. If we provide a list of potential keywords and
tell authors they must restrict their choices to terms on this list, we
are using a controlled indexing vocabulary. On the other hand, if we
allow the authors to assign any terms they choose, the resulting index
has an uncontrolled vocabulary [<A
HREF="bibrefs.html#Svenonius86">Svenonius86] .


To get a feel for the
indexing process, imagine that you are given a piece of text and must
come up with a set of keywords that describe what the document is Let's
make the exercise more concrete. You are the author of a report entitled
``Using a neural network for prediction,'' and are submitting it to a
journal. One of the things this particular journal requires is that the
author provide up to six keywords under which this article is going to
be indexed. If you are sending it to the {\em Communications of the
ACM}, you might pick a set of keywords that identify, to the audience of
computer scientists you think read this publication, connections between
this new work and others' prior work in related areas.


\term{NONLINEAR
REGRESSION; TIME SERIES PREDICTION} \smallskip


But now imagine instead
that you've decided to submit the {exact same paper} to {\em Byte
Magazine}, but must again pick keywords that have meaning to this
audience. You might choose:


\term{NEURAL NETWORKS; STOCK MARKET ANALYSIS}
\smallskip


What is the {context} in which these keywords are going to be
interpreted? Who's the audience? Who's going to understand what these
keywords \mean? Anticipating the FOA activity in which these keywords
will function, we know that the real issue to be solved is not only to
describe this one document, but to {\em distinguish} it from the
millions of others in the same corpus. How are the keywords chosen going
to be used to distinguish your document from others?


It is often easiest
to imagine keywords as independent features of each document. In fact,
however, keywords are best viewed as a { relation} between a document
and its prospective readers, sensitive to both characteristics of the
users' queries and other documents in the same corpus. In other words,
the keywords you pick for Byte should be different than those
you pick for {\em Communications of the ACM} and for deeper reasons than
what we might cynically consider ``spin control.''




Top of Page

 | UP: Keywords

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.2 Keywords





FOA Home

 | UP: Overview



Keywords

KEYWORDS are linguistic atoms -- typically words, pieces of words
or phrases -- used to characterize the subject or content of a document.
They are pivotal because they must bridge the gap between the users'
characterization of information need (i.e., their queries), and the
characterization of the documents' topical focus against which these
will be matched. We could therefore begin to describe them from either
perspective - how they are used by users, or how they become associated
with documents. We will begin with the former.

Subsections

	 1.2.1 Elements of the query language
	 1.2.2 Topical scope
	 1.2.3 Document descriptors




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.3.1 Query sessions





FOA Home

 | UP: Query syntax



Query sessions

As we consider the specific features of each query, it is important to
remember the role these short expressions play in the larger FOA
process. Queries are generated as an attempt by users to express their
information need. As with any linguistic expression, conveying a thought
you have can be difficult and this is likely to be especially true of
the muddled cognitive state of our FOA searcher. Users who are familiar
with the special syntactic features of a query language may be able to
express their need more easily, but others for whom this unnatural
syntax is new or difficult will have additional difficulties.


As with
many of the idealizing assumptions we are at least temporarily making,
it is often simpler to think about only one iteration of the three-step
query/retrieve/assess FOA process at a time. In most realistic
situations we can expect that single queries will not occur in isolation
but as part of an iteration of the FOA process. An initial query begins
the dialog; the search engine's response provides clues to the user
about directions to pursue next; these are expressed as another query.
An abstract view of this sequence is presented in Figure
(figure) . Note especially the concatenation of a series of
basic FOA three-step iterations. Data is produced by user, then by
search engine and then by user - constructs a very natural alternation
of user/search engine exchanges. Users' assessments can also function as
their next query statement. This can be achieved simply if we have some
method for automatically constructing a query from relevance feedback.
For example, if users click on documents they like, the search engine
can, by itself, form a new query that focuses on those keywords that are
especially associated with these documents.


While there are many such
techniques for using relevance feedback from a single query/retrieval,
there are many more things that we can learn from the entire query
session. The full query session provides more complete evidence about
the users' information need than we can gain from any one query. In
fact, as will be discussed extensively in Chapter <A
HREF="foa-7.html">&sect;7 , there exist algorithmic means by which
the search engine itself might ``learn'' from such evidence. Learning
methods might even be expected to make TRANSITIVE leaps, from the
users' initial expressions of their information needs, to those final
documents that satisfied them. (Of course, this transitive leap is only
warranted if we are certain that they ended the session satisfied, and
aren't just quitting in frustration!) For all these reasons, we must
make efforts to identify a query session's boundaries, i.e., when one
focused search session ends and the next session, involving the same
user but searching on an entirely different topic, begins.




Top of Page

 | UP: Query syntax

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.3 Query syntax





FOA Home

 | UP: Overview



Query syntax

Keywords therefore have a special status in IR and as part of the FOA
process. Not only must they be exhaustive enough to capture the entire
topical scope reflected by the corpora's domain of discourse, but they
must also be expressive enough to characterize any information needs the
users might have.


Of course we need not restrict our users to only one of
these keywords. It seems quite natural for queries to be composed of two
or three, perhaps even dozens, of keywords. Recent empirical evidence
suggests that many typical queries have only two or three keywords (cf.
Section &sect;8.1 ), but even this number
provides a great combinatorial extension to the basic vocabulary of
single keywords. Other applications, for example using a document itself
as a query, (i.e. using it as an example - ``Give me more like this.'')
can generate queries with hundreds of keywords. Regardless of size,
queries defined only as sets of keywords will be called SIMPLE
QUERIES . Many Web search engines support only simple queries. Often
however, the search engines also provide more advanced interfaces
including OPERATORS in the query language as well. Perhaps,
because you have previously been warped by an exposure to computer
science:), you may be thinking that sets of keywords might be especially
useful if joined by Boolean operators. For example, if we have one set
of documents about \term{NEURAL NETWORKS} and another set of
documents about \term{SPEECH RECOGNITION}, we can expect the
query: \smallskip \term{NEURAL NETWORKS AND SPEECH RECOGNITION}
\smallskip to correspond to the intersection of these two sets, while
\smallskip \term{NEURAL NETWORKS OR SPEECH RECOGNITION} \smallskip would
correspond to their union.


The Boolean operator is a bit more of a
problem. If users say they want things that are {\em not} about
\term{NEURAL NETWORKS}, they are in fact referring to the vast majority
of the corpus. That is, \term{NOT} is more appropriately considered a
binary, subtraction operator. To make this distinction explicit we will
call it \term{BUT\_NOT} .


There are other syntactic operators that are
often included in a search engine's query language, but these will be
put off until later. Even with these simple Boolean connectives and a
keyword vocabulary of reasonable size, users are capable of constructing
a vast number of potential queries in attempting to express their
information need.

Subsections

	 1.3.1 Query sessions




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.4.1 Structured aspects of documents





FOA Home

 | UP: Documents



Structured aspects of documents

In addition to their free text, many documents will also carry
META-DATA that gives some facts about the document. We may have
PUBLICATION INFORMATION , e.g., that this document appeared in
this journal on this day on this page. We are likely to know the
author(s) of the document. Queries will often refer to aspects of both
free-text and meta-data:


 I'm interested in documents about Fools'
Gold that have been published in children's magazines in the last five
years.


 The first portion of this query depends on the same relation that
is at the core of our FOA process. But the last two criteria, concerning
publication type and date, seem to be just the sort of query against
structured attributes that database systems perform very successfully.
In most real-life applications a hybrid of database and IR technologies
will be necessary. (We distinguish between these techniques in Section
&sect;1.6 .)


The most interesting examples
concern characteristics that do not clearly fall into either IR or
database categories. For example, can you define precisely what you
mean by a `children's magazine', in terms of unambiguous
attributes on which a database would depend? Consider another query:



What sort of work has K. E. Smith done on metabolic proteins affecting
neurogenesis?


 Finding an exact match for the string in the
\term{AUTHORS} attribute is straightforward. But the conventions in much
of medical and biological publication (as well as in some areas of
physics) sometimes lead to dozens of authors on papers, from the
director of the institute through all of the laboratory assistants.
While K. E. Smith might well fulfill the syntactic requirements of
authorship on a particular paper, users searching for ``the work of''
this person might well have a more narrowly-defined {\em semantic}
relationship in mind.




Top of Page

 | UP: Documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.4.2 Corpora





FOA Home

 | UP: Documents



Corpora

We have focused on individual documents, but of course the FOA problem
would not interest us except that we are typically faced with a corpus
of {millions} of such documents, and interested in finding only a
handful of interest. The actual number of documents and their cumulative
size will matter a great deal, as some of our IR methods have time or
space complexities that make them viable only within certain parameters.
To pick a simple example, if you are trying to find a newspaper article
(you read it a few days ago) for a friend, exhaustively searching
through all the pages is probably quite effective if you know it was in
today's paper, but not if you need to search through an entire month's
recycling pile! Similarly, a standard utility like the Unix grep command
can be a very practical alternative if the corpus is small and the
queries simple.




Top of Page

 | UP: Documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.4.3 Document proxies





FOA Home

 | UP: Documents



Document proxies

Do you remember the library's original card catalogs, those wooden,
beautifully constructed cabinets full of rows and rows of drawers, each
full of carefully typed index cards? The card catalog contained
PROXIES -- abridged representations of documents, acting as their
surrogate -- for the books it indexed. No one expected the full text of
the books actually to be found in these drawers.


Computerized card
catalogs are only capable of supporting a similiar function. They do
allow more extensive indexing and efficient retrieval, from terminals
that might be accessed far from the library building. At the heart of
this system is a text search engine capable of matching features of a
query against book titles. Just like the original index cards however,
retrieval is limited to some proxy of the indexed work, a bibliographic
citation or perhaps even an abstract. The text of ultimate interest --
in a book, magazine, or journal -- remained physically quite distinct
from the search engine used to find it.


As computer storage capacities
and network communication rates have exploded, it has become
increasingly common to find retrieval systems capable of presenting the
full text of retrieved items. In the modern context, proxies extend
beyond the bibliographic citation information and subject headings we
associate with card catalogs, and include a document's title, an
article's abstract, a judicial opinion's headnote, a book's table of
contents.


The distinction between the search engine retrieving documents
and retrieving proxies remains important, however, for at least two
reasons. First, the radically changing technical capabilities of
libraries (and computers and networks more generally) can create
conceptual confusion about just what the search engine is doing. While
it has been possible for a decade or more to get the full text of
published journal articles through commercial systems such as DIALOG and
Lexis/Nexis, free access to these through your public library would have
been almost unheard of until quite recently. In fact, most libraries did
not even try to index individual articles in their periodical
collections. Changing technical capacities, changes in the application
of intellectual property laws, changes in the library's role, and
resulting changes in the publishing industry are radically altering the
traditional balance. Even when all new publication is easily available
electronically, the issue of { retrospectively} capturing previously
published books and journals remains unresolved.


Looking far into the
future and assuming no technical, economic or legal barriers to a
complete rendering of any document in our corpus, there is still an
important reason to consider document proxies. Recall that FOA is a
{process} we are attempting to support, and that retrieving sets of
documents to show users is a step that we expect to repeat many times.
Proxies are abridged, shortened versions of the documents that are
easier for browsing users to quickly scan and react to (i.e., provide
relevance feedback) than if they had to read the entire document. If a
document's title is accurate (if its abstract is well-written, if its
bibliographic citation is complete), this proxy may provide enough
information for users to decide if it seems relevant.




Top of Page

 | UP: Documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.4.4 Genre





FOA Home

 | UP: Documents



Genre

A more subtle characteristic of documents that may need to concern us is
their GENRE - the voice or style in which a document is written.
You would, um, like, be pretty darn surprised to find stuff like this in
a textbook, but not if it came to you over the phone. The genre of Email
seems to be settling somewhere between typical printed media and spoken
conversation, with special markings of sarcasm:) and expletives \#!?\%
common. Newspaper journalists are carefully trained to produce articles
consistent with what newspaper readers are expecting, and their editors
are paid to ensure that these stories maintain a consistent voice.
Scientific journal articles are written to be understood by peers in the
same field, according to standards that pertain to that community [<A
HREF="bibrefs.html#Bayermann88">Bayermann88] . An important
component of this audience-focus is the vocabulary choice an
author makes (cf. &sect;8.2.1 ); stylistic
variations and document structure may differ as well. In a field like
psychology, for example, it would be difficult to get a paper accepted
in some journals if it is not subdivided into sections like
``Hypothesis,'' ``Methodology,'' ``Subjects.'' Legal briefs are also
written in highly conventionalized forms [<A
HREF="bibrefs.html#BlueBook">BlueBook] , and legislation is drafted
to satisfy political realities [<A
HREF="bibrefs.html#Goodrich87">Goodrich87] [<A
HREF="bibrefs.html#Levi82">Levi82] [<A
HREF="bibrefs.html#Nerhot91">Nerhot91] [<A
HREF="bibrefs.html#REF126">REF126] .


In part these variations in
genre are difficult to detect because they remain consistent within any
single corpus. That is, the typical email message would jump out at you
as out of place if it appeared in your newspaper, but probably not if it
were in the Letters to the Editor page. These examples highlight how
much {context} about the corpus we bring with us whenever we read
a particular document. They also foreshadow problems Web searchers are
just beginning to appreciate, as WWW search engines include every
document to which they can crawl, intermixing their very different
contexts and writing styles. Without the orienting features of the
newspaper's masthead, the ``Letters to the Editor'' rubric, or the
purposeful selection of a tool that scans only Usenet news, the browsing
users' abilities to understand an arbitrary document is diminished.
Individual textual passages have been stripped of much of the context
that made them sensible. As more and more of us generate {\em content}
-- in new hypermedia forms as well as traditional publications -- that
more and more of us retrieve, the range of genres we will experience can
only increase, and our methods for FOA must help to represent not just
the document but contextual information as well.




Top of Page

 | UP: Documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.4.5 Beyond text





FOA Home

 | UP: Documents



Beyond text

Our definition of `documents' has hewn closely to the printed forms that
still dominate the FOA retrievals most people now do. But print media
are not the only form of answer we might reasonably seek, and we must
ensure that our methods generalize to the other media that are
increasingly part of the Net. Sound, images, movies, maps, and more are
all appearing as part of the WWW, and typically intermixed with textual
materials. We need to be able to search all of these.


One reason for
casting the central problem of this text as ``finding out about'' is
that many aspects of multimedia retrieval remain the same from this
perspective. We still have users, with information needs. We can still
reasonably use the term ``document'' to include any potential answer to
users' queries, but now expand this term to include whatever media are
available. Most centrally, we must still characterize what each document
is about in order to match it to these queries, and users can
still assess how well the search engine has done.


At the same time, many
parts of the FOA problem change as we move away from textual documents
to other media. Most important is the increased difficulty of
algorithmically extracting clues related to the documents' semantic
content from their syntactic features. The primary source of semantic
evidence used within text-based IR is the relative frequencies of
keywords in document corpora, and a major portion of this text will show
that this is a powerful set of clues indeed. We will also discuss the
role other syntactic clues (e.g., bibliographic links) associated with
texts can play in understanding what they are As we move to other media,
the important question becomes what consistent features these new media
have that we can also process to reliably infer semantic content. For
example, what can we know about an image from the distribution of
its pixel values? Do all \term{SUNSETS} share a brightness profile (dark
below a horizontal line, bright above it) that is reliable enough that
this clue can be exploited to identify just these scenes? {Takeo Kanada
of the Carnegie Mellon Vision laboratory asserts that a very simple
predicate can be used to distinguish purely {\em natural} scenes from
those containing human artifacts: Natural scenes never contain more than
a single horizontal line!} If so, can this mode of analysis be
generalized sufficiently to allow retrieval of images based on more
typical descriptors such as \term{CHILDREN FEEDING ANIMALS}?


Even if we
imagine that certain obvious, superficial aspects of some images may be
extracted our hopes must not blind us to the rich vocabulary that many
images use every day. Consider a query like and consider Figure
(figure) Would any reasonable person claim that they could
provide an exhaustive list of all the things that these
pictures ``say''? Did you include the set of Hilary's jaw? The angle of
Bill's gaze? The attitudes about divorce prevalent when the the
Doles' picture was taken, and now? The tacit commentary by New York
Times editors produced by the juxtaposition of these two photos? Note
also that this picture (and its selection for use in this text!)
occurred years before anyone had even heard of Monica Lewinsky! {This
phenomenal news event, and the enormous amount of electronic ink spent
covering it did produce an interesting data set. M. Best [<A
HREF="bibrefs.html#Best00">Best00] has used it to provide some of
the first empirical testing of interesting hypotheses concerning
cultural change often attributed to Richard Dawkins [<A
HREF="bibrefs.html#REF346">REF346] : just as biological evolution
sifts through the gene pool to find fit individuals, cultural evolution
sifts through available MEMES (paradigms, theories, hypotheses,
ideas, words, etc.) to find the most fit. But theories of {\em
biological} evolution are notoriously subtle, and the data concerning
them is much better! While it is only a beginning, Best's statistical
analysis of phenomena like the rise and fall of the token {\tt MONICA}
within newspapers and Usenet newsgroups provides some of the first
concrete data to bear on some very interesting questions.}


Figure
(figure) gives a second example. This is a photograph of a
locking display case, containing a concert performance schedule. Pasted
over the glass of the case is a sign, saying: ``IGNORE THIS CALENDAR:
These dates are 3 years old.'' But the photo also reveals a number of
more subtle clues - that the key to the case has been lost (for 3
years!), that some frustrated teacher finally got tired of dealing with
confused parents, that none of the school's administrators can think of
a more imaginative solution.


These examples may seem far-fetched. But
those of you old enough to remember the Cold War may also remember that
there was an entire job category known as ``Kremlinologist:'' someone
expert at divining various power shifts among the Politburo based on
evidence such as where various participants were placed within group
photos! The conventional wisdom is that ``a picture is worth a thousand
words'' and while some images may not require much explanation, others
speak volumes. As we move from still images to movies entirely
new channels for meaning (conveyed with the camera's attentional
focus, sound track, etc.) are available to a skilled director. Music
itself has an equally rich but distinct vocabulary. Of course music,
film and motion pictures all predate their representations on computers.
And the ability to easily record and transmit digital SPOKEN
DOCUMENTS (sppech) makes this form of audio especially worthy of
analysis [SparckJones96] . The
convenience and availability of all these electronic media makes it more
possible and even more important to analyze them.


Once again, text
becomes an excellent place to begin. SEMIOTICS is one label for
the sub-field of linguistics concerned with words as symbols,
as conveyors of \rikmeaning. Words in a language represent a
particularly coherent system of symbol use, but so do the symbols used
by photo journalists, painters and movie directors. And the meaning of
these symbols changes with time; recall the pictures of the Clintons and
Doles, their interpetation at the time of publication and their
interpretation now. What these pictures mean is different if we
ask about the original context of 1996 and its meaning
now. And again complex, shifting meanings is typical not only of images
but of documents as well: Watson and Crick's publication of the DNA code
in {\em Nature} in 1953 [Watson53]
was important even then, but what that paper means now could not
have been anticipated.


The prospects for associating contentful
descriptors with images and even richer media are not quite as bleak as
they might seem. In many important cases (e.g., the archives of news
photos maintained by magazines and newspapers), images are accompanied
by CAPTIONS , and video streams with TRANSCRIPTS . This
additional {\em manually-constructed}, {\em textual} data means that
techniques for inferring semantic content directly from images can
piggy-back on top of the text-based IR techniques. In conjunction with
the machine learning techniques we will discuss (cf. Chapter <A
HREF="foa-7.html">&sect;7 ), statistically-reliable associations
found in captioned image and video corpora can be extrapolated to
situations where we have images without captions and video without
transcripts.


In the interim, we will again return to the narrower,
text-only notion of a document with which we began and consider FOA
solutions for this simpler (!) case.




Top of Page

 | UP: Documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.4 Documents





FOA Home

 | UP: Overview



Documents

When ``documents'' were first introduced as part of the FOA process, it
was as one of the set of potential, pre-defined answers to users'
queries. Here we will ground this abstract view in practical terms that
can be readily applied, for example to the searches that are now so
common on the Web. Our goal will be to balance this practical
description of how search engines work today with the abstract FOA view
that goes beyond current practices to other kinds of search still to
come.


A useful working definition is that a DOCUMENT is a {\em
passage of free text}. It is composed of text, strings of characters
from an alphabet. We'll typically make the (English) assumption that
uses the Roman alphabet, Arabic numerals and standard punctuation;
complications like font styles (italics, bold), and especially non-Roman
MARKED ALPHABETS that add characters like \term{\"{a}},
\term{\c{C}}, \term{\~{N}}, \term{\ae}, etc., and the iconic characters
of Asian languages, require even more thought.


By ``free'' text we mean
it is in natural language, the sort native readers and writers use
easily. Good examples of free text might be a newspaper article, a
journal paper, a dictionary definition. Typically the text will be
grammatically well-formed language, in part because this is {written}
language, not oral. People are more careful when constructing written
artifacts that last beyond the moment. Informal texts like email
messages, on the other hand, help to point to ways that some texts can
retain the spontaneity of oral communication, for better and worse [<A
HREF="bibrefs.html#REF803">REF803] .


Finally, we will be interested
in PASSAGES of such text, of arbitrary size. The newspaper
example makes us imagine documents of a few thousand words, but journal
articles make us think of samples ten times that large, and email
messages make us think of something only a tenth as long. We can even
think of an entire book as a single document. All such passages satisfy
our basic definition - they might be appropriate answers to a search
about some topic.


The length of the documents will prove to be a
critical issue in FOA search engine design, especially when the corpus
contains documents of widely varying lengths. The reason is, roughly,
that since longer documents are capable of discussing more topics, they
are capable of being about more. Longer documents are more likely
to be associated with more keywords, and hence more likely to be
retrieved (cf. Section &sect;3.4.2 ).


One
possible response is to make a simple but very consequential assumption:



All documents have equal \about-ness.


 In other words, if we ask the ({a
priori}) probability of any document in the corpus being considered
relevant, we will assume all are equiprobable. This would lead us to
{\em normalize} documents' indices in some way to compensate for
differing lengths. The normalization procedure is a matter of
considerable debate; we will return to consider it in depth later (cf.
&sect;3.4.2 ).


For now, we will take a
different tack towards the issue of document length, as captured by an
alternative pair of assumptions:


 The smallest unit of text with
appreciable \about-ness is the paragraph.


 All manner of longer documents
are constructed out of basic paragraph atoms.


 The first piece of this
argument is that the smallest sample of text that can reasonably be
expected to satisfy a FOA request is a paragraph. The claim is that a
word, even a sentence, does not by itself provide enough {context} for
any question to be answered, or ``found out about.'' But if the
paragraph has been well-constructed, as defined by conventional rules of
composition, it should answer many such questions. And unless the text
comes from James Joyce, Proust, or Lois Borges, we can expect paragraphs
to occupy about half an average screen page -- nicely viewable chunks.


Assumption
(FOAref) alludes to the range of structural relationships by
which the atomic paragraphs can typically be strung together to form
longer passages. First and foremost is simple sequential flow, the order
in which an author expects the paragraphs to be written. The sequential
nature of traditional printed media, from the first papyrus scrolls to
modern books and periodicals, has meant that a sequential ordering over
paragraphs has been dominant. It may even be that the modern human is
especially capable of understanding {\em rhetoric} of this form (cf. <A
HREF="foa-6-2-3.html">&sect;6.2.3 ).


In any case, a sequential
ordering of paragraphs is just one possible way they might be related.
Other common relationships include:


 \item {\em hierarchical} structure
composing paragraphs into sub-sections, sections, and chapters. \item
{\em footnotes}, embellishing the primary theme; \item {\em
bibliographic citations} to other, previous publications; \item
references to other sections of the same document; especially \item {\em
pedagogical prerequisite} relationships ensuring that conceptual
foundations are established prior to subsequent discussion;


 Of course
each of these relationships has grown up within the tradition of printed
publication. Special typographical conventions (boldface, italics, sub-
and superscripting, margins, rules) have arisen to represent them and
distinguish them from sequential flow.


But new, electronic media now
available to readers (and becoming available to authors) need not follow
the same strictly linear flow. The new capabilities and problems of
traversing text in nonlinear ways -- HYPERTEXT -- have been
discussed by some visionaries [REF701]
[Nelson87] for decades. This new
technology certainly permits us to make some traversals more easily
(e.g., jumping to a cited reference with the click of a button rather
than via a trip to the library), but this same ease may make it more
difficult for an author to present a cogent argument.


For now we will not
worry about just how arguments can be formed with nonlinear hypermedia.
Assumptions(FOAref) and (FOAref) simply allow us to
infer Assumption (FOAref) : If all the documents are
paragraphs, we can expect them to have virtually uniform `aboutness'.
These too are simplifying assumptions, however. In an important sense a
scientific paper's abstract is about the same content as the rest
of the paper, and a newspaper article's first paragraph attempts to
summarize the details of the following story. These issues of a text's
LEVEL OF TREATMENT will be discussed later.

Subsections

	 1.4.1 Structured aspects of documents
	 1.4.2 Corpora
	 1.4.3 Document proxies
	 1.4.4 Genre
	 1.4.5 Beyond text




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.5.1 Automatically selecting keywords





FOA Home

 | UP: Indexing



Automatically selecting keywords

We begin by considering the document at its most mechanical level, as a
string of characters. Our first candidates for keywords will be
TOKENS , things broken by WHITESPACE . That is, each and
every token in the document could be considered one of its keywords.


How
good is this simple solution? Suppose users ask for documents
about \term{CARS} and the document we are currently indexing has
the string \term{CAR}. It seems reasonable to assume that users are
interested in this document, despite the fact that the query happens to
contain the PLURAL form \term{CARS} while the document contains
the singular \term{CAR}. For many queries we might like to consider
occurrences of the words \term{CAR} and \term{CARS}, or even
\term{RETRIEVAL} and \term{RETRIEVE}, as roughly interchangeable with
one another; the suffixes do not affect meaning dramatically. And
of course our problem doesn't end with plurals - we could make similar
arguments concerning past tense -ED endings,
-ING participles.


This simple solution also depends too much
on where spaces happen to be. Consider the word German noun ,
corresponding to the English phrase \term{SPEED LIMIT}. In many ways,
the fact that English happens to put a whitespace between the words, or
that German does not, is not semantically critical to the meaning
of these descriptors, and hence the documents in which they might occur.
Such MORPHOLOGICAL features - used to mark relatively
superficial, surface-structure features (such as tense, singular vs.
plural, etc.) can be considered less important to their \rikmeanings.
And differences between German and English are trivial when compared to
Asian texts, where the relationship between characters and
words will be radically altered.


What about HYPHENATION ?
Use of the word \term{DATABASE}, the phrase \term{DATA BASE} and the
hyphenated phrase \term{DATA-BASE} is highly variable, depending on
author preference and current practice at the time and place of
publication. Yet we would hope that all occurrences of any of these
tokens would be treated as references to approximately the same semantic
category. Similarly, we should hope that the end-of-line hyphenation
(breaking long words at syllable boundaries) would not create two
keywords when we would expect only one. But simply adding ``-'' to the
set of whitespace characters defining tokens would make
\term{CLINTON-DOLE} and \term{A-Z} keywords, too?!


Hyphenation is
concerned with the situation where a potential keyword is broken up by
punctuation; what about those situations where a space also breaks up a
semantic unit? seems semantically cohesive, but what algorithm could
distinguish it from other BIGRAMS (consecutive pairs of words)
that just happen to occur sequentially? The problem only becomes that
much more complicated if we attempt to consider longer noun phrases like
\term{APPLES AND ORANGES} or \term{BACK PROPAGATION NEURAL NETWORK}, let
alone more complicated syntactic compounds such as verb phrases, clauses
or sentences. Identifying phrases is an important and active area of
research from the perspectives of both IR and computational linguistics.


Summarizing,
we will take a token to be our default keyword since this is most
straightforward. More sophisticated solutions will handle hyphenation,
multi-words phrases, sub-token stems, etc. (cf. Section <A
HREF="foa-2-3-1.html">&sect;2.3.1 ).




Top of Page

 | UP: Indexing

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.5.2 Computer-assisted indexing





FOA Home

 | UP: Indexing



Computer-assisted indexing

The field of library science has studied the manual process of
constructing effective indices for a very long time. This standard
becomes a useful comparison against which our best automatic techniques
can be compared, but also how difficult comparison will be. There is
data, for example, that suggests that the capacity of one person (e.g.
the indexer) to {anticipate} the words used by another person (e.g., a
second indexer, or the query of subsequent user) is severely limited [<A
HREF="bibrefs.html#Furnas90">Furnas90] ; we are all quite
idiosyncratic in this regard. The lack of inter-indexer consistency
among humans must make us humble in our expectations for automated
techniques.


But manual and automatic indexing need not be viewed as
competing alternatives. In economic terms, if we had sufficient
resources, we could hire enough highly trained catalogers to carefully
read each and every document in a corpus and index each of them. If we
couldn't afford this very expensive option, we would have to be
satisfied with the best index our automatic system could construct. But
if we have enough to hire one or two human indexers, what tools might we
give them that would make most effective use of their time?


We seek
methods which {leverage} the editorial resource, in the sense that this
manual effort does not grow as the corpus does. How might
editors/libarians guide an automatic indexing process? What information
should this computation provide that would allow intelligent human
readers the assurance of a high quality indexing function? Chapter 7
will discuss ways that editors can TRAIN machine learning
systems, and a number of analyses of interest to editors will be
mentioned as well, especially in Chapter 6.




Top of Page

 | UP: Indexing

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.5 Indexing





FOA Home

 | UP: Overview



Indexing

INDEXING is the process by which a vocabulary of keywords is
assigned to all documents of a corpus. Mathematically, an index is a
{\em relation} mapping each document to the set of keywords that it is
on} mapping each document to the set of keywords that it is pping each
document to the set of keywords that it is each document to the set of
keywords that it is document to the set of keywords that it is nt to the
set of keywords that it is the set of keywords that it is t of keywords
that it is eywords that it is s that it is it is \about.


The inverse
mapping captures, for each keyword, the documents it DESCRIBES :


This
assignment might be done manually or automatically. MANUAL
INDEXING means that people, skilled as natural language users and
perhaps also with expertise in the domain of discourse, have read each
document (at least cursorily) and selected appropriate keywords for it.
AUTOMATIC INDEXING refers to algorithmic procedures for
accomplishing this same result. Because the Index relation is the
fundamental connection between the users' expressions of information
need and the documents that can satisfy them, this simply-stated goal,
``Build the Index relation'', is at the core of the IR problem and FOA
generally.

Subsections

	 1.5.1 Automatically selecting keywords
	 1.5.2 Computer-assisted indexing




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.6 FOA versus database retrieval





FOA Home

 | UP: Overview



FOA versus database retrieval

Within the field of computer science, the subfields of databases and IR
are often closely aligned. Databases have well-developed theoretic
underpinnings [Vianu97] that has
generated efficient algorithms [<A
HREF="bibrefs.html#McFadden94">McFadden94] and become the foundation
for one of the most successful elements of the computer industry.


Both
databases and search engines attempt to characterize a particular class
of queries by which many users are expected to attempt to get
information from computers. Historically, database systems and theory
have been perceived as central to the discipline of Computer Science,
probably more so than the IR techniques that are the core technologies
for FOA. How many computer science departments in the US offer
undergraduate classes in Databases? In IR? How many graduate classes?
How many journals or conference proceedings, associated with the ACM or
IEEE, are published in each area? Things may be changing, however.


The
general public's discovery of the Internet and subsequent interest in
search engines like Alta Vista, InfoSeek and Yahoo! suggests that many
users find value in the lists of the Web pages returned in response.
These search engines are clearly doing an important job for many people,
and it is a different job than they use to organize their address book
(record collection, baseball statistics, ...) {databases}. How are IR
and database technologies to be distinguished?


To make the distinctions
more concrete, let's imagine a particular information need and think
about how both a database and a search engine might attempt to satisfy
it. An example query might be:


 What is the best SCSI disk drive to buy?



In the case of databases, strong assumptions must first be made about
{structure} among attributes of individual records. Good database design
demands that the fundamental elements of data, their format, and logical
relations among them be carefully analyzed and anticipated in a
LOGICAL DATA MODEL long before any data is actually collected and
maintained within some physical implementation. These assumptions allow
specification of a syntax for the query language, strategies for
optimizing the query's use of computational resources, and efficient
storage of the data on physical devices.


Let's now assume that a logical
data model like this has been constructed and that a large catalog of
information from various hard drive manufacturers and vendors has been
collated. We will also make the much larger and problematic assumption
that the users are capable of translating the natural language of Q1.3
into the somewhat baroque syntax of a query language like SQL. The
result of the database search might look something like:


 Creating an
example relation like this and populating it with a few instances is
simple, but performing the necessary data modeling, collating the data
from all of the various manufacturers and vendors, and keeping it all up
to date is a much more daunting task. If the database catalog is out of
date, or missing data from some important vendors, users might well
leave the database badly informed.


Now let's imagine using a search
engine on the same query. When run against a Usenet news search engine
like DejaNews, this query results in the retrieval shown in Figure
(figure) with the most highly ranked posting shown in Figure
(FOAref)


 On Thu, 28 Aug 1997 23:10:03 GMT, Michael Query
 wrote:


>My question is, should I get another 2 Gb
SCSI disk for putting the >OS (NT 4.0 WS), software, etc on, or should I
get an IDE disk for this?


Having played around with different configs for
a while, I'd say go SCSI. I'd do that even if I had to get a second SCSI
controller.


(You'll ``hear'' a lot of people arguing that IDE is good
enough, but if you are after overall improved performance SCSI is best.)


my
2Y. \caption{One posting retrieved}


 Users of this search engine will
read about many issues {\em related} to hard disks, some of which
may be {\em relevant} to their particular situation. For example, does
the ``best'' qualifier in Q1.3 mean lowest cost, maximum
capacity, minimum access time,...? Are users able to choose between IDE
and SCSI, or restricted to SCSI? Depending upon just what kind of users
they are, some of the information retrieved may be immediately
applicable to the purchase being considered, while other parts of it are
better considered COLLATERAL KNOWLEDGE (D. E. Rose, personal
communication) that simply leaves users more well-educated.


A very
different set of assumptions are necessary to imagine the search engine
working than those we made about the database system above. For example,
just who wrote these postings? Are they a credible source of good
information; what is their AUTHORITY ? The well-trained database
users should ask equally skeptical questions about the data retrieved,
but rarely are authority, data integrity, etc. considered really part of
database analysis.


But the key assumption for our IR users are that they
can ``listen in'' on this previous ``conversation'' and {interpret} the
text that has been left behind as containing potential answers to his or
her current question. The search engine is charged with retrieving
textual passages that are at least likely to answer the users'
questions. Once presented with these retrievals, the FOA users have more
humble expectations, and are willing to do more interpretive work.
Because FOA searches are often even less concrete than this query,
issued by users simply trying to learn about a topic, {\em
semantic} issues central to the interpretation of a textual passage and
its context, validity, etc. are at the heart of the FOA enterprise.


 has
summarized these issues along a number of dimensions by which IR and
database systems can be distinguished, and several of these are
duplicated in Table (FOAref) :


Database systems are almost
always assumed to provide data items directly. Search engines provide a
level of indirection, a { pointer} to textual passages which contain
many facts, hopefully including some of interest. The information need
of the users is quite vague when compared to that of the database users.
The search engine users are searching for information about a
topic they understand incompletely. Typical database users have a fairly
specific question, like Q1.3 above, in mind. It might even be that the
database is missing some data; for example the special null value \O\
shows that the price of the third disk drive in Figure (FOAref)
is not known. Even in this case, however, the database system ``knows
that it doesn't know'' this fact. FOA queries are rarely brought to such
a sharp point; ambiguity is intrinsic to the users' expectations.


Because
the queries are so general, an FOA retrieval must be described in
probabilistic terms. If a particular hard disk's price is part of our
database, we are certain, with probability=1.0, of its value. Never
would a database system reply with ``This hard disk might cost about As
discussed in depth in Section &sect;5.5 , a
search engine can employ sophisticated methods for reasoning
probabilistically, and available evidence might even allow it to be
quite confident that retrieved items will be perceived as relevant. But
never will we be entirely certain that a document will be what users
want, only that we have high confidence it may be.


Finally, one of the
problems in evaluating search engines is just what success criteria are
to be used. In a database system we typically assume that what we get
back from the database system is correct. (Try to find an ad for a
database system that boasts, ``Our system retrieves only right
answers''!) Database systems are claimed to be more efficient, cheaper,
easier to integrate into existing code, more user-friendly.


This list of
ways search engines might be distinguished from databases is far from
exhaustive; Blair has proposed a more extensive analysis [<A
HREF="bibrefs.html#Blair84">Blair84] . More recently, as search
engine technology and WWW-inspired applications have both burgeoned,
hybrids of database and search have blurred the historical differences
further. Some bases of database/search engine interaction are mentioned
in Chapter &sect;6 .


Chapter <A
HREF="foa-4.html">&sect;4 discusses the evaluation of search engines
in great detail, but typically the bottom line is - Does the system help
you? If you are writing a research paper, did this search engine help
you find materials that were useful in your search? If you are a lawyer
preparing a case, and you want to find every relevant judicial opinion,
does the search engine offer an advantage over an equavalent amount of
time combing through the books in a law library? Such squishy,
qualitative judgements are notoriously difficult to measure, and
especially to measure consistently across broad populations of users.
The next section provides a quick preview of several precise
measurements that have proven useful to the IR community, but would not
be found persuasive within the database community.




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.7 How well are we doing?





FOA Home

 | UP: Overview



How well are we doing?

Suppose you build a FOA search tool, and I do too; how might we decide
which does the better job? How might a potential customer decide on
their relative values? If we use a new search engine that seems to work
much better, how can we determine which of its many features are
critical to this success? If we are to make a science of FOA, or even if
we only wish to build consistent, reliable tools, it is vital that we
establish a methodology by which the performance of search engines can
be rigorously evaluated.


Just as your evaluation of a human
question-answerer (professor, reference librarian, etc.) might well
depend on subjective factors (how well you ``communicate''), and factors
which go beyond the performance of the search engine (does any available
{document} contain a satisfying answer?!), evaluation of search engines
is notoriously difficult. The field of IR has made great progress
however, by adopting a methodology for search engine evaluation that has
allowed objective assessment of a task that is closely related to FOA.
Here we will sketch this simplified notion of the FOA task.


The first
step is to focus on a particular query. With respect to this query, we
identify the set of documents that are determined to be relevant to it.
Then a good search engine is one which can retrieve all and only the
documents in \Rel . Figure (figure) shows both \Rel and \Ret,
the set of documents actually retrieved in response to the query, in
terms of a Venn diagram. Clearly, the number of documents that were
designated relevant and also retrieved, $\mRet \cap \mRel$, will be a
key measure of success.


But we must compare the size of the set $|\cap
\mRel|$ to something, and several standards of comparison are possible.
For example, if we are very concerned that the search engine retrieve
each and every relevant document, then it is appropriate to compare the
intersection to the number of documents marked as relevant, $|\mRel|$.
This measure of search engine performance is known as RECALL :


However,
we might instead be worried about how much of what the users see is
relevant, and so an equally reasonable standard of comparison is what
number of the documents retrieved $|are in fact relevant. This measure
is known as PRECISION :


Note that even in this simple measure of
search engine performance, we have identified two legitimate criteria.
In real applications, our users will often vary as to whether
hi-precision or hi-recall is most important. For example, a lawyer
looking for each and every prior ruling (i.e., judicial opinions,
retrievable as separate documents) that is ON POINT for his or
her case will be more interested in HIGH RECALL behavior. The
typical undergraduate, on the other hand, who is quickly searching the
Web for a term paper due the next day knows all too well that there may
be many, many relevant documents somewhere out there. But s/he cares
much more that the first screen of HITS be full of relevant
leads. Examples of high-recall and high-precision retrievals are also
shown in Figure (FOAref) .


To be useful, this same analysis must
be extended to consider the order in which documents are retrieved, and
it must consider performance across a broad range of typical queries
rather than just one. These and other issues of evaluation are taken up
in Chapter &sect;4 .




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1.8 Summary





FOA Home

 | UP: Overview



Summary

 \item We constantly, naturally Find Out About (FOA) many, many things.
Computer search engines need to support this activity, just as
naturally. \item Language is central to our FOA activities. Our
understanding of prior work in linguistics and the philosophy of
language will inform our search engine development, and the increasing
use of search engines will provide empirical evidence reflecting back to
these same disciplines. \item IR is the field of computer science that
traditionally deals with retrieving free-text documents in response to
queries. This is done by indexing all the documents in a corpus with
keyword descriptors. There are a number of techniques for automatically
recommending keywords, but also a great deal of art. \item Users'
interests must be shaped into queries constructed from these same
keywords. Retrieval is accomplished by matching the query against the
documents' descriptions and returning those that appear closest. \item A
central component of the FOA process is the users' relevance feedback,
assessing how closely the retrieved documents match what they had ``in
mind.'' \item Search engines accomplish a function related to database
systems, but their natural language foundations create fundamental
differences as well. \item In order to know how to shop for a good
search engine, as well as to allow the science of FOA to move forward,
it is important to develop an evaluation methodology by which we can
fairly compare alternatives.


 In this overview we've made some
simplifying assumptions and raised more questions than we've answered,
but that is exactly the goal! By now, I hope you have been convinced
that there are many facets to the problem of FOA, ranging from a good
characterization of what the users seek, to what the documents to
methods for inferring semantic clues about each document, to the
problem of evaluating whether our search engines are performing as we've
intended. The rest of this book will consider each of these facets in
greater detail, and others as well. But like all truly great problems,
issues surrounding FOA will remain long after this text is dust.




Top of Page

 | UP: Overview

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 1 Overview





FOA Home

 | UP: foa-book.tex



Overview
Subsections

	 1.1 Finding Out About - a cognitive activity
	 1.2 Keywords
	 1.3 Query syntax
	 1.4 Documents
	 1.5 Indexing
	 1.6 FOA versus database retrieval
	 1.7 How well are we doing?
	 1.8 Summary




Top of Page

 | UP: foa-book.tex

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.1 Building useful tools





FOA Home

 | UP: Extracting Lexical Features



Building useful tools

The promise offered by the first chapter is that many real-world
problems can be viewed as instances of the FOA problem. The proof is to
be found in concrete code - a relatively small technology base that will
prove useful in a wide array of applicatons. In this chapter we will
present a suite of software tools that together generate a ``search
engine" for a wide variety of situations. Source code is provided so
that these tools can be easily modified for applications of your own. We
will work through two different examples of IR systems, in order to
demonstrate how slight variants of the same basic code can handle both.


Compared
to the broad generalities of Chapter 1, the technical details of this
chapter will sound a much different tone. Describing a complex algorithm
requires the specification of many, sometimes tedious details. To make
the software executable on machines that are likely to be available to
you, the details are provided for several operating systems. But the
processor speeds, internal memory and harddisk sizes available on
computers is changing dramatically each year, so many of the assumptions
on which these routines are based will require constant re-evaluation.


You
will develop the software tools in three phases. The first phase will
convert an arbitrary pile of textual objects into a well-defined corpus
of documents, each containing a string of terms to be indexed. The
second phase involves building efficient data structures to ``invert"
the $Index$ relation so that, rather than seeing all the words that are
in a particular document, we can find all documents containing
particular keywords. All of these efforts are in anticipation of the
third and final phase, which matches queries against indices in order to
retrieve those that are most similar. These three major phases are
central to building any search engine.


This chapter will be most
concerned with the first two phases that together extract lexical
features. Our goal will be the extraction of a set of features worthy of
subsequent analysis. As in any cognitive science, the specification of
an appropriate LEVEL OF ANALYSIS -- whether it is the resolution
and depth of an image; the sub-phonemes of continuous speech; the speech
acts of language ... -- the specification of this atomic feature set is
the first important step.


This will involve a great deal of work, much of
it unpleasant except to those who enjoy designing efficient algorithms
and data-structures (some of us actually do enjoy this!:). The promise
is that we will, as a consequence of good software design, develop
useful tools that allow us to spend the rest of our time exploring
interesting features of language.




Top of Page

 | UP: Extracting Lexical Features

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.2 Inter-document parsing





FOA Home

 | UP: Extracting Lexical Features



Inter-document parsing

The first step is to break the corpus -- an arbitrary ``pile of
text'' -- into individually retrievable documents. This demands
that we be specific about the format of the corpus, and that we decide
how it is to be divided into individual documents. For all operating
systems we will consider, this problem can be defined more precisely in
terms of paths, directories, files, and
position within file. For any application in which the corpus
can be described by the path to its root, these tools will translate
directories/files/documents-within-files into a homogenous corpus. Of
course, there are some situations (e.g., when documents are maintained
within a database) that cannot be captured in these terms, but these
primitives do allow a wide range corpora to be specified.


Our model will
assume that many documents may be contained within a single file, and
that each document occupies a contiguous region within the file. Extend
the software to allow a document to be comprised of multiple,
non-contiguous textual fields.


 Issues concerning structure within a
single document are closely related to assumptions we may or may not be
able to make about the lengths of the documents in question. Our
assumptions about how long a typical document is will recur throughout
this book. It is obvious, for example, that different document browsers
are necessary if we need to browse through an entire book rather than
look at a single paragraph. Less obvious is that the fundamental
weighting algorithms used by our indexing techniques will depend very
sensitively on the number of tokens contained in each document.


 Take a
large \textbf{LaTeX}\ document and run it repeatedly through
\texttt{LaTeX2HTML}, systematically varying the logical unit of document
structure at which individual HTML pages are constructed. Discuss the
impact of these ``arbitrary'' decisions on the weight of the key words.
\end{exercise}


In this textbook we will focus primarily on two particular
test corpora, AI theses (AIT) and email; these are discussed in more
detail in Section &sect;2.4 . Each of these
have natural notions of the individual document: In the case of the AIT
it is the thesis's abstract, and for email it is the entire message. In
both cases, more refined notions of document (the individual paragraphs
within the abstract or within the email message) are possible.


With these
assumptions, we can define our corpus simply with two files: one
specifying full path information for each file, and a second specifying
where within these files each and every message resides. A large portion
of the task of navigating a directory full of files and visiting each of
them can be accomplished using the dirent. { The {\tt
dirent} interface began with a Berkeley Software Distribution (BSD)
specification written by Kirk McKusick in the mid-1980s. It has evolved
to be a part of the POSIX standard. Ports to various platformds (Linux,
MSDOS, MacOS) are available [Gwyn94] .


}
This utility allows the recursive descent through all directories from a
specified root, visiting every file contained therein.


In many cases, the
files we will be indexing will have a great deal of syntactic structural
information above and beyond the meaningful text itself. For example,
our email will often contain a great deal of mail header information, as
(loosely:) face? specified in RFC822. Many word processing
systems, for example in \TeX, XML and HTML, now produce documents with a
well-defined syntax. If, for example, the documents are written in HTML,
we don't want to index pseudo-words like . In many of
these situations, FILTERS exist that can extract just the
meaningful text from surrounding header or format information; <A
HREF="http://www.cs.purdue.edu/~trinkle/detex/index.html">DeTeX {an
example of a useful filter for removing \LaTeX and \TeX markup.} Use of
such utilities spares us the task of parsing this elaborate structure,
but it also means that more elaborate solutions for maintaining the
difference between the document's index and the document's presentation
must be addressed.


The basic data elements to be parsed from our two
examples, email and AIT, are shown in Figure (figure) .




Top of Page

 | UP: Extracting Lexical Features

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.3.1 Stemming and other morphological processing





FOA Home

 | UP: Intra-document parsing



Stemming and other morphological processing

From the perspective of linguistics, many of the early design issues we
address are considered MORPHOLOGICAL TRANSFORMATIONS of language;
i.e., an analysis of what we can infer about language based on
structural features. As discussed briefly in Chapter 1, the
arbitrary way in which whitespace may or may not separate tokens whose
meanings are inter-dependant (e.g., recall the German word
\term{GESCHWINDIGKEITSBESCRANKUNG}/English phrase \term{SPEED LIMIT}
example) will make us interested in phrasal units of indexing
as well. In many Asian texts, the relationship between
characters and words is quite radically altered. The
Kanji alphabet and Unicode standards help to define the problem, but
bring biases of their own [Fuji93] .


For
now we will focus on one of the most common morphological tricks,
STEMMING . Stemming is a quite direct attempt to remove certain
SURFACE MARKINGS from words so as to reveal ROOT form.
Beyond deciding just which characters are to be combined into tokens,
Chapter 1 discussed how important it can be to use a token's root form
as an index term: We can hope that our retrieval is robust even when the
query contains the PLURAL form \term{CARS} while the document
contains the singular \term{CAR}. Linguists would say that the
NUMBER feature (whether a noun is singular or plural) is
morphologically MARKED . Linguists also distinguish between
INFLECTIONAL morphology like plurals, and DERIVATIONAL
morphology which can change a word's syntactic category (e.g., changing
the noun {\tt PRODUCT} to the verb {\tt PRODUCTIZE}) and meaning
more radically.


In stemming, suffixes are dropped. Even in the simple
case of plural endings, it isn't as simple as removing 's. Consider:


Conversely,
we also cannot assume that every time there is an ending ``S'' we can
remove it; stemming and \term{CHESS} to \term{CRISI} and \term{CHES}
would damage their meaning.


The most common approach to this problem [<A
HREF="bibrefs.html#Frakes92b">Frakes92b] is to identify more
elaborate patterns over character sequences that reliably pare tokens
down to their root forms. A broad range of such patterns can be defined
in terms of a CONTEXT-SENSITIVE TRANSFORMATION GRAMMAR .


For
example, (.*)SSES & \rightarrow & /1 SS \\ (.*[A E I O U].*) ED &
\rightarrow & /1 \\ (.*[A E I O U].*) Y & \rightarrow & /1 I Rule
(FOAref) says that strings ending in {\tt -SSES} should be
transformed by taking the stem (i.e., characters prior to these four)
and adding only the two characters {\tt SS}. Rule (FOAref) says
that stems containing a vowel and ending in {\tt -ED} should be
transformed to leave only the stem; Rule (FOAref) says that
stems containing a vowel and ending in {\tt -Y} should be transformed to
the stem with an {\tt I} replacing the {\tt Y}.


A complete algorithm for
stemming involves the specificaton of many such rules, and then a regime
for handling conflicts when multiple rules match the same token. An
early and influential algorithm due to Lovins [<A
HREF="bibrefs.html#Lovins68">Lovins68] specified 260 suffix patterns
and then used an ITERATIVE LONGEST MATCH heuristic. This means
first that preference is given to the pattern (left-hand side of the
grammar rule) that matches most characters in a target token (since this
prefers more specific matches over shorter, more generally applicable
ones), and then that rules are iteratively re-applied until no more
rules match.


The Porter stemmer [<A
HREF="bibrefs.html#Porter80">Porter80] (included as part of the FOA
software) is a simplified version of Lovin's technique that uses a
reduced set of about 60 rules, and organizes them into sets, with
conflicts within one subset of rules resolved before going on to the
next. In fact, if only the first set of rules in Porter's stemmer
(focusing exclusively on plurals and the most straight-forward suffices
like {\tt -ED} and {\tt -ING}) are used, the result has been called
WEAK STEMMING [Walker89] . A
key advantage of all such rule-based grammatical representations of the
stemming process (and of efficient implementations of them, such as that
provided by Fox) is that modifications to the rules and to ordering
among the rules can be accomplished by changing the grammar rather than
by endless {\em ad hoc} hacking (ad hacking?:) in response to particular
character sequences.


The use of any stemmer obviously reduces the size of
the keyword vocabulary, and consequentially results in a compression of
the index files. Such compression can vary from 10-50depending on the
total size of the keyword vocabulary and how aggressively (e.g., how
many suffix rules are used) the stemmer is.


The primary affect of
stemming, however, is that two keywords that were once treated
independently are considered interchangeable. Stemming is therefore an
example of a recall increasing operation since it will cause a
keyword used in the query to match more documents.


The fundamental
problem with any stemming technique, of course, is that the
morphological features being stripped away may well obscure differences
in the words' meaning. For example, the token {\tt GRAVITY} has
two WORD SENSES , one describing an attractive force between any
two masses and the other having to do with a serious mood. But once the
word GRAVITATION has been stemmed, we have lost the
information that might constrain us to the first interpretation. Krovetz
[Krovetz93] considers several more
sophisticated approaches to keyword morphology, including augmenting
Porter's stemmer with a dictionary that is checked after each phase of
stemming rules has been applied.




Top of Page

 | UP: Intra-document parsing

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.3.2 Noise words





FOA Home

 | UP: Intra-document parsing



Noise words

From the earliest days of IR (e.g. Luhn's seminal work [<A
HREF="bibrefs.html#Luhn57">Luhn57] ) two related facts have been
obvious: first, that a relatively small number of words account for a
very significant fraction of all text's bulk. Words like IT,
AND and TO can be found in virtually every sentence.
Second, these  NOISE WORDS make very poor index terms. Users are
unlikely to ask for documents about TO and it is hard to
imagine a document about BE.} Due then to both their
frequency and their lack of indexing consequence, we will build in the
capability of ignoring noise words into our lexical analyzer.


As will be
discussed extensively in Chapter 3, noise words are often imagined to be
the most frequently occurring words in a corpus. One problem with
defining noise words in this way is that it requires a frequency
analysis of the corpus prior to lexical analysis. It is possible to use
frequency analyses from other corpora, assuming that the distribution of
noise words is relatively constant across corpora, but such an
extrapolation is not always warranted. Worse, the most frequent words
often include words that might make very good key words. Fox notes that
the words TIME, WAR, HOME, LIFE, WATER and
WORLD are among the 200 most frequent in general English
literature.[FoxC92]


 Instead, we will
define noise words extensionally, in terms of a finite list or
NEGATIVE DICTIONARY. The list we use, STOP.WRD, was derived by
Fox from an analysis of the Brown corpus[<A
HREF="bibrefs.html#Fox90">Fox90] .


The relationship between these
noise words and those words most critical to syntactic analysis of
natural language sentences is striking. note that the same tokens that
are thrown away as noise because they have no meaning are precisely
those FUNCTION WORDS most important to the syntactic
analysis of well-formed sentences. This is the first, but not the last,
suggestion of a fundamental complimentarity between FOA's concern with
semantics and computational linguistics' concern with syntax.




Top of Page

 | UP: Intra-document parsing

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.3.3 Summary





FOA Home

 | UP: Intra-document parsing



Summary

We have described the lexical analyzer in terms of the job it must do
processing each and every document in the corpus because this task
confronts us first. But because our central task will be to match
 these documents against subsequent users' queries, it is critical
that the identical lexical analysis be performed on the queries as was
performed on the documents. This creates several implementation
constraints (e.g., that the same code libraries are available to the
indexer and to the query processing interface), but these are minor. If
the query language is designed to support any special operators (e.g.,
Boolean combinators, proximity operators), the query's lexical analyzer
may accept a super-set of the tokens accepted by the document's
analyzer. In any case, it is imperative if queries and documents are to
be matched correctly that the same lexical analysis be applied to both
streams. Using an identical code library is the easiest way to ensure
this.


It may seem nonsensical to worry so much about processing each
character efficiently, when we assume that some other, previous process
has already identified each inter-document break - doesn't such
processing require the same computational effort, and, if so, doesn't
this make our current efficiency worries moot?


Perhaps. A conclusive
answer depends on many architecture and operating system specifics.
There are two reasons that we have made such assumptions. The first is
that the practicalities of delivering the FOA corpora and code currently
make this convenient. But the more serious reason is that the most
theoretically and intellectually interesting questions involve analysis
of operations downstream from the first stages of inter-document parsing
- how to identify tokens, how to count them, etc. If these latter
operations are made especially efficient, it means we can afford to do
more experimentation, more playfully. For a text, that is the primary
concern.




Top of Page

 | UP: Intra-document parsing

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.3 Intra-document parsing





FOA Home

 | UP: Extracting Lexical Features



Intra-document parsing

Having now focused our attention on a particular file, and beginning and
ending locations within that file associated with a particular document,
we can consider this file segment as simply a a STREAM of
characters. Reading each and every character of each and every document,
deciding whether it is part of a meaningful token or not, and deciding
whether these tokens are worth indexing will be the most computationally
intensive aspect of the indexing chore; this is our ``inner loop." For
that reason, we will devote some real attention to making this lexical
analysis as efficient as possible.


Several general criteria will shape
our design. First, since we are assuming that our textual corpus is very
large, we will do our best to avoid duplicating this primary text. That
is, we will attempt to deal with the text in situ, and not make
a second copy for use just by the indexing and retrieval system. Thus,
we will be creating a system of pointers into locations within the
corpora directories and files.


A wide range of alternative designs are
possible even at this early stage, and so we desire as much flexibility
as possible in the specification of the lexical analyzer. A LEXICAL
ANALYZER GENERATOR , such as the lex tool in UNIX,
allows the specification of very complicated lexical analyzers for very
elaborate languages. The fundamental representation used by all such
algorithms is a FINITE STATE MACHINE , like that shown in Figure
(figure) . This simple representation breaks the set of
possible characters coming from a text stream into classes (drawn as
circular states), with transitions from one state to the next on the
occurrence of particular characters. By careful construction of the sets
of characters (e.g., WHITE-SPACE characters corresponding to
state $0$ in (FOAref) ), arbitrary text sequences can be
handled very efficiently.


For our two example corpora and many other
situations, the stream of characters, a straight-forward analysis in
terms of simple FINITE STATE MACHINE will suffice. We will depend
on a utility written by Christopher Fox [<A
HREF="bibrefs.html#FoxC92">FoxC92] . This utility simultaneously
achieves two critical goals. First, the lexical analyzer
TOKENIZES the stream of characters into a sequence of word-like
elements. At first blush this seems straightforward - a token is
anything separated by white space, where the standard definition of
white space is used. But what about hyphens? Should the hyphenated
phrase DATA-BASE be treated as two separate tokens or as a
single one? Should a file name, like WINDOWS.EXE be treated
as a single token? Which host, directory and file elements in a full URL
like {\tt www.cs.ucsd.edu/\~{ }rik} are to be kept intact as
individuated tokens? More elaborate elements such as these can quickly
demand the sophistication of a tool like lex.


The presence
of digits among the alphabetic characters presents another problem. Are
numbers to be allowed as tokens? Perhaps we only want to allow
``special'' numbers (e.g., {1776, 1984, 2001, 3.14159}, ...). Perhaps we
want to use rules similar to those for programming language identifiers
and require that a token begin with an alphabetic character which may
then be followed by numbers or letters.


We must also worry about the
CASE of the characters at this earliest lexical analysis stage.
Are we to treat capitalization as significant in distinguishing tokens
one from another, or not? An enormous reduction in vocabulary size is
possible if we FOLD CASE so as to treat upper and lower
characters interchangeably. But of course then we have also precluded
the possibility of many proper name analyses that may be useful for
identifying SINGULAR people, places or events (see Chapters <A
HREF="foa-6.html">&sect;6 .). In some cases the semantics of the
documents make decisions about case automatic. For example, if the
documents are program source files, the language in question may or may
not treat differences in case as significant.

Subsections

	 2.3.1 Stemming and other morphological processing
	 2.3.2 Noise words
	 2.3.3 Summary




Top of Page

 | UP: Extracting Lexical Features

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.4 Example corpora





FOA Home

 | UP: Extracting Lexical Features



Example corpora

In these experiments, and the rest to follow, we will consistently be
using two example corpora.


The first of these we will call the
``Artificial Intelligence Thesis" corpus. This is approximately 5,000
Ph.D. and Masters dissertations and abstracts. Virtually every
dissertation published within the last 30 years has been microfilmed by
University Microfilms, Inc.. These were selected by Suzanne Humphries.}
The corpus is a fairly exhaustive set of theses classified as AI by
University Microfilms, Inc., from the years 1987 to 1997. A histogram of
the theses distribution by year is shown in Figure (figure) .


We
will focus on a handful of characteristics of each thesis: \item
Thesis\# \item Title \item Author \item Year \item University \item
Advisor \item Language \item Abstract \item Degree ree \eit


For now, we
will lump these attributes into two categories: TEXTUAL FIELDS
and STRUCTURED ATTRIBUTES . Structured attributes are ones for
which we are able to reason more formally, using database and artificial
intelligence techniques. For now, we will concentrate on only the
textual fields. The abstract will be the primary textual element
associated with each thesis, while its title (also a textual field) will
be used as its PROXY - a synopsis of the thesis that conveys much
of its meaning in a highly abbreviated form. Proxies will prove very
important surrogates for the documents (for example when users are
presented with hitlists of retrieved documents).


The second corpus we
will study could not be provided on the CD because you must provide it
yourself - it is all of {your} email. We will assume that with disk
storage as cheaply available as it is today, you have at some point
begun to collect email. Typically some of this will be email others have
sent you, but you may have also kept a copy of all of your ``outgoing"
email as well. Many email clients support on-the-fly segregation of
email into separate folders. Minimally, this means that our procedures
for indexing email must be capable of traversing elaborate directory
structures. Later we will also consider the use of this user-generated
structure as a source for learning; cf. Section <A
HREF="foa-7-4.html">&sect;7.4 .


The directory in which you have filed
an email message is one feature we may associate with each message;
whether it is an incoming or outgoing message is another. But of course
email also has many structured attributes associated with it, in its
HEADER . These include: \bit \item From: \item To: \item Cc:
\item Subject: \item Date: e: \eit


In general we will put off all
consideration of structured attributes associated with documents until
later chapters. For now simply note the many parallels between our two
example corpora: both have well-defined authors, well-defined
time-stamps, and excellent and obvious candidates for proxy text.




Top of Page

 | UP: Extracting Lexical Features

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.1 Basic algorithm





FOA Home

 | UP: Implementation



Basic algorithm

Then, the basic flow of what we will call the ``postdoc" function
operates as follows.


For every document in the corpus we will iterate
through a loop until we've exhausted every token in that document. So
let's call { getNonNoiseToken } a routine which repeatedly builds tokens
from the document's stream, does whatever character assessments are
required, checks it against a negative dictionary, and returns a token.
If stemming is to be applied, we'll stem the word at this point. Then we
will save a POSTING for that token's occurrence in that document.
A posting is simply a correspondence between a particular word and a
particular document, representing the occurrence of that word in that
document. That is, we have a document in front of us and it contains a
set of tokens. We are now going to build a representation for each token
that tells all of the documents in which it occurs. For each keyword we
will maintain the token itself as the key used for subsequent access,
and also the head of a linked list of all of postings, each containing
the document number and the number of occurrences of the keyword in that
document. A sketch of these data structures is shown in Figure
(figure) .


After having gone through every single document in
the corpus in this fashion, we have a large collection of postings. Here
we recommend SPLAY TREES as an appropriate data structure for
these keywords and their postings. In the C implementation shown in
Figure (FOAref) , the InstallTerm() function inserts a new
posting into the Terms tree.


$ is the same as the last time, and simply
increment the appropriate counter in this case.


Splay trees are
appropriate technology because we can count on the many frequency-biased
queries to cause the resulting tree to become well-balanced with use.}


During
the processing of each document, it will prove important to know how
many keywords are extracted from it. This will be known as the
DOCUMENT'S LENGTH , denoted $length_d$; this quantity is
important when {\em normalizing} documents of different lengths. One way
to implement this computation is to maintain a small, separate file {\tt
doclend.d} containing only this one number for each document.


When the
set of documents has been exhausted, we need to write out this
INVERTED representation to a file for subsequent processing. For
every token in the splay tree (typically the traversal will be in
lexicographic order), we will organize all its postings. First, we count
the number of occurrences of the keyword across all the documents in the
corpus; we will call this variable $totfreq_k$. A second, less obvious
statistic we will also maintain is how many documents contain this
keyword; this variable will be called $docfreq_k$. If there is exactly
one occurrence of a keyword in each document, then these two numbers
will be the same. But typically there are multiple occurrences of the
same keyword in a single document and $totfreq_k > docfreq_k$. Both
variables will be important to us in determining appropriate weights for
the $Index$ relation (cf. Chapter 3.)


After having gone through all of
the documents, and accumulating for each one these two statistics, we
must sort the postings in decreasing frequency order. The reason for
this won't be apparent until we discuss the matching algorithms (cf.
Section &sect;3.5 ), but it turns out to be
important that documents that use a keyword most often are at the
beginning of the list.


Once the documents' postings have been sorted into
descending order of frequency, it is likely that several of the
documents in this list will happen to have the same frequency, and we
can exploit this fact to compress their representation. Figure
(figure) . shows the $\mathname{POSTING}$ list broken into a
list of $\mathname{FPOST}$ sublists, one for unique frequency count.




Top of Page

 | UP: Implementation

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.2.1 Changing indices for dynamic corpora





FOA Home

 | UP: Fine points



Changing indices for dynamic corpora

One reason to keep raw frequency counts in the inverted keyword file
used by our experimental implementation is that this provides maximum
flexibility as we consider various keyword weighting schemes. But there
is another reason that these raw statistics are useful in real
applications, too.


Often it is important to be able to {update} a corpus'
index as documents are added and/or deleted to and from it. Retention of
raw keyword frequency information allows these statistics to be updated
as our corpus changes. Adding a new document simply requires that it be
analyzed (as outlined above), simply incrementing existing counters for
each key word. is used, however, incremental addition of document
postings is slightly more awkward.} Similarly, deletion of documents
from an index exploits the full text of the document itself to identify
all keywords it contains. For each keyword then, posting counts are
simply decremented. {The optimized{\tt fpost} data structure makes this
update awkward as well.}




Top of Page

 | UP: Fine points

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.2.2 Posting resolution





FOA Home

 | UP: Fine points



Posting resolution

Typically we need only keep track of {which } document contains the
posting. But an important element of many query languages are
PROXIMITY operators which allow users to specify how close two
keywords must be (adjacent words, within the same sentence, within the
same paragraph, within a $k$-word window of one another, etc.). To
support such queries, we may also be concerned with recording higher
RESOLUTION posting information than just which document it is in.
For example, many systems retain the exact {\em character} position of
the (beginning of the) keyword. Figure (figure) shows the
elaborate data structure used by the STAIRS IR system. In addition to
very high-resolution postings, this representation supported other query
attributes (e.g. security) as well.




Top of Page

 | UP: Fine points

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.2.3 Emphasizing proxy text





FOA Home

 | UP: Fine points



Emphasizing proxy text

The fact that keyword tokens occur in both the proxy text and the main
text of the document gives us the opportunity to treat them differently.
For example, we can {emphasize } the importance of words used in the
proxy over those occurring in the raw text. This would be sensible if we
believed that those occurrences in the subject of a message, for
example, or in the title of a dissertation, are better characterizations
of a document than words picked from the text of the abstract or the
text of the email message. In our code, this emphasis will be controlled
by an integer variable EmphProxy, which notes occurrences
of keywords in the proxy by doubling (EmphProxy=2) or
tripling (EmphProxy=3) the keyword counters for proxy text.




Top of Page

 | UP: Fine points

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.2.4 Document number





FOA Home

 | UP: Fine points



Document number

Because we have made the first stage of our processing flexible with
respect to how a corpus extends across multiple files, in general, {two}
numbers will uniquely identify each of your documents: its file number
and document number within that file. For that reason, and because each
posting must retain a unique identifier for each document, it becomes
important to construct a single number that folds them together.
Maintaining a single integer, instead of two integers, therefore becomes
a worthwhile space saving.


One simple way to accomplish this is to
multiply the document's file number by some number larger than the
maximum number of documents within any file, then add its document
number. Just how large a number this must be, whether your
machine/compiler efficiently supports integers this large (or whether
you are better off keeping the two numbers separate) will vary
considerably. For this reason it makes good sense to isolate these
issues in a separate routine.




Top of Page

 | UP: Fine points

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.2.5 Dependencies on document type





FOA Home

 | UP: Fine points



Dependencies on document type

The process of indexing has been idealized, as having a first stage
where we worry about what kind of document it is (e.g., whether it's a
thesis or an email message), and then assuming subsequent processing is
completely independent of document type. Like all software designs, this
idealization breaks down in the face of real data.


Consider email
messages. One common element of these documents is {quoted} text from
another email message. Often this is marked by a caret prefix, as shown
in Figure (FOAref) . The role of inter-document citations like
this is considered in depth in Section <A
HREF="foa-6-1.html">&sect;6.1 , but for the present a reasonable
design decision is that all text should be indexed only once. Especially
appropriate if we have both the original email message and the quoted
version of it, we might want to elide (ignore) quoted lines.


On Thu, 28
Aug 1997 23:10:03 GMT, Michael Query  wrote:


>My
question is, should I get another 2 Gb SCSI disk for putting the >OS (NT
4.0 WS), software, etc on, or should I get an IDE disk for this?


Having
played around with different configs for a while, I'd say go SCSI. I'd
do that even if I had to get a second SCSI controller.


(You'll ``hear'' a
lot of people arguing that IDE is good enough, but if you are after
overall improved performance SCSI is best.)


my 2Y. \caption{Quoted lines
in an Email message}


 Other software designs are possible, but the
easiest way to implement this is to check for quoted lines within the
{postdoc} routine - if the first character of a line is a caret mark,
don't do any of the subsequent processing. Don't check it against noise
words, don't stem, index, or install it in the term tree. Unfortunately
this creates precisely the kind of Email-specific processing that should
be avoided by well-engineered software.




Top of Page

 | UP: Fine points

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.2 Fine points





FOA Home

 | UP: Implementation



Fine points
Subsections

	 2.5.2.1 Changing indices for dynamic corpora
	 2.5.2.2 Posting resolution
	 2.5.2.3 Emphasizing proxy text
	 2.5.2.4 Document number
	 2.5.2.5 Dependencies on document type




Top of Page

 | UP: Implementation

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5.3 Software libraries





FOA Home

 | UP: Implementation



Software libraries

As much as possible, we will depend on standard libraries for some of
our basic utilities. In particular, it is recommended that you use:



\item gethash - This is the standard hashtable routine,
part of most Unix distributions.




Top of Page

 | UP: Implementation

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2.5 Implementation





FOA Home

 | UP: Extracting Lexical Features



Implementation

The range of potential implementations of the basic techniques discussed
in this chapter and subsequent ones is quite remarkable. Each depends on
features of the specific application, available hardware, etc.: \item
using a massively parallel super computer of the mid-1980s to provide
current news to financial analysts [<A
HREF="bibrefs.html#REF378">REF378] ; \item searching for file names
as part of the MacOS Finder on a single personal computer, then being
extended to support file \defn{content} searching as part of the MacOS
Sherlock utility;


 <A
HREF="http://ftp://ftp.cs.cornell.edu/pub/smart/">SMART is a classic
software suite designed to support experimentation into basic IR
techniques (see Section &sect;3.4.3 for
more details);


 providing a generic utility for <A
HREF="http://www.mds.rmit.edu.au/mg/welcome.html">Managing Gigabytes
(MG) for example building an index for a CD-ROM or DVD.


making all of
the pages on a WWW server searchable via <A
HREF="http://www.searchtools.com/tools/tools.html">Web Server Search
Tools or <A
HREF="http://www.glue.umd.edu/~dlrg/filter/software.html">Information
filtering tools


 Design decisions depend on features such as corpus
size, available memory, queryr response time, etc. Two implementations
have been developed to acocmpany this textbook, an earlier one in C and
a more recent one in Java; see FOA website
for details.

Subsections

	 2.5.1 Basic algorithm
	 2.5.2 Fine points
	 2.5.3 Software libraries




Top of Page

 | UP: Extracting Lexical Features

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 2 Extracting Lexical Features





FOA Home

 | UP: foa-book.tex



Extracting Lexical Features
Subsections

	 2.1 Building useful tools
	 2.2 Inter-document parsing
	 2.3 Intra-document parsing
	 2.4 Example corpora
	 2.5 Implementation




Top of Page

 | UP: foa-book.tex

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.1 Microscopic semantics and the statistics of  communication





FOA Home

 | UP: Weighting and matching against Indices



Microscopic semantics and the statistics of  communication

In the last chapter, we described the FOA process linguistically, in
terms of words that occur in documents, morphological features of these
words, structures organizing the sentences of documents, etc. We now
want to treat all of these words --- which have meaning to their
authors and to us reading them --- as a \rikmeaning-less stream of data
- word after word after word. (Imagine it coming from some SETI radio
telescope, eaves-dropping on the communication of some other planet!) We
will now seek common patterns and trends to this data, using the same
sorts of statistical tricks that physicists typically use on their data
streams. What can we learn from looking at the statistics of our data
stream, treating text as \rikmeaning-less and attempting to infer a new
notion of meaning from those statistics?


But now let's narrow our focus,
all the way down to the bits and characters used to represent the
corpus, for example as a file on a physical device, like a hard disk.
Imagine that you are an archaeologist, trying to study some civilization
that had left this evidence behind. How might you interpret this modern
Rosetta Stone?


Let's ignore those issues relating to the basic ASCII
encoding. That is, suppose we have special knowledge of a character set.
Then the frequency of these characters' occurrences would already give
us a great deal of information. Anyone who has studied simple cipher
techniques (or played Scrabble!) knows that a table of most frequently
used letters (cf. Figure (FOAref) [<A
HREF="bibrefs.html#Welsh88">Welsh88] ) can be well-exploited for
breaking simple codes.


 \hline Letter & Frequency\\
\hline E & .120 \\ T & .085 \\ A & .077 \\ I & .076 \\ N & .067 \\ O &
.067 \\ S & .067 \\ R & .059 \\ H & .050 \\ D & .042 \\ L & .042 \\ U &
.037 \\ C & .032 \\ F & .024 \\ M & .024 \\ W & .022 \\ Y & .022 \\ P &
.020 \\ B & .017 \\ G & .017 \\ V & .012 \\ K & .007 \\ Q & .005 \\ J &
.004 \\ X & .004 \\ Z & .002 \\ \hline \caption{English letter
frequencies}


 In this chapter we will move another level above
characters. We will consider morphological transformations we can
perform on character sequences that help us to identify root words. We
will briefly mention phrases by which multiple words can be joined into
simple phrasal units.


At each level we will ask very similar questions:
What is our unit of analysis; i.e., just what are we counting? Then,
what does the distribution of frequency occurrences across this level of
features tell us about the pattern of their use? What can we tell about
the meaning of these features, based on such statistics [<A
HREF="bibrefs.html#Francis82">Francis82] ?


In fact, many influential
thinkers have looked at such patterns among symbols. Going back to some
of our most ancient writings suggests that statistical analyses of the
original Hebrew characters and their positions within the
two-dimensional array of the page reveals new ``codes'' [<A
HREF="bibrefs.html#Witztum94">Witztum94] [<A
HREF="bibrefs.html#Drosnin97">Drosnin97] .


Donald Knuth, one of
computer science's most reknowned theoreticians, has analyzed an
apparently random verse (Chapter 3, verse 16) from 59 of the Bible's
books and used these as the basis of STRATIFIED SAMPLING of the
approximately 30000 Biblical verses [<A
HREF="bibrefs.html#Knuth90">Knuth90] . He found, for example, that
the 3:16 verses were particulary richin occurrances of {\tt YHWH}, the
ancient Hebrew name for God. Personally, Knuth found this analysis the
source for ``historical and spiritual insights,'' as part of a Bible
study class he lead. But speaking scientifically, how can we
find meaning in text, and when are such attempts merely
numerology?




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2.1.1 Zipf's own explanation





FOA Home

 | UP: Looking for meaning in all the wrong places \\         (at the character level)



Zipf's own explanation

To explain his empirical observations, Zipf himself proposed a
theoretical model that aimed at describing the ultimate purpose of
communication between authors and readers.


Zipf's theory was
extraordinarily broad, addressing not only (!) patterns in text but also
patterns across all human activities. According to Zipf's fundamental
PRINCIPLE OF LEAST EFFORT all activities can be viewed as
interactions between {\em jobs} needing to be done, and {\em tools}
developed to accomplish them. In a mature society in which a variety of
jobs and tools have existed for some time, a ``reciprocal economy''
forms. That is, there is a set of tools good for doing certain jobs, and
there is a set of jobs requiring certain tools. The Principle of Least
Effort asserts that a person attempting to apply a tool to a job does so
in order to minimize the probable effort in using that tool for that
particular job.


In applying this principle to texts, Zipf makes an
important correspondence - words also work as tools, accomplishing jobs
we need done. To simplify the situation greatly, imagine that the job an
author is attempting to accomplish is simply to ``point'' to some
``referent,'' something in the world. Authors would find it most
convenient to simply use one word all the time for all the jobs they are
trying to accomplish. It makes their task much easier; picking the right
word is effortless. The author has a pressure towards {\em unification}
of the vocabulary.


From the reader's point of view, it would be least
ambiguous if a completely unique term was used for every possible
function, every possible interpretation, every meaning. Readers
therefore have a pressure towards {diversification} of the vocabulary.
This leads to the VOCABULARY BALANCE we observe in Zipf's rule.
ZIpf hypothesized that interplay between the forces of diversification
and unification results in the use of existing words, which do not
extend the vocabulary, in most situations, together with the inclusion
of new words in those novel situations that demand them. The trick is to
find an ``economy of language'' that best satisfies both writer and
reader. Note that the maintance of the balance requires, however, that
authors receive {\em feedback} from their readers, confirming that they
are both ``pointing'' to the same referent.


Blair has extended Zipf's
analysis, considering Zipf's tool/job setting as it's applied to our FOA
task [REF704] [<A
HREF="bibrefs.html#Blair92">Blair92] . He argues that one of the
primary reasons FOA systems fail is that the vocabulary balance is
upset. The system of descriptors indexing the authors' works (viz., the
library, or the Web), standing between the authors who are writing the
books and the searchers attempting to find them {\em breaks the feedback
channel} keeping their shared vocabulary in balance when author and
reader are in direct contact.




Top of Page

 | UP: Looking for meaning in all the wrong places \\         (at the character level)

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2.1.2 Benoit Mandelbrot's explanation





FOA Home

 | UP: Looking for meaning in all the wrong places \\         (at the character level)



Benoit Mandelbrot's explanation

The early days of cybernetics were heady, and Zipf was not alone in
seeking a grand, unifying theory that might explain the phenomena of
communication on computational grounds like those proving so successful
in physics. Benoit Mandelbrot was equally ambitious.


Mandelbrot's
background as a physicist is clear when he considers the message decoder
as a physical piece of apparatus, ``... cutting a continuous incoming
string of signs into groups, and recoding each group separately.'' This
``differentiator'' complements an ``integrator'' which reconstitutes new
messages from individual words. Within this model communication can be
considered ``fully analogous to the perfect gas of thermodynamics.''
Minimizing the cost of transmission corresponds to minimization of free
energy in thermodynamics.


Mandelbrot was also interested in how the
critical parameter \alpha \) varied from one vocabulary to another.
Extending the physical analogy of thermodynamic energy, the
``informational temperature'' or ``temperature of discourse'' is
proportional to \( 1/\alpha \), which Mandelbrot argues provides a much
better measure of the richness of a vocabulary than simply counting the
number of words it contains.


The value 1/\alpha \) can also be used to
relate our analysis of Zipf's Law to Mandelbrot's fractals. If the
letters of our alphabet are imagined to be digits of numbers base n + 1,
and a leading decimal point is placed before each word. Then each word
corresponds to a number between 0 and 1. \bq The construction amounts in
effect to cutting out of [0, 1] all the numbers that include the digit 0
otherwise than at the end. One finds that the remainder is a Cantor
dust, the fractal dimension of which is precisely \( 1/\alpha \) . []
(p. 346) [] (p. 346) ] (p. 346) (p. 346) . 346) 346) 46) ) \eq


Mandelbrot
proposed a more general form of Zipf's Law: F(r)={C\over{(r+b)^\alpha}}
that has proved important to analysis of the relationship between word
frequencies and their rank (cf. Section <A
HREF="foa-3-2-1-1.html">&sect;3.2.1.1 ).


Mandelbrot also suggested
how this model might be applied within a model of cognition: Whatever
the detailed structure of the brain it recodes information many times.
The public representation through phonemes is immediately replaced by a
private one through a string of nerve impulses.... This recorded message
presumably uses fewer signs than the incoming one; therefore when a
given message reaches a higher level it will have been reduced to a
choice between a few possibilities only without the extreme redundancy
of the sounds. The last stages are ``idea'' stages, where not only the
public representation has been lost, but also the public elements of
information. (p. 488- 489) \eq He also makes other provocative
suggestions, for example that schizophrenics provide the best test of
his theory since these individuals impose fewest ``semantic
constraints'' on the random process (generating language) of interest?!


While
unsuccessful at his more ambitious goals of wedding a physical model of
communcation to models of semantics such as Saussure's, Mandelbrot was
probably the first to characterize the real truth underlying Zipfian
distributions. Before considering this derivation, one more historical
perspective, due to Herbert Simon, will be mentioned.




Top of Page

 | UP: Looking for meaning in all the wrong places \\         (at the character level)

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2.1.3 Herbert Simon's explanation





FOA Home

 | UP: Looking for meaning in all the wrong places \\         (at the character level)



Herbert Simon's explanation

Simon considered a much different model, focused on the author's
activity constructing a text. Simon's model is based on two assumptions.
First, that new words --- NEOLOGISMS --- introduced by the author
and not previously used, are introduced at some constant probability.
Second, that the probability of a word having occurred exactly $i$ times
is proportional to the total number of occurrences of all words that
have appeared exactly $i$ times. These assumptions allow Simon to use
the basic mathematics of ``pure birth processes'' to account for word
frequency rankings.


Simon was interested in models for the
``associative'' processes underlying authors' cognition ``... sampling
earlier segments of the word sequence'' [p. 434] as they compose. He
acknowledged that authors also use processes of ``imitation: sampling
segments of word sequences from other works of other authors, and, of
course, from sequences he has heard.'' Simon imagined this model as
potentially applying to three different distributions: \item the
distribution of word frequencies in the whole historical sequence of
words that constitute a language; \item the distribution of word
frequencies in a continuous piece of prose; \item the distribution of
word frequencies in a sample of prose assembled from compositive
sources. He seems most engaged by the second of these alternatives,
considering the activity of a particular author. (He uses James Joyce's
{\em Ulysses} as an example.)


Obviously this word-based model provides a
much different explanation of Zipf's Law than Mandelbrot's and Miller's
character-based ones. Simon was familiar with such models, but argues
[Simon55] that his own ``averaging
rather than [Miller's] maximizing assumptions'' are more desirable. But
as Miller notes, ``The assumption of maximization can be replaced by the
assumption of random spacing.'' Worse, in terms of the empirical
bottom-line, Simon's equation does not fit available data as well.




Top of Page

 | UP: Looking for meaning in all the wrong places \\         (at the character level)

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2.1 Looking for meaning in all the wrong places \\         (at the character level)





FOA Home

 | UP: Rembember Zipf



Looking for meaning in all the wrong places \\         (at the character level)

The ubiquity of data obeying Zipf's Law has made it a lightning rod,
attracting a number of ``explanations.'' More, these explanations come
from an extremely impressive set of original thinkers, from widely
ranging disciplines: \item Noam Chomsky, the most influential linguist
of the last 30 years; \item George Miller, the mathematical psychologist
famous for such insight as the ``$7\pm2$ chunks'' of memory limitation;
\item Herbert Simon, the Nobel Prize winning economist and one of the
fathers of artificial intelligence; \item Benoit Mandelbrot, the
computational physicist most famous for his work on fractals.


 Herbert
Simon, a keen observer of much cognitive activity, suggests the ubiquity
of Zipf's Law across heterogeneous collections should make us somewhat
suspicious of its ability to address the ``fine structure'' of
linguistics: No one supposes that there is any connection between horse
kicks suffered by soldiers in the German army and blood clots on a
microscope slide other than that the same urn scheme provides a
satisfactory abstract model of both phenomena. It is in the same
direction that we shall look for an explanation of the observed close
similarities among the five classes of distributions listed above. [<A
HREF="bibrefs.html#Simon55">Simon55] \eq (With ``urn'' Simon is
referring to mathematical models, e.g., related to Poisson
processes.\dhfoot{Poisson processes} See <A
HREF="foa-3-2-1-3.html">&sect;3.2.1.3 for more on the ``five
classes'' of Simon's models.)


We therefore begin this section by
reviewing a number of early attempts to explain the phenomena underlying
ZIpf's Law; its mathematical derivation is reserved for Chapter 5.

Subsections

	 3.2.1.1 Zipf's own explanation
	 3.2.1.2 Benoit Mandelbrot's explanation
	 3.2.1.3 Herbert Simon's explanation




Top of Page

 | UP: Rembember Zipf

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2.2 More recent Zipfian sightings





FOA Home

 | UP: Rembember Zipf



More recent Zipfian sightings

The debate concerning these models date back almost 40 years, but
Zipfian distributions and attempts to explain them contine to arise. For
example, many have been struck by language-like properties exhibited by
the long sequences of {genetic} codes found in all living species' DNA.
That a simple ``alphabet'' of four nucleic acid BASE-PAIRS (BPs)
({\tt A,C,G,T} in DNA) are broken into three-letter CODONS that
mean one of twenty possible ``words'' corresponding to amino
acids has lead many to wonder what we might learn by viewing the genome
as a linguistic object [Sereno91] .


Mantegna
et al. [Mantegna94] was led to
consider the ``word'' frequency distributions of such words in the DNA
``corpus.'' Further, they considered differences in the distributions
across coding regions of the genome as well as non-coding regions that
never are expressed. Their first result is that this sequence data does
indeed contain ``linguistic features,'' especially in the non-coding
regions. By Analyzing various genentic corpora (e.g., approximately one
million BPs taken from 14 mammalian sequences), they found that, in
contrast to what we might expect of completely random sequences, the
rank-frequency distribution of six-BP words could be well fit by a
(log-log linear) Zipf exponent= -0.28. They conclude: \bq These results
are consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences.
These results are consistent with the possible existence of one (or more
than one) structured biological languages present in non-coding DNA
sequences. These results are consistent with the possible existence of
one (or more than one) structured biological languages present in
non-coding DNA sequences. ese results are consistent with the possible
existence of one (or more than one) structured biological languages
present in non-coding DNA sequences. e results are consistent with the
possible existence of one (or more than one) structured biological
languages present in non-coding DNA sequences. results are consistent
with the possible existence of one (or more than one) structured
biological languages present in non-coding DNA sequences. sults are
consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences. lts
are consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences. s
are consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences. are
consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences. e
consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences.
consistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences.
nsistent with the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences.
istent with the possible existence of one (or more than one) structured
biological languages present in non-coding DNA sequences. tent with the
possible existence of one (or more than one) structured biological
languages present in non-coding DNA sequences. nt with the possible
existence of one (or more than one) structured biological languages
present in non-coding DNA sequences. with the possible existence of one
(or more than one) structured biological languages present in non-coding
DNA sequences. ith the possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences. h
the possible existence of one (or more than one) structured biological
languages present in non-coding DNA sequences. the possible existence of
one (or more than one) structured biological languages present in
non-coding DNA sequences. e possible existence of one (or more than one)
structured biological languages present in non-coding DNA sequences.
possible existence of one (or more than one) structured biological
languages present in non-coding DNA sequences. ssible existence of one
(or more than one) structured biological languages present in non-coding
DNA sequences. ible existence of one (or more than one) structured
biological languages present in non-coding DNA sequences. le existence
of one (or more than one) structured biological languages present in
non-coding DNA sequences. existence of one (or more than one) structured
biological languages present in non-coding DNA sequences. xistence of
one (or more than one) structured biological languages present in
non-coding DNA sequences. stence of one (or more than one) structured
biological languages present in non-coding DNA sequences. ence of one
(or more than one) structured biological languages present in non-coding
DNA sequences. ce of one (or more than one) structured biological
languages present in non-coding DNA sequences. of one (or more than one)
structured biological languages present in non-coding DNA sequences. f
one (or more than one) structured biological languages present in
non-coding DNA sequences. one (or more than one) structured biological
languages present in non-coding DNA sequences. one (or more than one)
structured biological languages present in non-coding DNA sequences. one
(or more than one) structured biological languages present in non-coding
DNA sequences. e (or more than one) structured biological languages
present in non-coding DNA sequences. (or more than one) structured
biological languages present in non-coding DNA sequences. r more than
one) structured biological languages present in non-coding DNA
sequences. more than one) structured biological languages present in
non-coding DNA sequences. re than one) structured biological languages
present in non-coding DNA sequences. than one) structured biological
languages present in non-coding DNA sequences. han one) structured
biological languages present in non-coding DNA sequences. n one)
structured biological languages present in non-coding DNA sequences.
one) structured biological languages present in non-coding DNA
sequences. e) structured biological languages present in non-coding DNA
sequences. structured biological languages present in non-coding DNA
sequences. tructured biological languages present in non-coding DNA
sequences. uctured biological languages present in non-coding DNA
sequences. tured biological languages present in non-coding DNA
sequences. red biological languages present in non-coding DNA sequences.
d biological languages present in non-coding DNA sequences. biological
languages present in non-coding DNA sequences. ological languages
present in non-coding DNA sequences. ogical languages present in
non-coding DNA sequences. ical languages present in non-coding DNA
sequences. al languages present in non-coding DNA sequences. languages
present in non-coding DNA sequences. anguages present in non-coding DNA
sequences. guages present in non-coding DNA sequences. ages present in
non-coding DNA sequences. es present in non-coding DNA sequences.
present in non-coding DNA sequences. resent in non-coding DNA sequences.
sent in non-coding DNA sequences. nt in non-coding DNA sequences. in
non-coding DNA sequences. n non-coding DNA sequences. non-coding DNA
sequences. non-coding DNA sequences. non-coding DNA sequences. n-coding
DNA sequences. coding DNA sequences. ding DNA sequences. ng DNA
sequences. DNA sequences. NA sequences. sequences. equences. uences.
nces. es. . \eq


Subsequent analysis, however, makes it quite clear that
any such interpretations are ill-founded [<A
HREF="bibrefs.html#Bonhoeffer96">Bonhoeffer96] . Deviations from
fully random sequence behavior can be attributed to two simple
characteristics of biological sequence data. First, define $H(n)$ to be
the entropy of the distribution of $n$-length nucleotides sequences.
Then the redundancy $R(1)$ of length $n=1$ words is: R(n) = 1 -
\frac{H(n)}{2n} $R(1)$ then reflects a simple increase with the {\em
variance} of the four base pairs; but the fact that the bases occur with
much different frequencies is a well-known biolgical fact. Second, very
short range correlations between nucleic acids (which are very easy to
imagine given the basic three letter genetic code) and the fact that in
DNA the most common words are simply combinations of the most probable
letters because recombinations events cross over, especially in regions
of short repeats like this. There are still interesting questions (e.g.,
why coding and non-coding regions differ in their nucleic acid
frequencies) but does undermine any large scale language-like properties
within DNA sequence.


A final, very recent example of how Zipf-like
distributions arise is offered by analyses of WWW SURFING
behaviors [Huberman98] , and makes
this same point (but cf. Section &sect;8.1
for more recent, apparently contradictory data generated from massive
AltaVista logs). Consider each page click by a browsing user to be a
character, and the amount of time spent by the same user on the same
host to be the length of a ``word.'' Then (surprise!), empirical data
capturing the rank-frequency distribution of each WWW surfing ``ride''
again shows a (log-log linear) Zipfian relationship with slope equal to
-1.5, as shown in Figure (figure) .


Huberman et al. also propose
a model explaining this empirical data. Assume that the ``value'' (what
we might think of as perceived relevance) $V(L)$ of each page in a
browsing sequence of length $L$ goes up or down according to identical,
independently distributed (iid) Gaussian random variables $\[ V(L) =
V(L-1) + \epsilon_L \] Using economic reasoning, Huberman et al. then
hypothesize: \bq ... an individual will continue to surf until the
expected cost of continuing is perceived to be larger than the
discounted expected value of the information to be found in the
future.... Even if the value of the current page is negative, it may be
worthwhile to proceed, because a collection of high value pages may
still be found. If the value is sufficiently negative, however, then it
no longer worth the risk to continue. \eq If users's browsing behaviors
follow a random walk governed by these consideration, Huberman et al.
show that the passage times to this cutoff threshold is given by the
inverse Dousian distribution: \Pr (L)=\sqrt{\frac{\lambda }{2 \pi
L^{3}}} \exp \left[ \frac{-\lambda (L-\mu )^{2}}{2\mu ^{2}L}\right]
\label{eq:websurf} where $\mu $ is the mean of the random walk length
variable $L$, $\mu ^{3}/\lambda $ is its variance and $\lambda $ is a
scaling parameter.




Top of Page

 | UP: Rembember Zipf

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2.3 Summary





FOA Home

 | UP: Rembember Zipf



Summary

This historical background is provided because the stories of these
eminent scientists stories remain compelling. The plausibility of the
proposed theories, coupled with our retrospective knowledge of their
incorrectness, also provides a sobering background as we attempt to
infer semantic properties from the statistics arising in FOA. As we
shall discuss in Chapter 5 (cf. Section <A
HREF="foa-5-1.html">&sect;5.1 ), the real basis of Zipf's Law can be
traced to much simpler mechanisms relating only to patterns of
characters rather than any underlying semantics or purposes, is
sufficient to derive this generalized form of Zipf's Law. Mandelbrot,
and then George Miller and Noam Chomsky, have shown that the underlying
phenomena relating a word's frequency to its rank order is obeyed as
much by random text -- generated by monkeys at typewriters, for
example -- as it is by other samples of text (the Bible, James
Joyce's Ulysses, etc.) we tend to find more literate.


The fact
that the simple, four-character sequence {ZIPF} should bring together
such a rich combination of mathematical and semantic issues is ironic to
say the least. There is obviously a great deal we can predict about our
language by assuming nothing more than we would about monkeys at
keyboards. At the same time, the fact that we can change the
meaning of a simple sequence of characters, for example the title
of this section {\tt REMEMBER ZIPF}, so dramatically by adding a single
additional character to form either {\tt REMEMBER ZIPF!} or {\tt
REMEMBER ZIPF?} should also make it clear how much more there is still
to say.




Top of Page

 | UP: Rembember Zipf

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.2 Rembember Zipf





FOA Home

 | UP: Weighting and matching against Indices



Rembember Zipf

Looking at our corpus as a very long string of characters, something
that even a monkey could generate, provides a useful baseline against
which we can evaluate larger constructs.


Associate with each word $w$ its
frequency $F(w)$, the number of times it occurs anywhere in the corpus.
Now imagine that we've sorted the vocabulary according to frequency, so
that the most frequently occurring word will have rank $r=1$, the next
most frequent word will have $r=2$, and so on.


George Kingsley Zipf
(1902-1950) has become famous for noticing that the distribution we find
true of our corpus is in fact very reliably true of any large sample of
natural language we might consider. Zipf [<A
HREF="bibrefs.html#REF323">REF323] observed that the words'
rank-frequency distribution can be fit very closely by the relation:


This
empirical rule is now known as Zipf's Law. But why should this pattern
of word usage, something we can reasonably expect to vary with author or
type of publication, be so universal?! Even more, the notion of ``word''
used in this formula has also varied radically - in tabulations of word
frequencies by Yule and Thorndike, words were stemmed to their root
form; Yule counted only nouns [Yule24]
[Thorndike37] . Dewey [<A
HREF="bibrefs.html#Dewey29">Dewey29] and Thorndike collected
statistics from multiple sources, others were collected from a single
work (for example, James Joyce's {\em Ulysses\/}). The frequency
distribution for a small subset of (non-noise words in) our AIT corpus
is shown in Figure (figure) . Note the nearly linear ,
negatively-sloped relation when frequency is plotted as a function of
rank, and both are plotted on log scales.

Subsections

	 3.2.1 Looking for meaning in all the wrong places \\         (at the character level)
	 3.2.2 More recent Zipfian sightings
	 3.2.3 Summary




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.1 Lexical consequences, internal/external perspectives





FOA Home

 | UP: A statistical basis for keyword meaning



Lexical consequences, internal/external perspectives

The plot in Figure (FOAref) is based on word-frequency
statistics like those shown in Table (FOAref) . Note that on
this log-log plot, frequency is a nearly linear inverse function of
rank.


One way to make the various lexical decisions considered in the
last chapter is to consider the effects of various decisions in terms of
statistics such as these. Table (FOAref) shows the statistics
for stemmed, non-noise word tokens (shown in {\tt \\tt}, eg
SYSTEM) we typically assume together with noise words
(shown in italics, eg the). As expected, the noise
words are very frequent. But it is interesting to contrast those very
frequent words defined a priori in the NEGATIVE
DICTIONARY from those that are especially frequent in this
particular corpus. In many ways these are excellent candidates for
EXTERNAL KEYWORDS : characteristizations of this corpus' content,
from the ``external'' perspective of general language use. That is,
these are exactly the words (cf. NEURAL NETWORK, BASE, LEARN,
WORLD, KNOWLEDGE) that could suggest to a browsing WWW user that
the AIT corpus might be worth visiting. Once ``inside'' the topical
domain of AI, however, these same words become as ineffective as other
noise-words as INTERNAL KEYWORDS , discriminating the contents of
one AI thesis dissertation from the next (cf. SYSTEM,
MODEL,PROCESS, DESIGN).


Also shown in (FOAref) are
statistics both with and without stemming. For example, the token
SYSTEM itself appeared only 8632 times; variations like
SYSTEMS, SYSTEMATIC, etc. must account for the
other 12856. This simple example also demonstrates how issues of phrase
recognition (cf. NEURAL NETWORK), and other, messy issues
(e.g., the presence of French noise words in some of the
dissertation abstracts but not in our English negative dictionary!) can
arise in even the simplest, ``cleanest'' corpora.




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.2 Word occurrence as a  Poisson process





FOA Home

 | UP: A statistical basis for keyword meaning



Word occurrence as a  Poisson process

\newcommand{\prob}{probability\ }


When the words contained in a corpus
are ranked and shown to be distributed according to a Zipfian
distribution, an obvious but important observation can be made: The most
frequently occurring words are not really about anything. Words
like NOT, OF, THE, OR, TO, BUT, BE, etc., obviously play an
important {\em functional} role, as part of the syntactic structure of
sentences. But it is hard to imagine users asking for documents
about OF, or about BUT. {A query
like {\tt TO BE OR NOT TO BE} is not hard to imagine, however, and leads
to special retrieval techniques like PAT-trees that allow indexing of
each and every word; cf. Section <A
HREF="foa-2-5-2.html">&sect;2.5.2 .} Define FUNCTION WORDS to
be those that have only (!) a syntactic function, for example \term{OF},
\term{THE}, \term{BUT} and distinguish them from CONTENT WORDS
which are descriptive in the sense that we're interested in for the
indexing task. This is one of the first but most certainly not the last
example FOA makes using a priori determinations of a word's
semantic utility based on its statistical properties.


For
example, we might hope that function words occur {randomly} throughout
arbitrary text, while content words do not. One ubiquitous model of
randomness is as a POISSON PROCESS , used in the past to model
things like: \bit \item raisins' distribution across slices of bread; or
\item misprints' distribution across printed pages; or \item the
distribution of peoples' birthdays across days of the year. distribution
of peoples' birthdays across days of the year. stribution of peoples'
birthdays across days of the year. ibution of peoples' birthdays across
days of the year. tion of peoples' birthdays across days of the year. n
of peoples' birthdays across days of the year. f peoples' birthdays
across days of the year. eoples' birthdays across days of the year. les'
birthdays across days of the year. ' birthdays across days of the year.
irthdays across days of the year. hdays across days of the year. ys
across days of the year. across days of the year. ross days of the year.
s days of the year. ays of the year. of the year. the year. e year. ear.
. \eit


In the case of our documents, we'll start with a slightly simpler
BERNOULLI model wherein we imagine an author making
binary decisions, picking a keyword $k$ with \prob $p_k$. Then
in a document of length $L$ the \prob that it was selected exactly $n$
times in document $d$ is: \Pr(f_{kd} = n) = {L \choose n} (p_k)^n
(1-p_k)^{L-n} In other words, we'd expect it to occur an average of $p_k
\cdot L$ times in a document of length $L$.


As $L \infty$ and $p
\rightarrow 0$ (and the mean value $\lambda \equiv p \cdot L \rightarrow
1$), the Poisson distribution: \Pr(f_{kd} = n) = \frac{e^{- \lambda}
(\lambda)^{n}}{n!} converges to this same distribution. We will
generally be interested in a large set of parameters $\lambda_k$, each
corresponding to a particular keyword $k$. If we imagine a
Bernoulli-like experiment where individual function words are placed
with low probability and observed across the many ``experiments'' of
words occurring in documents, we can expect that a particular word \( k
\) will occur \( n \) times in a randomly selected document according to
a Poisson distribution (Since documents are of different length, we must
also take care to normalize them all to the same number of experiments.)


As
an example of how a Poisson model might be applied to good use, work
pioneered by Bookstein and Swanson in the mid-1970's proposed that
function words are distributed according to a relatively constant
Poisson distribution [<A
HREF="bibrefs.html#Bookstein74">Bookstein74] [<A
HREF="bibrefs.html#Bookstein77">Bookstein77] [<A
HREF="bibrefs.html#Croft79">Croft79] while content words were not.
That is, when a keyword is found in a document, it is for one of two
possible reasons: either it just happens (randomly) to be there, or it
really means something. Robertson and Walker [<A
HREF="bibrefs.html#Robertson94">Robertson94] distinguish the latter,
ELITE occurrences of a keyword: \bq We hypothesize that
occurrences of a term in a document have a random or stochastic element,
which nevertheless reflects a real but hidden distinction between those
... ``elite'' documents which are about the concept represented
by the term and those which are not. We may draw an inference about
eliteness from the term frequency, but this inference will of course be
probabilistic. Furthermore, relevance (to a query which may of course
contain many concepts) is related to eliteness rather than directly to
term frequency, which is assumed to depend only on eliteness.
[Robertson94] \eq


Beyond
discriminating function from content words, the Poisson model has been
used to measure the degree to which a content word is effective
as a keyword for a document [<A
HREF="bibrefs.html#Robertson94">Robertson94] . If we assume that a
potential keyword effectively describes some documents in a corpus but
occurs at the level of chance throughout the rest of the corpus, the
distribution of this keyword across the corpus can be described as the
mixture of a Poisson process with some other distribution.


The so-called
TWO-POISSON MODEL models both distributions (i.e., one over the
\Rel documents that could accurately be characterized as about
this keyword, and a second over the rest of the \IRel documents which
are not) as Poisson, but with distinct means \( \lambda_w^{1} \) and \(
\lambda_w^{2} \), with the superscripts 1 and 2 refering to the \Rel and
\IRel distributions, resp. One advantage of assuming both distributions
are Poisson, and that we only need to discriminate between two classes
(relevant vs. nonrelevant), is that a single parameter \( p_{rel}
\equiv\Pr(\mathname{Relevance})\) controls the probability that the word
\( w \) is relevant: \Pr(d {\textstyle \about} w \mid k {\textstyle
occurrences of} w = \frac{p_{rel} e^{-\lambda_w^{1}}
(\lambda_w^{1})^{k}} {p_{rel} e^{-\lambda_w^{1}} (\lambda_w^{1})^{k} +
(1 - p_{rel}) e^{-\lambda_w^{2}} (\lambda_w^{2})^{k}} This probability
can then be used as part of a decision theoretic model related to the
costs of indexing too many or too few documents with a keyword \( w \)
(cf. Section &sect;5.5.6 ).




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.3 Resolving power





FOA Home

 | UP: A statistical basis for keyword meaning



Resolving power

Zipf observed that the frequency of words' occurrence varies
dramatically, and Poisson models explore deviations of these occurrence
patterns from purely random processes. We now make the first important
move towards a theory of why some words occur more frequently
and how such statistics can be exploited when building an index
automatically. Luhn, as far back as 1957, said clearly: It is hereby
proposed that the frequency of word occurrence in an article furnishes a
useful measurement of word significance. [<A
HREF="bibrefs.html#Luhn57">Luhn57] That is, if a word occurs
frequently, more frequently than we would expect it to within a corpus,
then it is reflecting emphasis on the part of the author
about that topic. But the raw frequency of occurrence in a
document is only one of two critical statistics recommending good
keywords.


Consider a document taken from our AIT corpus, and imagine
using the keyword with it. By construction, virtually every document in
the AIT is about \term{ARTIFICIAL INTELLIGENCE}!? Assigning the
keyword \term{ARTIFICIAL INTELLIGENCE} to any document in AIT would be a
mistake, not because this document isn't about \term{ARTIFICIAL
INTELLIGENCE}, but because this term can not help us {\em discriminate}
one subset of our corpus as relevant to any query. If we change our
search task to looking not only in our AIT corpus but through a much
larger collection (for example, all computer industry newsletters) then
associating \term{ARTIFICIAL INTELLIGENCE} with those articles in our
AIT subcorpus becomes a good idea. This term helps to distinguish AI
documents from others.


The second critical characteristic of good indices
now becomes clear - a good index term not only characterizes a document
{absolutely}, as a feature of a document in isolation, but also allows
us to discriminate it {\em relative} to other documents in the corpus.
Hence keywords are not strictly properties of any single document, but
reflect a relationship between an individual document and the collection
from which it might be selected.


These two, countervailing considerations
suggest that the best keywords will not be the most ubiquitous,
frequently occurring terms, nor those that occur only once or twice, but
rather those occurring a moderate number of times. Using Zipf's rank
ordering of words as a baseline, Luhn hypothesized a modal function of a
word's rank he called RESOLVING POWER centered exactly at the
middle of this rank ordering. If resolving power is defined as a word's
ability to {\em discriminate} content, Luhn assumed that this quantity
is maximal at the middle and then falls off at either very high or very
low frequency extremes, as shown in Figure (figure) . The next
step is then to establish maximal and minimal occurrence
thresholds defining useful, mid-frequency index terms.
Unfortunately, Luhn's view does not provide theoretical grounds for
selecting these bounds, and so we are reduced to the engineering task of
tuning them for optimal performance.


We'll begin with the
maximal-frequency threshold, used to exclude words that occur too
frequently. For any particular corpus, it is interesting to contrast
this set of most-common words with the negative dictionary of noise
words, defined in Section &sect;2.3.2 .
While there may often be great overlap, the negative dictionary list is
typically a list that has proven itself to be practically useful across
many different corpora, while the most frequent tokens in a particular
corpus may be quite specific to it.


Establishing the other, low-frequency
threshold is less intuitive. Assuming that our index is to be of limited
size, including a certain keyword means we must exclude some other. This
suggests that a word that occurs in exactly one document can't possibly
be used to help discriminate that document from others regularly. For
example, imagine a word -- suppose it is -- that occurs exactly once, in
a single document. If we took out that word \term{DERIVATIVE} and put in
any other word, for example \term{FOOBAR}, in terms of the word
frequency co-occurrence statistics that are the basis of all our
indexing techniques, the relationship between that document and all the
other documents in the collection will remain unchanged! In terms of
overlap between what the word \term{DERIVATIVE} \means, in the FOA sense
of what this and other documents are \about, a single word occurrence
has no rd occurrence has no ce has no \rikmeaning!


The most useful words
will be those that are not used so often as to be roughly common to all
of the documents, and not so rare so as to be (nearly) unique to any one
(or small set) of documents. We seek those keywords whose
combinatorial properties, when used in concert with one another
as part of queries, help to compare and contrast topical areas of
interest against one another.




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.4 Language distribution





FOA Home

 | UP: A statistical basis for keyword meaning



Language distribution

We next move beyond characteristics of single keywords, to an analysis
of the distribution of the entire set of index terms. Any
index, whether constructed manually or automatically based on word
frequency patterns, is defined by a tension between EXHAUSTIVITY
on the one hand and SPECIFICITY on the other. An index is
exhaustive if it includes many topics. It is specific if users can
precisely identify their information needs.


Unfortunately, these two
intuitively reasonable desiderata are in some senses at odds
with one another, as suggested by Figure (figure) . The best
explanation of this trade-off is in terms of precision and recall (cf.
Section &sect;4.3.4 ): High recall is
easiest when an index is exhaustive but not very specific; high
precision is best accomplished when the index is not very exhaustive but
highly specific. If we assume that the same index must serve many users,
each with varying expectations regarding the precision and recall of
their retrieval, the best index will be at some balance point between
these goals.


If we index a document with many keywords, it will be
retrieved more often; hence we can expect higher recall, while precision
may suffer. van Riesbergen has talked about this extreme as a
``document'' orientation, or REPRESENTATION BIAS , \vanR{24, 29}.
A document-oriented approach to index-building focuses the system
builder's attention on a careful representation of each document, based
on an analysis of what it is document-oriented approach to
index-building focuses the system builder's attention on a careful
representation of each document, based on an analysis of what it is
nt-oriented approach to index-building focuses the system builder's
attention on a careful representation of each document, based on an
analysis of what it is ented approach to index-building focuses the
system builder's attention on a careful representation of each document,
based on an analysis of what it is approach to index-building focuses
the system builder's attention on a careful representation of each
document, based on an analysis of what it is ch to index-building
focuses the system builder's attention on a careful representation of
each document, based on an analysis of what it is index-building focuses
the system builder's attention on a careful representation of each
document, based on an analysis of what it is building focuses the system
builder's attention on a careful representation of each document, based
on an analysis of what it is ng focuses the system builder's attention
on a careful representation of each document, based on an analysis of
what it is uses the system builder's attention on a careful
representation of each document, based on an analysis of what it is he
system builder's attention on a careful representation of each document,
based on an analysis of what it is tem builder's attention on a careful
representation of each document, based on an analysis of what it is
uilder's attention on a careful representation of each document, based
on an analysis of what it is 's attention on a careful representation of
each document, based on an analysis of what it is ention on a careful
representation of each document, based on an analysis of what it is on a
careful representation of each document, based on an analysis of what it
is careful representation of each document, based on an analysis of what
it is l representation of each document, based on an analysis of what it
is esentation of each document, based on an analysis of what it is tion
of each document, based on an analysis of what it is f each document,
based on an analysis of what it is document, based on an analysis of
what it is ent, based on an analysis of what it is based on an analysis
of what it is on an analysis of what it is analysis of what it is is of
what it is what it is t is \about.


But an index's fundamental purpose is
to reconcile a corpus' many document descriptions with the many users'
queries that can be expected to come. We could equally well analyze the
problem from a QUERY-ORIENTED perspective - how well do the query
terms discriminate one document from another?


From the users'
perspective, we'd like to have these queries match meaningfully onto the
vocabulary of our index. From the perspective of the corpus, we'd like
to be able to discriminate documents one from another. These are quite
different perspectives on an index, and reflect a fundamental
VOCABULARY MISMATCH , between the way users describe their
interests and the way documents have been described.


If an indexing
vocabulary is specific, then a user should expect that just the right
keyword in a MAGIC BULLET QUERY will elicit all and only relevant
documents. The average number of documents assigned to specific keywords
should be low. In an exhaustive indexing, the many aspects of a document
will each be reflected by expressive keywords; on average many keywords
will be assigned to a document: Exhaust & \propto & \langle
\frac{kw}{doc} \rangle \nonumber \\ Spec & \propto^{-1} & \langle
\frac{doc}{kw} \rangle \nonumber \\ & \propto & \langle \frac{kw}{doc}
\rangle


 The important observation is that these two averages must be
taken across much different distributions. We already know from Zipf's
Law that the number of occurrences varies dramatically from one keyword
to another. Once we make an assumption about how keywords occur within
separate documents, we can derive the distribution of keywords across
documents. But the distribution of keywords assigned to documents can be
expected to be much more uniform - documents are about a nearly
unform or constant number of topics. Figure (figure) represents
the index as a graph, where edges connect keyword nodes on the left with
document nodes on the right. The \mathname{Index} graph is a bi-partite
graph, with its nodes divided into two subsets (keywords and documents),
and nodes in one set having only connections with those in the other. If
we assume that the total number of edges must remain constant, we can
assume that the total area under both distributions is the same. The
quantity capturing the exhaustivity/specificity trade-off is therefore
the ratio of $Vocab$ to corpus size $\mathname{NDoc}$.


While this
analysis is crude, it does highlight two important features of every
index. First, in most applications $$ is fixed while $\mathname{Vocab}$
is a matter of discretion, a free variable which can be tuned to
increase or decrease specificity/exhaustivity. Second, certainly in most
modern applications (i.e., with the huge disk volumes now common),
$\mathname{NDoc} \gg \mathname{Vocab}$. This is one of the most
important ways in which experimental collections (including AIT!) differ
from real corpora. A useful indexing vocabulary can be expected to be of
a relatively constant size, $\mathname{Vocab} \approx 10^{3}-10^{5}$
while corpora sizes are likely to vary dramatically: $\mathname{NDoc}
\approx 10^{4}-10^{9}$.


Along similar lines, it is always useful to think
about what this means in the context of the WWW, where the notion of a
closed corpus disappears. The WWW is an organic, constantly and
growing set of documents; our vocabulary for describing it is more
constrained.


Several other basic features of the Index are shown in
Figure (FOAref) . The flipped histogram along the left side is
meant to reflect the Zipfian distribution over keywords, with the most
frequent keywords beginning at the top. Recall that this distribution
captures the total number of word occurrences, regardless of
how these occurrences happen to be distributed across inter-document
boundaries. A second distribution is also sketched, suggesting how the
number of documents versus word occurrences might also be
distributed - we can expect these two quantities to be at least loosely
correlated. The distinction between intra- and inter-document word
frequencies is a topic we return to in Section <A
HREF="foa-3-3-7.html">&sect;3.3.7 .




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.5 Weighting the index relation





FOA Home

 | UP: A statistical basis for keyword meaning



Weighting the index relation

The simplest notion of an index is binary - either a keyword is
associated with a document or it is not. But it is natural to imagine
degrees of \about-ness. We will capture this strength with a
single real number, a {\em weight}, capturing the strength of the
relationship between keyword and document. This weight can be used in
two different ways. One example is to reduce the number of links to only
the most significant relationships. In this respect a weighted indexing
system is a more general formulation than a binary formulation; we can
always go to a binary relation from the weighted one. This might make
weights useful even if our retrieval method was Boolean (as it often was
in early IR systems). But today the more common reason for using a
weighted indexing relation is that the retrieval method can exploit
these weights directly.


One way to describe what this number means
is probabilistic - we seek a measure of a document's relevance,
conditionalized on the belief that a keyword is relevant: \[ w_{kd}
\propto \Pr(d \mathrm{\ relevant\ } | \ k \mathrm{\ relevant}) \] Note
that this is a \emph{directed} relation: we may or may not believe that
the symmetric relation: \[ w_{dk} \propto \Pr(k \mathrm{\ relevant\ }\ |
\ d \mathrm{\ relevant}) \] should be the same. Unless specified
otherwise, when we refer to a weight $w$ we will intend it to mean
$w_{kd}$.


In order to compute statistical estimates for such
probabalities we define several important quantities: f_{kd} & \equiv &
\mathrm{number of occurrences of keyword} k \mathrm{in document}d
\nonumber \\ f_{k} & \equiv & \mathrm{total number of occurrences of
keyword} k \mathrm{acros entire corpus} \nonumber \\ D_{k} & \equiv &
\mathrm{number of documents containing keyword} k


 We will make two
demands on the weight reflecting the degree to which a document is
about a particular keyword/topic. The first one goes back to
Luhn's central observation [Luhn61] :
Repetition is an indication of emphasis. If an author uses a
word frequently, it is because he or she thinks it's important. Define
$f_{kd}$ to be the number of occurrences of keyword $k$ in a document
$d$.


Our second concern is that a keyword be a useful
discriminator within the context of the corpus. Capturing this
notion of corpus-context statistically proves much more difficult; for
now we simply give it the name $\mathname{discrim}_k$.


Since we care
about both, we will devise our weight to be the { product} of the two
factors, corresponding to their conjunction: w_{kd} \propto f_{kd} \cdot
\mathname{discrim}_k


 We will now consider several different index
weighting schemes that have been suggested over the years. These all
share the same reliance on $f_{kd}$ as a measure of keyword importance
within the document, and the same product form as Equation
(FOAref) . What they do not share is how best to quantify the
discrimination power $\mathname{discrim}_k_k\ $ of the keyword.




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.6 Informative signals versus noise words





FOA Home

 | UP: A statistical basis for keyword meaning



Informative signals versus noise words

We begin with a weighting algorithm derived from information theory.
Information theory has proven itself to be an extra-ordinarily useful
model of many different situations in which some message must
be communicated across a noisy channel and our goal is to
devise an encoding for messages that is most robust in the face
of this noise.


In our case, we must imagine that the ``messages''
describe the content of documents in our corpus. On this account, the
amount of information we get about this content from a word is
inversely proportional to its probability of occurrence. In other words,
the least informative word in our corpus is the one that occurs
approximately uniformly across the corpus. For example, the word
\term{THE} occurs at about the same frequency across every document in
the collection; its probability of occurrence in any one document is
almost uniform. We gain the least information about the document's
contents from observing it. {Entire courses are given on information
theory so we cannot do it justice here. But its basic features are so
simple, and so important, that it is tempting to try.


My favorite
definition of information is due to Gregory Bateson [<A
HREF="bibrefs.html#REF1008">REF1008] : ``Information is a difference
that makes a difference.'' Information is about surprise, ways in which
an expectation has been violated in some way. If I tell you that your
grade is based on 1) a final and 2) a midterm, you wouldn't be very
surprised. But if I tell you that your grade in this class will also
depend on 3) how long you can stand on one foot without moving, you'd
probably be much more surprised. There's more {\em information} in that
part of my message.


We can demonstrate this in terms of a conversation
you might have after class with someone who missed it. ``What did you
learn in class today?'' they will ask. ``Oh, not much really'' you'd say
in the first case, because your expectation about grading, and your
friend's (not to mention your friend's expectation that you can be
relied upon to convey the information; cf. Section~ <A
HREF="foa-8-2-1.html">&sect;8.2.1 ) have been confirmed. But in the
second case you'd have to reply, ``You won't believe this - part of our
grade is based on how long we can stand on one foot!'' You've learned
something - you've gained information. }


Salton and McGill [<A
HREF="bibrefs.html#">] , following Dennis [<A
HREF="bibrefs.html#Dennis67">Dennis67] , use Shannon's classic
binary logarithm to measure the amount of information conveyed by each
words occurrence in bits, and NOISE to be its inverse: p_k &=&
\Pr(\mathrm{keyword\ } k \mathrm{\ occurs}) \nonumber \\
\mathname{Info}_k &\equiv& -\log\; p_k \nonumber \\ \mathname{Noise}_k
&\equiv& -\log\; (1 / p_k ) \\


 Note that our evidence about the
probability of a keyword occurring comes from
statistics of how frequently it occurs. We must compare how
frequently a keyword occurs in a particular document, relative to how
frequently it occurs throughout the entire collection. We can calculate
the expected NOISE associated with a keyword across the corpus,
and from this infer its remaining SIGNAL . Signal then becomes
another measure we can use to weight the frequency of occurrence of the
keyword document:


Two hypothetical distributions, for a noisy word and a
useful index term, are shown in Figure (figure) . A noisy word
is equally likely to occur anywhere; its distribution is nearly uniform.
If on the other hand all of the occurrences of a keyword are localized
in a few documents (conveniently clustered together in the cartoon of
Figure (FOAref) ) and mostly zero every place else, this is an
informative word. You've learned something about the document's
content when you see it.




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3.7 Inverse document frequency





FOA Home

 | UP: A statistical basis for keyword meaning



Inverse document frequency

Up to this point, we've been concerned only with the total number of
times a word occurs across the entire corpus. Karen Sparck Jones has
observed that, from a discrimination point of view, what we'd really
like to know is the number of documents containing a keyword.
This thinking underlies the INVERSE DOCUMENT FREQUENCY (IDF)
weighting:


The basis for IDF weighting is the observation that people
tend to express their information needs using rather broadly defined,
frequently occurring terms, whereas it is the more specific, i.e.,
low-frequency terms that are likely to be of particular importance in
identifying relevant material. This is because the number of documents
relevant to a query is generally small, and thus any frequently
occurring terms must necessarily occur in many irrelevant documents;
infrequently occurring terms have a greater probability of occurring in
relevant documents --- and should thus be considered as being of greater
potential when searching a database. [<A
HREF="bibrefs.html#SparckJones97">SparckJones97] \eq


Rather than
looking at the raw occurrence frequencies, we will aggregate occurrences
within any document and consider only the { number of documents} in
which a keyword occurs. IDF proposes, again using a ``statistical
interpretation of term specificity'' [<A
HREF="bibrefs.html#SparckJones72">SparckJones72] that the value of a
keyword varies inversely with the $\log$ of the number of documents in
which it occurs: w_{kd}=f_{kd} * \left(\log {\mathname{Norm} \over D_k}
+ 1\right) where $D_{k}$ is defined in Equation (FOAref) .


The
formula in (FOAref) is still not fully specified, in that the
count \( D_{k} \) must be normalized with respect to a constant \(
\mathname{Norm} \). We could normalize with respect to the total number
of documents in the corpus [<A
HREF="bibrefs.html#SparckJones72">SparckJones72] [<A
HREF="bibrefs.html# Croft79"> Croft79] ; another possibility is to
normalize against the maximum document frequency (i.e., the most
documents any keyword appears in) [<A
HREF="bibrefs.html#SparckJones79a">SparckJones79a] [<A
HREF="bibrefs.html#SparckJones79b">SparckJones79b] .



\mathname{Norm}=\left\{ \begin{array}{ll} \mathname{\mathname{NDoc}}
&[SparckJones72] \\ %
\stackunder{k}{\mathname{argmax}} D_k &[<A
HREF="bibrefs.html#SparckJones79">SparckJones79] \\
{\mathname{argmax}_{k}} D_k &[<A
HREF="bibrefs.html#SparckJones79a">SparckJones79a] [<A
HREF="bibrefs.html#SparckJones79b">SparckJones79b] \\ \right.


 Today
the most common form of IDF weighting is that used by Robertson and
Sparck Jones [Robertson76] ,
which normalizes with respect to the number of documents not containing
a keyword $(\mathname{NDoc}- D_k)$ and adds a constant of $\frac{1}{2}$
to both numerator and denominator to moderate extreme values:
w_{kd}=f_{kd} * \left(\log {(\mathname{NDoc}- D_k)+0.5\over D_k +
0.5}\right)




Top of Page

 | UP: A statistical basis for keyword meaning

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.3 A statistical basis for keyword meaning





FOA Home

 | UP: Weighting and matching against Indices



A statistical basis for keyword meaning
Subsections

	 3.3.1 Lexical consequences, internal/external perspectives
	 3.3.2 Word occurrence as a  Poisson process
	 3.3.3 Resolving power
	 3.3.4 Language distribution
	 3.3.5 Weighting the index relation
	 3.3.6 Informative signals versus noise words
	 3.3.7 Inverse document frequency




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.4.1 Keyword discrimination





FOA Home

 | UP: Vector space



Keyword discrimination

We can immediately use this vector space for something useful, as the
source of yet another approach to the question of appropriate keyword
weightings. Recall that in Figure (FOAref) our initial
assumption was that each and every keyword was to be used as a dimension
of the vector space. Now we ask: What would happen if we removed one of
these keywords?


The first step is to extend the measure
\mathname{Sim}(\mathbf{q},\mathbf{d}) \) of document-query similarity to
measure inter-document similarities \(
\mathname{Sim}(\mathbf{d}_{i},\mathbf{d}_{j}) \) as well. Then, for an
arbitrary measure of document-document similarity (e.g., the inner
product measure mentioned above), we consider all pairs of documents,
and then the AVERAGE SIMILARITY across all of them. {Fortunately,
it turns out that there is a more efficient way to compute average
similarity than actually comparing all ${n}\choose{2}$ document pairs.
First, define the CENTROID document to be the average document;
i.e., the resulting of adding all\( \mathname{NDoc} \) vectors and
dividing it by \( \mathname{NDoc} \). Then the average similarity can
also be defined in terms of the distance of each document from this
center. \overline{\mathname{Sim}_k}&\equiv&{1\over
\mathname{NDoc}^2}\sum_{i,j} \mathname{Sim}(d_i, d_j) \\ &=
&\alpha\sum_{i=1}^{\mathname{NDoc}} \mathname{Sim}(d_i, D^*) }


Recall
that our goal is to devise a representation of documents that makes it
easiest for queries to discriminate them. Since each keyword corresponds
to a dimension, {removing} one results in a {\em compression} of the
space into $K-1$ dimensions, and we can expect that the representation
of each of the documents will change at least slightly. More precisely,
removing a dimension along which the documents varied significantly
means that vectors which were far apart in the $K$-dimensional space are
now much closer together.


This observation can be used to ask just how
useful each potential keyword is. If it is discriminating, removing it
will result in a signficant compression of the documents' vectors; if
removing it changes very little, the keyword is less helpful. Using the
average similarity as our measure of just how close together the
documents are, and asking this question for each and every keyword, we
arrive at yet another measure of keyword discrimination:




Top of Page

 | UP: Vector space

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.4.2.1 Making weights sensitive to document  length





FOA Home

 | UP: Vector length normalization



Making weights sensitive to document  length

Unfortunately, this most-simple normalization is often inadequate, as
can be shown in terms of the inverse document frequency (IDF) weights
discussed in &sect;3.3.7 . IDF weighting
highlights the distinction between inter- and
intra-document keyword occurrences. Since its primary focus is
on discrimination among documents, intra-document occurrences of the
same keyword become insignificant. This makes IDF very sensitive to the
definition of just how document boundaries happen to be defined (cf.
Section &sect;2.2 ), as suggested by Figure
(figure) .


The IDF weight which results from encapsulating more
text within the same ``document'' is , in a sense, the converse of
normalizing the number of keywords assigned to every document. In either
case, the advantage of using the paragraph as our canonical document
(cf. Section &sect;1.4 ), and/or relying on
all documents in the corpus to be of nearly uniform size (as in the AIT
dissertation abstracts) is apparent.


The OKAPI retrieval system of
Robertson et al[Robertson94] has
proven itself successful (in retrieval competitions like TREC; cf.
Section &sect;4.3.3 ) by combining IDF
weightings with corpus-specific sensitivities to the lengths of the
document's retrieved . They propose that the the average length of all
documents in a corpus, $\overline{\mathname{DLen}}$, provides a
``natural'' reference point against which other documents' lengths can
be compared.


Define $(d)$ to be the number of keywords associated with
the document. OKAPI then normalizes the first, keyword-frequency
component of our weighting formula by a term that is sensitive to each
documents deviation from this corpus-wide average: w_{kd}={f_{kd} \over
\frac{k \cdot \mathname{Len}(d)}{\overline{\mathname{DLen}}} + f_{kd}}
\cdot \left(\log {(\mathname{NDoc}- D_k)+0.5\over D_k + 0.5}\right)
Robertson and Walker report that $k \approx 1.0-2.0 \mid Q \mid$ seems
to work best, where $\mid Q \mid$ the number of query terms.


Singhal et
al[Singhal96] approach the problem
of length normalization by doing a post hoc analysis of the
distributions of retrieved versus relevant documents (in the TREC
corpus), as a function of their length . A sketch of typical curves is
shown in Figure (figure) .A. The fact that these two
distributions cross suggests a corpus-specific LENGTH NORMALIZATION
PIVOT value, $p$, below which match scores are reduced and above
which they are increased. The amount of this linear increase/decrease,
shown as the LENGTH NORMALIZATION SLOPE $m$ of the length
normalization function of Figure (FOAref) .B, is the second
corpus-specific parameter of Singhal et al's model. Returning to the
``generic'' form of the weighting function originally given in Equation
(FOAref) , the pivot-based length normalization is:
w_{kd}={f_{kd} \over (1-m) \cdot p + m \cdot \mathname{norm}} \cdot
\mathname{discrim}_k where $\mathname{norm}$ is whatever other
normalization factor (e.g., cosine) is already in use; several possible
values are given in the next section.


Both OKAPI and pivot-based document
length normalization rely on the specification of additional
corpus-specific parameters ($k_{1}$ and $p,m$, resp.). While the
addition of yet more ``nobs to twiddle'' is generally to be avoided in a
retrieval system, recent experience with machine learning techniques
suggest the possibility of training such parameters so as to
best match each corpus. This approach is sometimes called a
REGRESSION technique and is discussed more fully in Chapter <A
HREF="foa-7.html">&sect;7 .




Top of Page

 | UP: Vector length normalization

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.4.2 Vector length normalization





FOA Home

 | UP: Vector space



Vector length normalization

One good example involves the length of document and query
vectors. So far, we have placed no constraint on the number of keywords
associated with a document. This means that long documents which,
caeteris paribus, can be expected to give rise to more keyword
indices, can be expected to match (more precisely, have non-zero inner
product with) more queries and be retrieved more often. Somehow (as
discussed in Section &sect;1.4 ) this doesn't
seem fair - the author of a very short document who worked hard to
compress his words into a pithy few paragraphs is less likely to have
his or her document retrieved, relative to some wordy guy who says
everything six times, six different ways!


These possiblities have been
captured by Robertson and Walker in a pair of hypotheses regarding a
document's SCOPE versus its VERBOSITY : document: Some
documents may simply cover more material than others ... (the ``Scope
hypothesis''). An opposite view would have long documents like short
documents but longer: in other words, a long document covers a similar
scope to a short document, but simply use more words (the ``Verbosity
hypothesis''). (p. 235) [<A
HREF="bibrefs.html#Robertson94">Robertson94] .


 Once we have decided
that {\about-ness is conserved} across documents, all documents' vectors
therefore have constant length. If we make the same assumption about the
query vector, then all of the vectors will lie on the surface of a
sphere, as shown in Figure (figure) . Without loss of
generality, we will assume the radius of the sphere is unity.

Subsections

	 3.4.2.1 Making weights sensitive to document  length




Top of Page

 | UP: Vector space

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.4.3 Summary: SMART weighting specification





FOA Home

 | UP: Vector space



Summary: SMART weighting specification

While the variety of potential keyword weighting schemes (signal/noise,
IDF, keyword discrimination, etc.) may seem large, there is in fact a
systematicity to this variation.


SMART is an extremely influencial and
widely used software system for investigating IR techniques [<A
HREF="bibrefs.html#REF153">REF153] [<A
HREF="bibrefs.html#REF869">REF869] . One secret of its success is
that SMART provides a simple parameter-driven mechanism for easily
changing from one form of index weighting to another.


In SMART, the
weight is decomposed into three factors:


Each of these components can
then be specified independently:




Top of Page

 | UP: Vector space

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.4 Vector space





FOA Home

 | UP: Weighting and matching against Indices



Vector space

One of life's most satisfying pleasures is going to a good library and
browsing in an area of interest. After having negotiated the library's
organization and found just which floor and shelves are associated with
the CALL NUMBERS of your topic, you are physically
surrounded by books and books, all of interest! Some are reassuring old
friends, already known to you; others are new books by familiar authors,
and (best of all!) some are brand new titles.


This system works because
human catalogers have proven themselves able to reliably and
consistently identify the (primary!) topic of books according to
conventional systems of subject headings like the Library of Congress
Subject Headings, or the Dewey Decimal system.


Our goal is to abstract
away from this very friendly notion of physical space in the
library, to a similar but generalized notion of SEMANTIC space in
which documents about the same topic remain close together. But
rather than allowing ourselves to be restricted by the physical
realities of three-dimensional space, and the fact that books can only
be shelved in a single place in a library, we will consider abstract
spaces of thousands of dimensions. we can represent in electronic
indices.}


We can make concrete progress towards these lofty goals
beginning with the \( \MATHNAME{INDEX} \) MATRIX relating each
document in a corpus to all of their keywords. A very natural and
influential interpretation of this matrix (due to Gerry Salton [<A
HREF="bibrefs.html#Salton75">Salton75] [<A
HREF="bibrefs.html#Salton83">Salton83] ) is to imagine each and
every keyword of the vocabulary as a separate dimension of a VECTOR
SPACE . In other words, the DIMENSIONALITY of the vector
space is the size of our vocabulary. Each document can be represented as
a vector within such a space. Figure (figure) shows a very
simplified (binary) \( \mathname{Index} \) matrix, and a cartoon of its
corresponding vector representation.


Estimates of vocabulary sizes of
native speakers of a language approach 50,000; if you are articulate,
your speaking and/or reading vocabularies might be 100,000 or more.
Assuming that we have an even modest $10^6$ document corpus, this matrix
is therefore something like $10^6\times\; 10^5$. That's a big matrix,
even by modern supercomputing standards.}.


In addition to the vectors
representing all documents, one special row matrix and vector
corresponds to a query. Since documents and queries exist within a
common vector space, we naturally characterize how we'd like our
retrieval system to work - just as we go to a physical location in the
library to be near books about a topic, we seek those
documents that are close to our query vector. This is a useful
characterization of what we'd like our retrieval system to accomplish,
but it is still far from a specification of an algorithm for
accomplishing it. For example, it seems to require that the query vector
be compared against each and every document, something we can hope to
avoid.


An even more important issue to be resolved before the vector
space model can be useful is being specific about just what it means for
a document and query to be close to one another. As will be discussed in
Section &sect;5.2.2 , there are many
plausible measures of proximity within a vector space. For the time
being, we will assume the use of the INNER PRODUCT of query and
document vectors as our metric:


People have difficulty imaging spaces
with more than the three physical dimensions of experience, and so it is
no wonder that abstract spaces of 10^5 \) dimensions are hard to
conceptualize. Sketches like (FOAref) do the best they can to
convey ideas in the three dimensions we appreciate, but it is critically
important that we not let intuitions based on such small-dimensional
experiences bias our understanding of the large-dimensional spaces
actually being represented and searched.

Subsections

	 3.4.1 Keyword discrimination
	 3.4.2 Vector length normalization
	 3.4.3 Summary: SMART weighting specification




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.5.1 Measures of association





FOA Home

 | UP: Matching queries against documents



Measures of association

Given a vocabulary of lexical indexing features, our central concern
will be how to moderate the raw frequency counts of these features in
documents (and, to a lesser extent, queries) based on {distributional}
characteristics of that feature across the corpus. While we will be led
to use real-valued weights to capture these relations, we begin with
cruder methods, simply counting shared keywords.


The most direct way to
say that a query and a document are similar is to measure the
{intersection} of their respective feature sets. Let $K_q$ be the set of
keyword features used in a query, and $K_d$ be the analogous set found
in a document. COORDINATION LEVEL is exactly how many terms in
the query overlap with terms in the document: \beq
\mathname{CoordLevel}(\mathbf{q,d}) = \left| \left( K_\mathbf{q} \cap
K_\mathbf{d} \right) \right| \eeq If we have many, many features and our
query and documents are highly variable, then the presence of any
significant overlap may be enough to identify the set of documents of
interest. On the other hand, if there is a great deal of overlap between
our query and many of the documents, then simply counting how big this
intersection is will look like a gross measure. This fine line between
one or two documents matching a query and an avalanche of thousands of
matching documents occurs regularly as part of FOA.


One part of the
problem is NORMALIZATION - the coordination level of intersecting
features shared by both query and document seems a good measure of their
similarity, but compared to what?! It's easy to show that this
normalization matters. Consider the case where
$\mathname{CoordLevel}(q,d) =1$, and imagine first that $K_q = K_d = 1$
also; i.e., a single word query and a one-word document. In this case
the two match on the one feature they each have. Intuitively, this is a
perfect match; you couldn't do any better. But now imagine that the
query includes 10 keywords, the document contains 1000, but still
$\mathname{CoordLevel}(q,d) =1$. The same intuition suggests that this
should be judged a much poorer match, but our measure does not reveal
this. Unless we normalize by something that reflects how many other
features we might have matched, we can't have a useful measure of
association.


One natural normalizer is to take the {average} of the
number of terms in the query and in the document and compare the size of
the intersection to it. This gives us the Dice coefficient: \beq
\mathname{Sim}_{Dice}(\mathbf{q,d}) = 2 \frac{\left| \left( K_\mathbf{q}
\cap K_\mathbf{d} \right) \right|} {\mid K_\mathbf{q} \mid + \mid
K_\mathbf{d} \mid} \eeq The average number of features may or may not be
appropriate, again depending on what a typical query and typical
document is. Often a document has many, many keywords associated with it
while queries may have only one or two. This average is not a very good
characterization of either.


Another perspective on similarity says that
missing features are as significant as shared ones. The SIMPLE
MATCHING COEFFICIENT gives equal weight to those features {\em
included} in both query and document and those {\em excluded} from both,
and normalizes by the total number of keyword features $NKw$: \beq
\mathname{Sim}_{simple}(\mathbf{q,d}) = \frac{\left| \left( K_\mathbf{q}
\cap K_\mathbf{d} \right) \right| + \left| \left( (NKw - K_\mathbf{q})
\cap (NKw - K_\mathbf{d}) \right) \right|} {\mid NKw \mid} \eeq




Top of Page

 | UP: Matching queries against documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.5.2 Cosine similarity





FOA Home

 | UP: Matching queries against documents



Cosine similarity

We will focus here on the COSINE measure of similarity. Not only
does this respect the useful mathematical properties mentioned at the
beginning of this section, but it is consistent with a geometric
interpretation of the vector space that many find insightful.




Top of Page

 | UP: Matching queries against documents

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.5 Matching queries against documents





FOA Home

 | UP: Weighting and matching against Indices



Matching queries against documents

In Chapter 2 we first identified documents, and then lexical features to
be associated with each. Then we built an inverted keyword list to make
going from keywords to documents about those keywords as easy as
possible. Now we become specific about how we measure the similarity
between a document and a query.


The discussion of matching queries and
documents is simplified if we adopt the vector space perspective of
Section &sect;3.4 and imagine both the query
$\mathbf{q}$ and all documents $\mathbf{d}$ to be vectors in a space of
dimensionality equal to the $\mathname{NKw}$, the keyword vocabulary
size.


In this space, the answer to the question of which documents are
the best match for a query seems straightforward - those documents which
are most similar, relative to some particular METRIC
$\mathname{Sim}$ measuring distance between points in the space.
Students of algebra and abstract vector spaces know that a wide range of
choices are possible; see Section <A
HREF="foa-5-2-2.html">&sect;5.2.2 .

Subsections

	 3.5.1 Measures of association
	 3.5.2 Cosine similarity




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.6 Calculating TF-IDF Weighting





FOA Home

 | UP: Weighting and matching against Indices



Calculating TF-IDF Weighting

Following the careful empirical investigation of Salton and Buckley [<A
HREF="bibrefs.html#Salton88a">Salton88a] [<A
HREF="bibrefs.html#Salton90">Salton90] and many others since [<A
HREF="bibrefs.html#Harman92a">Harman92a] , we will concentrate on
the TF-IDF weighting which multiplies the raw TERM
FREQUENCY (TF) of a term in a document by the term's INVERSE
DOCUMENT FREQUENCY (IDF) weight: \mathname{idf}_{k} &=&\log \left(
\frac{\mathname{NDoc}}{D_{k}}\right) \\ w_{kd} &=&f_{kd} \cdot
\mathname{idf}_{k} where $f_{kd}$ is the frequency with which keyword
$k$ occurs in document $d $; $\mathname{NDoc}$ is the total number of
documents in the corpus; and $D_{k}$ is the number of documents
containing keyword $k$.


Due to the wide variety observed in users' query
patterns, methods for weighting queries vary more, primarily depending
on the length of the query. We will consider two weighting
methods which are especially designed for ``short'' and ``long''
queries. Short queries (of as few as one or two terms! cf. [<A
HREF="bibrefs.html#Silverstein99">Silverstein99] ) seem typical of
those issued by Web search engine users. For these, we can assume
multiple occurrences of the same keyword will be rare, and also ignore
length normalization. This leaves us with simply the term's inverse
frequency weight: w_{kq_{\mathname{short}}}=\mathname{idf}_{k}



Long queries are often generated indirectly, as the result of
RelFbk from the users in response to prior retrievals. The long
query corresponds to a particular {\em document} that the users like;
searching for others like a known target is called
QUERY-BY-EXAMPLE. By symmetry, it makes sense to use the same
weighting of terms in this query cum document as we used for documents
(Equation (FOAref) ) above:


Notice that once the lengths of the
$$ and $\mathbf{d}$ vectors in the denominator of Eq. (FOAref)
are known, the computation of $\mathname{Sim}$ requires a simple
sum-of-products over all terms shared by query and document. Since both
these lengths can be computed independently, it makes sense to compute
them first. {The fact that the keyword's total document frequency
$D_{k}$ cannot be known until the entire corpus has been processed
suggests a ``second pass.'' In some cases, lengths are computed for each
document and stored in a doclen file, separate from the
main kw-inv inverted postings file. (It would also be
possible to normalize the frequencies $f_{kd}$ by document lengths and
store this quotient in the postings file, but that would require the
storage of floats rather than small integers.) For the small corpora we
consider here, it proves easier to simply compute these values as the
inverted index is read in, prior to the first matching against queries}.


In
fact, the document length $(\mathbf{d})$ can be computed before any
query activity takes place: \mathname{Len}(\mathbf{d})
&=&\sqrt{\sum\limits_{k\in d}w_{kd}^{2}} \\ &=&\sqrt{\sum\limits_{k\in
d}\left( f_{kd}\,\mathname{idf}_{k}\right) ^{2}^{2}\ } \end{eqnarray}


With
these definitions in place, we can begin to design an algorithm for
computing similarity.




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.7 Computing partial match scores





FOA Home

 | UP: Weighting and matching against Indices



Computing partial match scores

With length normalization to the side, we can concentrate on the main
calculation of matching, summing the weight products of terms shared by
query $q$ and document $d$:


But this mathematical characterization hides
a number of messy details associated with actually computing it. We will
need to make efficient use of two data structures in particular. The
first is the inverted index (recall Figure (FOAref) .). The
critical feature of this organization is that we can, for each term in
the query, find the set of all doument postings associated with it. In
particular, the freq statistic maintained with each posting
allows us to compute the weight $w_{kd}$ we need for each. But even with
these efficiencies, the need to consider every document posting for
every query term can create a serious processing demand, and one that
must be satisfied immediately - the users are waiting!


Since the elements
of the sum can be re-ordered arbitrarily, we will consider a PARTIAL
RANKING (a.k.a. filtering, or pruning) algorithm that attempts to
include the dominant elements of the sum but ignore small,
inconsequential ones [Salton89] [<A
HREF="bibrefs.html#Buckley85">Buckley85] [<A
HREF="bibrefs.html#Harman90">Harman90] .


The fact that both $w_{kq}$
and $w_{kd}$ vary considerably suggests that, by ordering the inclusion
of products into the sum, we may be able to truncate this process when
they become smaller than we care to consider.


We therefore begin by
sorting the terms in decreasing order of query weights $w_{kq}$.
Considering terms in this order means we can expect to accumulate the
match score beginning with its largest terms. Then, the fact that our
list of postings was similarly (and not coincidentally:) ordered by
decreasing frequency means that: \left( \forall j>i\right)
\,w_{kd_{i}}>w_{kd_{j}}


 Once these weights diminish below some threshold
$$, we can quit walking down the postings list. (In fact, it may be that
the weight associated with the very first posting is too small and we
can ignore all weights associated with this term.)


The second important
data structure is an ACCUMULATOR queue in which each document's
match score is maintained. Since each query term may add an additional
term to the match score for a particular document, these accumulators
will keep a running total for each document. For moderate corpus sizes,
it may not be unreasonable to allocate an accumulator for each document,
but this can demand too much memory for very large corpora. Define
$\mathname{NAccum}$ to be the number of accumulators we are willing to
allocate. Then one obvious way to restrict this set is to only allocate
an accumulator when a document's score becomes significant, again in
comparison to some threshold $\tau_{insert}$. Since we will be
processing query terms in decreasing $w_{kq}$ order and heuristically
value the space associate with new accumulators more than the slight
extra time to run down posting lists a bit farther, we can assume
$\tau_{\mathname{insert}}>\tau _{\mathname{add}}$.


Picking appropriate
values for these two thresholds is something of a black art, but Persin
[Persin94] reports one especially
careful experiment in which both are made proportional to the most
highly matched document's accumulator $A^{*}$ (i.e., $A^{*}$ is the
maximum match score in any document's accumulator):
\tau_{\mathname{insert}} &=&\eta _{\mathname{insert}}\,\cdot \,A^{*} \\
\tau_{\mathname{add}} &=&\eta _{\mathname{add}}\,\cdot \,A^{*} Persin's
experiments suggest that values of $\eta _{\mathname{insert}
}=0.07,\,\eta _{\mathname{add}}=0.001$ give retrieval effectiveness near
that of full matches (i.e., considering all query-document term
products) while mininizing $\mathname{NAccum}$.


These two thresholds
divide the range of possible query-document term products into three
conditions: & w_{kd}\cdot w_{kq} > \tau_{\mathname{insert}} &
\mathrm{Always\ add;\ create\ new\ accumulator\ }A_{d}\mathrm{\ if\
necessary} \\ \tau_{\mathname{add}} & < w_{kd}\cdot w_{kq} \leq
\tau_{\mathname{insert}} & \mathrm{Add\ only\ if\ accumulator\
}A_{d}\mathrm{\ already\ exists} \\ & w_{kd}\cdot w_{kq} \leq
\tau_{\mathname{add}} & \mathrm{Ignore;\ move\ on\ to\ next\ query\
term}


 Since we want to remain flexible with respect to both long and
short queries, we will assume that the query weights $w_{kq}$ are
pre-computed and passed to our ranking procedure.


 Using our definition
for $w_{kd}$ and focusing first on the $_{\mathname{insert}}$ threshold:
& w_{kd}\cdot w_{kq} > & \tau _{\mathname{insert}} \\
\Longleftrightarrow & \left( f_{kd} \cdot \mathname{idf}_{k}\,\right)
\cdot w_{kq} > & \tau_{\mathname{insert}} \\ \Longleftrightarrow &
f_{kd}& f_{kd}\ > &
\frac{\tau_{\mathname{insert}}}{\mathname{idf}_{k}w_{kq}} \\
\Longleftrightarrow & f_{kd}\ > &
\frac{\eta_{\mathname{insert}}\,\,A^{*}}{\mathname{idf}_{k}\,w_{kq}}
\end{eqnarray} This finally becomes an operational question we can apply
with respect to each posting's frequency $f_{kd}$. Note that this
threshold must be updated every time we move to a new term of the query.
And of course, the computation of $\tau _{\mathname{add}}$ proceeds
similarly.


All the basic features of a partial ranking algorithm are now
in place, and a pseudo-code sketch is shown in Figure (FOAref)
. It also includes a few minor complications. First, a hashtable is
required to find accumulators associated with a particular
docno. Second, the set of accumulators $A_{d}$ is described
as a queue, but must be slightly trickier than most - it must maintain
them in order so that only the top $\mathname{NAccum}$ are maintained,
and must support length() queries and a pop()
function when it is full. Nondeterministic skip lists [<A
HREF="bibrefs.html#Pugh90">Pugh90] are recommended for this
purpose.[Cutting97] .




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3.8 Conclusion





FOA Home

 | UP: Weighting and matching against Indices



Conclusion

We've covered enormous ground in this chapter. We began by wondering
just what it might mean to try to understand communicative acts,
like the publication of a document or the posing of a question/query. We
looked in some detail at one of the most fundamental characteristics of
texts, Zipf's Law, and found it to be, in fact, quite \rikmeaning-less!
But the next level of features, tokens like those produced by the
machinery of Chapter 2, supported a rich analysis of the Index, as a
balance between the vocabularies of the documents' authors and the
searchers' subsequent queries. The vector space provides a concrete
model of potential keyword vocabularies and what it might mean to match
within this space. Finally, we considered an efficient implementation of
nearly-complete matching. In the next chapter we consider the problem of
evaluating just how well all these techniques work in practice. But
there are gaps in this story that are so obvious we don't even need to
measure.


Soem invovle implementation issues that can be critical,
especially when faced with very large corpora [<A
HREF="bibrefs.html#Harper92">Harper92] . Parallel implementation
techniques, for example those pioneered on ``massively parallel'' SIMD
(single instruction/multiple data) Connection machines become important
in this respect [REF589] [<A
HREF="bibrefs.html#REF378">REF378] [<A
HREF="bibrefs.html#REF583">REF583] . In the modern age of multiple
search engines each indexing (only partially overlapping versions! [<A
HREF="bibrefs.html#Lawrence98">Lawrence98] ) of the WWW techniques
for FUSING multiple hitlists into a single list for the same user
suggests another level of parallelism in the FOA task [<A
HREF="bibrefs.html#Voorhees95">Voorhees95] .


But linguists, in
particular, must have more serious, implmentation-independent concerns.
Imagine that you are someone who has studied the range of human
languages and who appreciates both their wide variety and equally
remarkable commonalities. You would be appalled at the violence we have
done to the syntactic structure of language. For linguists,
finding out about documents by counting their words is like trying to
understand Beijing by listening to a single microphone poised high over
the city. You can pick up on a few basic trends (like when most people
are awake) but most of the texture is missing!


{DOG BITES MAN} and {\tt
MAN BITES DOG} clearly mean two different things. Word order
obviously conveys meaning beyond that brought by the three words.
And the problem doesn't end with word order. Look how different the
meanings of these phrases are: \bit \item {\tt NEUTRALIZATION OF
THE PRESENT} \item {\tt REPRESENTING NEUTRONS} \item {\tt
REPRESENTATIONS, NOT NEUTRONS} \eit despite the fact that all of them
(conceivably) reduce to the same set of indexable tokens! Note
especially how critical the same ``noise'' words thrown away on
statistical grounds (in Chapter 2) are in analyzing a sentence's
syntactic structure.


The attempt to understand the phenomena of
meaning -- words -- by looking for patterns in word frequency
statistics alone is reminiscent of the tea leaves and entrails of this
chapter's opening quote. Still, the success of many WWW search engines
that use very little beyond this kind of gross analysis suggests that
their is much more information in the statistics than traditional,
syntactically-focussed linguists might have believed.




Top of Page

 | UP: Weighting and matching against Indices

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 3 Weighting and matching against Indices





FOA Home

 | UP: foa-book.tex



Weighting and matching against Indices

The Bible Code as Ouija Board: The fascination with the
subliminal, the camouflaged, and the encrypted is ancient. Getting a
computer to munch away at long strings of letters from the Old Testament
is not that different from killing animals and interpreting the
entrails, or pouring out tea and reading the leaves. It does add the
modern impersonal touch - a computer found it, not a person, so it must
be ``really there." But computers find what people tell them to find. As
the programmers like to say, ``prophesy in, prophesy out." [<A
HREF="bibrefs.html#Menaud96">Menaud96] \eq

Subsections

	 3.1 Microscopic semantics and the statistics of  communication
	 3.2 Rembember Zipf
	 3.3 A statistical basis for keyword meaning
	 3.4 Vector space
	 3.5 Matching queries against documents
	 3.6 Calculating TF-IDF Weighting
	 3.7 Computing partial match scores
	 3.8 Conclusion




Top of Page

 | UP: foa-book.tex

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.1.1.1 Prototypic retrievals





FOA Home

 | UP: Cognitive assumptions



Prototypic retrievals

From the perspective of cognitive psychology, the task facing users who
are asked to produce RELEVANCE FEEDBACK (\RelFbk) can best be
described as one of OBJECT RECOGNITION , in the tradition of
Rosch and others [REF392] [<A
HREF="bibrefs.html#Rosch77">Rosch77] . The object to be recognized
is an internally represented {\em prototypic} document satisfying the
users' INFORMATION NEED. In this case, the prototype corresponds
to the model the subjects maintain of ideally relevant documents. As
users consider an actual, retrieved document's relevance, they evaluate
how well it {\em matches} the prototype. Barry and others have suggested
the many and varied features over which the prototypes can be defined
[Barry94] . Only a small number of
these may be revealed by any one of the users' queries, of course.


Here
we will simply assume the users are capable of grading the { quality} of
this match. Users might be asked to score the quality of relevance match
according to a five-point scale such as that shown in Figure
(figure) . Users can qualify the middle ``Relevant'' response
either by weakening it (``possibly relevant'') or strengthening it
(``critically relevant''). Such distinctions are often made in
experimental settings (e.g., in the use of the STAIRS retrieval system
by lawyers [Blair85] ), and relate to
the varying purposes for FOA different users may have. To make these
concrete, we might imagine ``Critically relevant'' to apply only to
those documents that must be read even for an undergrad term paper,
while ``Possibly relevant'' would be much more broadly applied to those
that a PhD student does as part of their literature review.


For now,
however, we will simplify the types of RelFbk to allow users to
reply with only a single grade of ``Relevant'' ($\oplus$) and a single
grade of ``Not relevant'' ($\ominus$). These two assesments require
overt action on the part of subjects; ``No response'' (\#) is the
default RelFbk assessment for documents not receiving any other
responses. Again, this frees users from the much more cognitively
demanding task of exhaustively assessing each and every retrieved
document. Those documents that ``jump out'' at users as particularly
good, or especially bad, examples of the prototype they seek provide the
most informative \RelFbk. Figure (FOAref) also introduces a
color-code convention: \blue will be used to indicate positive \RelFbk,
and \red to indicate negative cate negative gative \RelFbk.




Top of Page

 | UP: Cognitive assumptions

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.1.1.2 RelFbk is nonmetric





FOA Home

 | UP: Cognitive assumptions



RelFbk is nonmetric

As we move from a cognitive understanding of the users' tasks to
statistical analyses of their behaviors, it is important to understand
one important feature of the RelFbk data stream: RelFbk is
NONMETRIC data. That is, while users find it easy and natural to
critique retrieved documents with \blue{$\oplus$}, \red{$\ominus$} and
\#, they would find it much more difficult to reliably assign
numeric quantities reflecting something like the relative
goodness of each retrieval.


Think for a moment just why this is hard, by
imagining your reactions to a typical retrieval. Is the first document
to be rated 10, or 6743? If you rate the second document as 6, and the
third 2, then you must also ensure that the third document is exactly as
much less relevant than the second as the second is from the first.
Trying to keep all $Rel(d_{i})$ assesments consistent in the metric
sense, for many retrieved documents or any other set, makes people
crazy. Issue a query of your choosing to any Web search engine that will
retrieve 10 or more hits. Using the order of retrieval as an index, go
to odd retrieval documents (i.e., hits \#1, 3, 5, etc.) and
assign to each a score in the range [-10,+10] indicating how relevant
you found it. Now repeat the exercise for the even documents.
Finally, analyze your assessments for each of the triplets of documents:
${1,4,7}, {2,5,8}$ and ${3,6,9}$. How well do they satisfy the required
properties of a metric (cf. Section <A
HREF="foa-5-2-2.html">&sect;5.2.2 )?


 This is not only a property of
relevance assessments. A large literature on psychological assessment
[Kruskal77a] [<A
HREF="bibrefs.html#Kruskal77b">Kruskal77b] [<A
HREF="bibrefs.html#REF611">REF611] has demonstrated that while human
subjects can quite easily and reliably sort objects into ``piles'' where
they like one pile better than another, they find it more difficult to
quantify just how much they like each object, and make these
quantitative assessments consistent with one another in the way that
would be necessary if we were to have a true preference {\em metric}. .
Rather than assuming that users can provide a separate score for each
retrieved document, we will therefore treat this as an ordered,
non-metric scale of increasing PREFERENCE : \beq \red{\ominus}
\prec \# \prec \blue{\oplus} \eeq


Each of these assumptions is a matter
of considerable debate [Wilson73]
[Froehlich94] , and likely to be
the topic of much future work. It is also interesting to note that to
linguists attempting a comprehensive model of how and why humans use
language, ``relevance'' plays a central role (cf. Section <A
HREF="foa-8-2-2.html">&sect;8.2.2 ).




Top of Page

 | UP: Cognitive assumptions

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.1.1 Cognitive assumptions





FOA Home

 | UP: INDIVIDUAL'S assessment of relevance



Cognitive assumptions

``Garbage in, garbage out'' is one of the first insights every software
developer learns, and FOA is no exception. The primary source of data
considered by traditional IR methods, and the focus of Chapters <A
HREF="foa-2-1.html">&sect;2.1 and &sect;3
of this text, are the documents of the corpus, particularly the keywords
they contain. (Chapter &sect;6 will consider
the use of other document attributes.) A fundamental feature of the
broader FOA view is that {\bf browsing users provide an equally
important source of data} concerning what keywords mean and what
documents are about. It is therefore appropriate to begin by
characterizing just who these users are that we will be watching.


We
begin with one important cognitive assumption we must make about our
users: How thorough a FOA search do they wish to perform? Is
this an important search to which the users are willing to dedicate a
great deal of time and attention, or will a quick, cursory answer
suffice? For example, Chapter 1 mentioned how much less exhaustive the
typical undergraduate (doing some quick research before submitting a
term paper) is than the PhD candidate (who wants to ensure his or her
proposed dissertation topic is new). The typical WWW searcher seems
satisfied with only a few useful leads, but the professional searcher (a
lawyer looking for any case that might help, a doctor looking for any
science that might heal a patient) will search diligently if there is
even a small chance of finding another relevant document. This kind of
variability can be observed not only across different classes of users
but even across the same user at different times. The idea is that a
very pragmatic need might initially cause users to come to the \EB, but
often continue to read as they learn that their initial query does not
have a simple answer. Imagine that you want the answer to a simple,
factual query, for example,the height of Mt. Everest. The first couple
of paragraphs of the article on Mt. Everest would meet a simple version
of this information need quite admirably: \bq Tibetan CHOMOLUNGMA,
Chinese (Wade-Giles) CHU-MU-LANG-MA FENG, (Pinyin) QOMOLANGMA FENG,
Nepali SAGARMATHA, peak on the crest of the Great Himalaya Range in
Asia, the highest point on Earth. It lies on the border between Nepal
and China (Tibet), at 27 [{degree}] 59' N, 86[{degree}] 56' E.


Three
barren ridges--the Southeast, Northeast, and West--culminate in two
summits at 29,028 feet (8,848 m; Everest) and 28,700 feet (8,748 m;
South Peak). The mountain can be seen directly from its northeastern
side, where it rises about 12,000 feet (3,600 m) above the Plateau of
Tibet. The lesser peaks of Changtse (north; 24,803 feet [7,560 m]),
Khumbutse (northwest; 21,867 feet [6,665 m]), Nuptse (southwest; 25,791
feet [7,861 m]), and Lhotse (south; 27,890 feet [8,501 m]), which rise
around its base, hide the summit from Nepal.
{}
WW.EB.COM:180/CGI-BIN/G?DOCF=MICRO/199/84.html>}
.EB.COM:180/CGI-BIN/G?DOCF=MICRO/199/84.html>}
B.COM:180/CGI-BIN/G?DOCF=MICRO/199/84.html>}
COM:180/CGI-BIN/G?DOCF=MICRO/199/84.html>}
M:180/CGI-BIN/G?DOCF=MICRO/199/84.html>}
180/CGI-BIN/G?DOCF=MICRO/199/84.html>}
0/CGI-BIN/G?DOCF=MICRO/199/84.html>} CGI-BIN/G?DOCF=MICRO/199/84.html>}
I-BIN/G?DOCF=MICRO/199/84.html>} BIN/G?DOCF=MICRO/199/84.html>}
N/G?DOCF=MICRO/199/84.html>} G?DOCF=MICRO/199/84.html>}
DOCF=MICRO/199/84.html>} CF=MICRO/199/84.html>} =MICRO/199/84.html>}
ICRO/199/84.html>} RO/199/84.html>} /199/84.html>} 99/84.html>}
/84.html>} 4.html>} html>} ml>} >} \eq


But in fact there are at least
three numbers that could legitimately be given as this answer, each
associated with a separate expedition at a different point in history!
The online version of the Encyclopedia Britannicamakes this additional
``Research Note'': \bq The generally accepted figure of 29,028 feet
(8,848 m) for the height of Mount Everest was established by the Indian
government's Survey of India in 1952-54. This datum is used by, among
others, the (U.S.) National Geographic Society.


A Chinese survey in 1975
obtained the figure of 29,029 feet, and an Italian survey, using
satellite surveying techniques, obtained a value of 29,108 feet (8,872
m) in 1987, but, owing to questions about the methods used, neither of
these results is widely accepted. In 1986 a measurement of K2, regarded
as the second highest mountain, seemed to indicate that it was higher
than Everest, but this was subsequently shown to be an error. In 1992
another Italian survey, using a global satellite positioning system and
laser measurement technology, yielded the figure 29,023 feet (8,846 m)
by subtracting from the measured height the 6.5 feet (2 m) of ice and
snow on the summit; this value has not found general acceptance.
{} \eq These sagas make
for very interesting reading, but only if you have the additional time
and energy available to benefit from such education. Section <A
HREF="foa-8-3-4.html">&sect;8.3.4 will explore this connection
between FOA and educational objectives in further detail. }

Subsections

	 4.1.1.1 Prototypic retrievals
	 4.1.1.2 RelFbk is nonmetric




Top of Page

 | UP: INDIVIDUAL'S assessment of relevance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.1 INDIVIDUAL'S assessment of relevance





FOA Home

 | UP: Assessing the retrieval



INDIVIDUAL'S assessment of relevance
Subsections

	 4.1.1 Cognitive assumptions




Top of Page

 | UP: Assessing the retrieval

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.2.1 Using \RelFbk for query refinement





FOA Home

 | UP: Extending the dialog with \RelFbk



Using \RelFbk for query refinement

While Figure (FOAref) showed the retrieved set of documents as
a simple set, it is interesting to impose the RelFbk labeling on the
retrieved documents viewed in vector space. Figure (figure)
shows a query vector and a number of retrieved documents, together with
a plausible distribution of RelFbk over them. That is, we can
expect that there is some localized region in vector space where
\blue{$\oplus$} relevant documents are most likely to occur. If we
believe that these positively labeled retrieved documents are in fact
clustered, it becomes appropriate to consider a hypothetical
CENTROID (average) document $d^{+}$ which is at the center of all
those documents that users have identified as relevant.


It is less
reasonable, however, to imagine that the {negatively} labeled
\red{$\ominus$} documents are similarly clustered. Documents which were
inappropriately retrieved failed to be relevant for one reason or
another. There might be a number of such reasons. These are shown as
\green, discriminating planes in Figure (figure) .


The vector
space view also lets us easily portray two quite different uses to which
RelFbk information might be applied. Most typically, RelFbk is used
to refine the user's query. Figure (figure) represents this
refinement in terms of two changes we can make to the initial query
vector. The first of these is to ``take a step towards'' the centroid of
the positive RelFbk cluster. The size of this step$ is treated as
the right answer, the parameter alpha becomes analogous to neural net's
learning rate.} reflects how confident we are that the positive cluster
centroid is a better characterization of the user's interests than their
original query.


There is one important difference between the query and
even a slight perturbation of it towards a cluster's centroid: While the
original query is often very sparse and resulting from just those few
words used in the user's original query, any movement towards the
centroid will include (linear combinations of) keywords used in any of
the positively labeled documents. The additional difficulty in
implementing this much more densely-filled feature vector becomes a
serious obstacle in many system implementations. The fact that refined
queries involve many more non-zero keyword entries also means that query
weighting and matching techniques may be sensitive to this difference.


Seminal
work on the use of RelFbk was done by Salton, especially with
students Roccio, Brauen and Ide in the early 1970s [<A
HREF="bibrefs.html#Rocchio66">Rocchio66] [<A
HREF="bibrefs.html#REF567">REF567] [<A
HREF="bibrefs.html#REF319">REF319] . More recent students have
extended the theory of query refinement, and related it to topics in
machine learning [Buckley94] [<A
HREF="bibrefs.html#Buckley95">Buckley95] [<A
HREF="bibrefs.html#Allan96">Allan96] .


Some of these experiments
explored a second modification to the query vector. In addition to
moving towards the $d^{+}$ center of \blue{$\oplus$}, it is
also plausible to move away from the irrelevant retrieved
documents \red{$\ominus$}. As noted above, however, it is much less
likely that these irrelevant documents are as conveniently clustered. As
Salton [Salton83] reports: ...
retrieval operation is most effective when the relevant documents as
well as the non-relevant documents are tightly clustered and the
difference between the two groups is as large as possible. .... The
RelFbk operation is less favorable in the more realistic case where the
set of non-relevant documents covers a wider area of the space. [p. 145]
One possible strategy is to take a single element $d^{-}$ of the
irrelevant retrieved documents (for example, the most highly ranked
irrelevant retrieval) and define the direction of movement with respect
to it alone.


As we have discussed above in connection with Figure
(FOAref) , RelFbk helps to link together individual
queries into a browsing sequence. And so, while we have focused here on
the the simplest form of query refinement, with respect to the users'
initial queries, RelFbk can be given again and again. An initial
query vector is moved towards the centroid of documents identified as
relevant (perhaps away from an especially bad one), this modified query
instigates a new retrieval which is again refined. In practice, it
appears that such adjustments result in diminishing returns after only a
few iterations of query refinement [<A
HREF="bibrefs.html#Salton83">Salton83] [<A
HREF="bibrefs.html#Stanfill86">Stanfill86] .


However, Section <A
HREF="foa-7-3.html">&sect;7.3 will discuss a type of FOA in which a
document corpus is constantly changing and the user's interest in a
topic is long-lived. In this case, we can imagine the query as a
FILTER against which a constant stream (e.g., of newly published
Web documents) is applied. RelFbk has also been used in this
setting, to make ongoing changes to the query/filter that continue to
improve its effectiveness [Allan96] .


Using
RelFbk for query refinement produces results which are immediately
satisfying to the users. First, it automatically generates a new query
with which they can continue their browsing process. Second, the
statistical analysis of positively labeled retrieved documents can
provide other forms of useful information to the users as well. For
example, rather than simply retrieving a new set of documents, new {\em
keywords} not originally in the users' queries but present in positive
documents at statistically significantly levels, can be suggested to the
users as new vocabulary. Conversely, words that were in the original
query but {\em negatively} correlated with $d^{+}$ (and/or positively
correlated with $d^{-}$) can be identified as well.




Top of Page

 | UP: Extending the dialog with \RelFbk

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.2.2 Using \RelFbk to adapt documents' indices





FOA Home

 | UP: Extending the dialog with \RelFbk



Using \RelFbk to adapt documents' indices

An alternative use of RelFbk is to make changes to the documents
rather than to the query. While before the argument was that it is
sensible to make the query look more like those documents the users
liked, the argument now is the converse: Documents found relevant to a
query should be described more like the query used to identify them.
Changes made to document vectors according to this heuristic are known
as DOCUMENT MODIFICATIONS and shown in Figure (figure) .


Note
that unlike query modification, adaptive document modifications made in
response to RelFbk are not expected to be of (immediate) use to the
users who provide them. Instead, the hope is that these changed
documents' representations are available later, to others who might be
searching. As Salton [Salton83]
describes the goal: Following a large number of such interactions
documents which are wanted by the users will have been moved slowly into
the active portion of the document space - that part in which large
numbers of users' queries are concentrated, while items which are
normally rejected will be located on the periphery of the space. [p.
145] This provocative proposal, {\em allowing a search engine to learn
from its users}, is considered in much greater detail in Chapter <A
HREF="foa-7.html">&sect;7 .




Top of Page

 | UP: Extending the dialog with \RelFbk

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.2.3 Summary





FOA Home

 | UP: Extending the dialog with \RelFbk



Summary

We have been discussing RelFbk from the individual user's point of
view. We've focused on how this information might be collected, and how
it might be used, both in the short-term to modify users' retrievals and
in a longer term way to change the document's indices. Now we want to
consider a third use for RelFbk information. When we have more than
one system to use for retrieval and would like to evaluate which is
doing the better job, users' assessments of retrieved documents'
relevance can be used as ``grades''. If one system can consistently,
across a range of typical queries, more frequently retrieve documents
that the users mark as relevant, and fewer that they mark as irrelevant,
then that system is doing a better job.




Top of Page

 | UP: Extending the dialog with \RelFbk

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.2 Extending the dialog with \RelFbk





FOA Home

 | UP: Assessing the retrieval



Extending the dialog with \RelFbk

Figure (figure) focuses on a single instance of \RelFbk, shown
as a labeling over the set of \Ret documents. But beyond any one
reaction to a single retrieved set product, a central premise
of the FOA process is that users' reactions to just-retrieved
documents provides the pivotal link many such assesments into the FOA
search dialog. This is perhaps most clear in Figure (figure) ,
where RelFbk is used to link a series of reactions into a QUERY
SESSION .


Attempts to support this searching process, and then
attempts to rigorously evaluate how well software sytems support
browsing users as they FOA is one of the most vexing issues within IR
evaluation. [Daniels85] [<A
HREF="bibrefs.html#Saracevic88">Saracevic88] [<A
HREF="bibrefs.html#Larson91">Larson91] [<A
HREF="bibrefs.html#REF1045">REF1045] [<A
HREF="bibrefs.html#ODay93">ODay93] [<A
HREF="bibrefs.html#Cawsey92">Cawsey92] [<A
HREF="bibrefs.html#Russel93">Russel93] [<A
HREF="bibrefs.html#Koenemann96">Koenemann96] . Part of the problem
is the misconception that, if a search engine works perfectly, and the
user issues the perfect MAGIC BULLET QUERY , out will spill all
and only relevant documents! Such simplistic definitions of optimality
come naturally to computer scientists; library scientists who are used
to the naturalistic behaviors of real patrons in their libraries know
that a much more extended and nebulous form of support is required.


Marcia
Bates' famous BERRY PICKING metaphor [<A
HREF="bibrefs.html#REF839">REF839] [<A
HREF="bibrefs.html#Bates89">Bates89] is useful here. [a] query is
not satisfied by a single final retrieved set, but by a series of
selections of individiual references and bits of information at each
stage of the ever-modifying search A bit-at-a-time retrieval of this
sort is here called beryypicking. This term is used by analogy to
picking huckleberries or blueberries in the forest. The berries are
scattered on the bushes; they do not come in bunches. One must pick them
one at a time. One could do berry-picking of information without the
search need itself changing (evolving) but ... [we] consider searches
that combine both of these characteristics. [<A
HREF="bibrefs.html#Bates89">Bates89]


 In addition to highlighting the
same iterative, browsing behavior central to FOA's characterization of
the dialog, the ``evolving'' character of the information need in Bates'
metaphor is also important. Imagine that you are in the forest on an
idyllic day with only one purpose: fill your bucket with the best
blueberries you can find. Early in the day, with your whole afternoon in
front of you, you are likely to be very choosy. At this juncture, you
could bump into a bush full of blueberries that were not as ripe nor as
large as you imagine must exist somewhere else in the forest,
and not drop a single one into your basket. But late in the afternoon,
if you have had poor luck and little to show for your efforts, you could
come across an even worse bush and grab every single berry, shriveled or
not!


Applying this metaphor to FOA is provacative in many respects. For
example, it suggests that maintaining an explicit representation of the
retrieved document ``basket'' might be a useful addition to any search
engine interface. It predicts a time course to the distribution of
users' RelFbk assessments. For now, we simply observe that it seems
quite likely that an assessment of one document's relevance will depend
greatly on the ``basket'' of other documents we already have seen. The
general idea of thinking of an ``evolutionary ecology of information
foraging'' [Pirolli97] has become
less metaphoric and more concrete as information search agents (like the
InfoSpiders described in &sect;7.6 ) explore
the ``environment'' of the WWW.

Subsections

	 4.2.1 Using \RelFbk for query refinement
	 4.2.2 Using \RelFbk to adapt documents' indices
	 4.2.3 Summary




Top of Page

 | UP: Assessing the retrieval

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.1 Underlying assumptions





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Underlying assumptions

As with all scientific models, our attempts to evaluate the performance
of a search engine rests on a number of assumptions. Many of these
involve the user, and simplifying assumptions about how users assess the
relevance of documents.


{\bf Real FOA versus laboratory retrieval}


From
the FOA perspective, users retrieve documents as part of an extended
search process. They do this often, and because they need the
information for something important to them. If we are to collect useful
statistics about FOA, we must either capture large numbers of such users
``in the act,'' (i.e., in the process of a real, information seeking
activity), or we must attempt to create an artificial, laboratory
setting. The former is much more desirable, but makes strong
requirements concerning a desirable corpus, a population of users, and
access to their retrieval data. So typically we must do the latter, and
work in a lab. The first big assumption, then, is that our lab setting
is similar to real life; i.e., ``guinea pig'' users will have reactions
that mirror real ones. {Yes and no. The Web generally and Web engines in
particular obviously generate huge traffic, and potentially lots of
interesting data about how real (versus experiment subjects) FOA. But
access to these statistics, most conveniently collected by the Web
search servers, is an increasingly value commodity! Many people would
like to know what sorts of things people are searching for, and how they
are search for it.


It's also important not to think of the Web that
everyone is searching as ``the same corpus.'' One of the Web's most
salient features is its dynamism. New documents are added and others (or
at least the links to them!) are removed all the time. This makes
comparing search retrieval results at two different times difficult.}


{\bf
Inter-subject reliability}


Even if we assume we have a typical user and
that this user is engaged in an activity that at least mirrors the
natural FOA process, we have to believe that this user will assess
relevance the same as everyone else! But clearly the educational
background of each user, the amount of time he/she has to devote to the
FOA process relative to the rest of the task and similar factors will
make one user's reaction differ from another's. For example, there is
some evidence that stylistic variations also impact perceived
relevance[Karlgren96] . The {\em
consensual} relevance statistic (cf. Section <A
HREF="foa-4-3-2.html">&sect;4.3.2 ) is one mechanism for aggregating
across multiple users.


This becomes a concern with INTER-SUBJECT
RELIABILITY . If we intend to make changes to document
representations based on one user's RelFbk opinions, we would like
to believe that there is at least some consistency between this user's
opinion and those of others. This is a critical area for further
research, but some encouraging, prelimary results are available. For
example, users of a multi-lingual retrieval system which presents some
documents in their first language (``mother tongue'') and others in
foreign languages they read less well, seem to be able to provide
consistent RelFbk data even for documents in their second, weaker
language! [Sheridan96] .


 {\bf
Independence of inter-document relevance assessments}


Finally, notice
that the atomic element of data collection for relevance assessments is
typically a $(query_{i}, document_{j})$ pair: $ document_{j}$ is
relevant to $query_{i}$. Implicitly, this assumes that the relevance of
a document can be assessed independently of assessments of
other documents. Again, this is a very questionable assumption.


Recall
also that often the {proxy} on which the user's relevance assessment
depends is distinct from the document itself. The user sees only the
proxy, a small sample of the document in question, for example its
title, first paragraph, or bibliographic citation. While we must
typically take user reaction to the proxy as an opinion about the whole
document, this inference depends critically on how informative the proxy
is. Cryptic titles and very condensed citation formats can make these
judgements suspect. And of course the user's ultimate assessment of
whether a document is relevant, after having read it, remains a paradox.




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.10.1 Expected search length





FOA Home

 | UP: Other measures



Expected search length

The ordered list of relevance assessments described in Section
(FOAref) also recommends another, holistic evaluation of the
entire retrieval's behavior; this method is known as EXPECTED SEARCH
LENGTH (ESL). ESL considers the length of a ``path'' as users walk
down the ordered hit list, measuring how many irrelevant documents were
seen on this path before each relevant document; ``expected'' refers to
the average length of each path ending in a relevant document. Cooper
initially proposed this model to measure the work a search engine saves,
in comparison to searching the entire collection at random [<A
HREF="bibrefs.html#Cooper68">Cooper68] .


Given that a search engine
retrieves documents in hitlist order, ESL also requires some criterion
by which the users' wandering paths are stopped. discusses a number of
predicates that might be used to terminate the search: some fixed number
of relevant documents, some fraction of all relevant documents, etc.
Since the generality of queries can vary considerably, it is desirable
to terminate the ESL after some {\em fixed} fraction $E$ of relevant
documents have been retrieved.


For this same reason, it makes sense to
normalize ESL with respect to the number of relevant documents we might
expect to retrieve, if we were retrieving simply at random. If we use $$
for the number of retrieved documents (i.e., those satisfying the
predicate mentioned above), we can estimate the expected random search
length $\mathname{RandSL}$ as: \beq \mathname{RandSL} \equiv
{{\mathname{NRet}\cdot(\mathname{NDoc}-\mathname{Rel})}\over{\mathname{Rel}}+1}
\eeq then the EXPECTED SEARCH LENGTH REDUCTION FACTOR :


\mathname{ESL-RF}
\equiv {{\mathname{RandSL}-\mathname{ESL}}\over{\mathname{RandSL}}} \eeq
captures the amount a real search method improves over the random case.
Compute the ESL of a random retrieval using each of the two predicates:
\benum \item $N$ relevant documents; \item $P$ percent of relevant
documents. \eenum Discuss the effect of the measures relative to query
generality.




Top of Page

 | UP: Other measures

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.10.2 Operating characteristic curves





FOA Home

 | UP: Other measures



Operating characteristic curves

Swets [Swets63] enumerated a number
of abstract desiderata (and quoted by \vanR{155}) that we might wish for
any assessment measure. According to these, IR's standard Re/Pre plot
leaves much to be desired, in particular because this two-dimensional
assessment makes direct comparision impossible. Swets therefore
recommends an analysis from the perspective of signal detection, based
on several key assumptions: \item[(A1)] There is a ``relevant'' signal
we wish to distinguish from background noise. We can consider the worst
case to be comparison against an ``irrelevant'' signal, with both
signals imposed over the data collection. We can imagine that this
signal is generated by the presence or absence of some keywords;
\item[(A2)] These two signals are to be discriminated according to only
a single dimension; and \item[(A3)] These signals are both distributed
{\em normally} across the corpus.


 In this idealized case, we get a
picture something like Figure (figure) . Then, since our corpus
has been ordered by the ranking, the goal becomes to select a value
${\bf \mathname{Rank}}_\tau$ that best separates these two modal
distributions.


Using a simple retrieval rule that retrieves a document
just in case its value is above the threshold ${\mathname{Rank}}_\tau$,
wherever we place this threshold we are bound to make two types of
errors. There will be some \Rel documents which fall below our threshold
(cross-hatched in \blue in Figure (FOAref) ) and some
irrelevant documents which fall above it (cross-hatched in \red).
Following signal detection theory we can call the first set ``FALSE-''
errors, and the second ``FALSE+'' errors. (These are often called Type 1
and Type 2 errors, resp.) Note that the ratio of the right tail of the
\Rel curve (that area {\em not} cross-hatched in \blue in Figure
(FOAref) ) to the total area under the \Rel curve corresponds
exactly to the Recall measure defined earlier (Equation
(FOAref) ), while the ratio of the right tail of the NRel curve
(\red cross-hatched in Figure (FOAref) ) to the total area
under the NRel curve corresponds exactly to Fallout (Equation
(FOAref) ).


The paremetric curve defined by the percentage of
\mathname{Rel} \) versus \( \overline{\mathname{NRel}} \) documents
retrieved as \( \tau \) is varied is called the OPERATING
CHARACTERISTIC CURVE . Obviously, if these two distributions are
identical, this curve will be exactly a diagonal line, from (0,0) to
(1,1). If the mean value of the \( \mathname{Rel} \) distribution is
greater than that of the \( \overline{\mathname{NRel}} \), the OC curve
is moved closer to the upper-left corner, as shown in Figure
(figure) .


While Swets (and subsequently others [<A
HREF="bibrefs.html#Robertson69">Robertson69] [<A
HREF="bibrefs.html#Bookstein77">Bookstein77] ) then considered
fairly elaborate tests to discriminate the relative performance of
retrieval systems with respect to such curves, it is fair to say that
\vanR{154} assessment of 1979 still stands: ``...although Swets' model
is theoretically attractive and links IR measurements to readymade and
well-developed statistical theory, it has not found general acceptance
among workers in the field.'' Optimal selection of ${\bf
\mathname{Rank}}_\tau$ depends on specification of the COSTS
(losses) of making FALSE+ or FALSE- errors. For example, if you are an
over-worked and underpaid law clerk and you read an irrelevant document
(FALSE+) you've wasted precision attention, but that's all; if you miss
a reference you should have found (FALSE-) the cost might be huge. But
if you're a partying undergraduate with one more term paper between you
and summer vacation, your assessments might be quite different. Section
&sect;5.5.6 gives an example of how
explicit models of these various costs can be incorporated within a
Bayesian decision-making framework.




Top of Page

 | UP: Other measures

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.10 Other measures





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Other measures

The performance measures already listed are by far the most common ways
in which search engine are evaluated in the literature. Several others,
however, have been important in the past and may again prove useful in
some situations.

Subsections

	 4.3.10.1 Expected search length
	 4.3.10.2 Operating characteristic curves




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.2 Consensual relevance





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Consensual relevance

In most search engine evaluation, the assumption has been that a single
expert can be trusted to provide reliable relevance assessments. Whether
any one, ``omniscent'' individual is capable of providing reliable data
about the appropriate set of documents to be retrieved remains a
foundational issue within IR. For example, a number of papers in a
recent special issue of the Journal of the American Society for
Information Systems devoted to relevance advocated a move towards a
more ``user-centered, situational,'' view of relevance [<A
HREF="bibrefs.html#Froehlich94">Froehlich94] .


Our attention to the
opinions of individual users suggests the possibility of {combining}
evidence from {\em multiple} human judges. Rather than having relevance
be a Boolean determination made by a single expert, we will consider
``relevance''to be a {\em consentual, central tendancy of the searching
users' opinions}. The relevance assessments of individual users and the
resulting central tendancy of relevance is suggested by Figure
(figure) . Two features of this definition are significant.
First, consentual relevance posits a ``consumers'' perspective on what
will count as IR system success. A document's relevance to a query is
not going to be determined by an expert in the topical area, but by the
users who are doing the searching. If they find it relevant, it's
relevant, whether or not some domain expert thinks the document
``should'' have been retrieved.


Second, consentual relevance becomes a
statistical, aggregate property of multiple users' reactions rather than
a discrete feature elicited from any one individual. By making relevance
a {statistical} measure, our confidence in the relevance of a document
(with respect to a query) increases as more relevance assessment data is
collected. This reliance on statistical stability creates a strong link
between IR and machine learning (cf. Chapter <A
HREF="foa-7.html">&sect;7 ). Allen's investigation into
idiosyncratic cognitive styles of browsing users [<A
HREF="bibrefs.html#Allen92">Allen92] , and Wilbur's assessment of
the reliability of RelFbk across users [<A
HREF="bibrefs.html#Wilbur98">Wilbur98] provide a more textured view
of how multiple relevance assesments can be compared and combined.


It
seems, however, that our move from omniscent to consentual relevance has
only made the problem of evaluation that much more difficult. Test
corpora must be large enough to provide robust tests for retrieval
methods, and multiple queries are necessary in order to evaluate the
overall performance of a search engine. Getting even a single person's
opinion about the relevance of a document to a particular query is hard,
and we are now interested in getting many! However, software like RAVe
(cf. Section &sect;4.4 ) allows an IR
experimenter to effectively collect large numbers of relevance
assessments for an arbitrary document corpus.




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.3 Traditional evaluation methodologies





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Traditional evaluation methodologies

Before surveying all of the ways that evaluation might be
performed, it is worthwhile sketching how it has typically been done in
the past [Cleverdon63] . In the
beginning, computers were slow, had very limited disk space and even
more limited memories; initial test corpora needed to be small, too. One
benefit of these small corpora was that it allowed at least the
possibility of having a set of test queries compared
exhaustively against each and every document in the corpus.


The
source of these test queries, and the assessment of their relevance,
varied in early experiments. For example, in the Cranfield experiments
[Lancaster68] , 1400 documents in
metallurgy were searched according to 221 queries generated by some of
the documents' authors. In Saltan's experiments with the ADI corpus, 82
papers presented at a 1963 American Documentation Institute meeting were
searched against 35 queries and evaluated by students and ``staff
experts" associated with Saltan's lab [<A
HREF="bibrefs.html#Salton68">Salton68] . Lancaster's construction of
the MEDLARS collection was similar [<A
HREF="bibrefs.html#Lancaster69">Lancaster69] .


As computers have
increased in capacity, reasonable evaluation has required much larger
corpora. The Text Retrieval Conference (TREC), begun in 1992 and
continuing to the present, has set a new standard for search engine
evaluation [Harman95] . The TREC
methodology is notable in several respects. First, it avoids exhaustive
assessment of all documents by using the POOLING method, a
proposal for the construction of ``ideal'' test collections that
predates TREC by decades [<A
HREF="bibrefs.html#SparckJones76">SparckJones76] . The basic idea is
to use each search engine independently and then ``pool'' their results
to form a set of those documents that have at least this recommendation
of potential relevance. All search engines retrieve ranked lists of $k$
potentially relevant document, and the union of these retrieved
sets are presented to human judges for relevance assessment.


In the case
of TREC, $k=100$ and the human assessors were retired security analysts,
like those that work at the National Security Agency (NSA), watching the
world's communications. Since only documents retrieved by one of the
systems being tested are evaluated there remains the possibility that
relevant documents remain undiscovered and we might worry that our
evaluations will change as new systems retrieve new documents and these
are evaluated. Recent analysis seems to suggest that at least in the
case of the TREC corpus, evaluations in fact are quite stable [<A
HREF="bibrefs.html#Voorhees98">Voorhees98] .


An important consequence
of this methodological convenience is that unassessed documents are
assumed to be irrelevant. This creates an unfortunate dependence on
the retrieval methods used to nominate documents, which we can expect to
be most pronounced when the methods are similar to one another. For
example, if the alternative retrieved sets are the result of
manipulating single parameters of the same basic retrieval procedure,
the resulting assesments may have overlap with, and hence be useless for
comparison of, methods producing significantly different retrieval sets.
For the TREC collection, this problem was handled by drawing the top 200
documents from a wide range of 25 methods which had little overlap [<A
HREF="bibrefs.html#Harman95">Harman95] . Vogt [<A
HREF="bibrefs.html#Vogt98">Vogt98] has explored how similarities and
differences between retrieval methods can be similarly exploited as part
of combined, hybrid retrieval systems (cf. Section <A
HREF="foa-7-4-4.html">&sect;7.4.4 ).


It is also possible to
sample a small subset of a corpus, submit the entire sample to
review by the human expert, and extrapolate from the number of
relevant documents found to an expected number across the entire corpus.
One famous example of this methodology is Blair \& Maron's assessment of
IBM's STAIRS retrieval system [<A
HREF="bibrefs.html#Blair85">Blair85] of the early 1980's. This
evaluation studied the real-world use of STAIRS by a legal firm as part
of a LITIGATION SUPPORT task: 40000 memos, design documents, etc.
were to be searched with respect to 51 different queries. The lawyers
themselves then agreed to evaluate documents' relevance. As they report:
To find the unretrieved relevant documents we developed sample frames
consisting of subsets of unretrieved database that we believed
to be rich in relevant documents\ldots. Random samples were taken from
these subsets, and thesamples were examined by the lawyers in a blind
evaluation; the lawyers were not aware they were evaluating sample sets
rather than retrieved sets they had personally generated. The total
number of relevant documents that existed in these subsets could then be
estimated. We sampled from subsets of the database rather than the
entire database because, for most queries, the percentage of relevant
documents in the database was less than \%2, making irt almost
impossible t have both manageable sample sizes and a high level of
confidence in the resulting Recall estimates. Of course, no
extrapolation to the entire database could be made from these Recall
calculations. Nonetheless, the estimation of the numbe of relevant
unretrieved documents in the subsets did give us a maximum value for
Recall for each request. [p. 291--293] This is a difficult methodology,
but it allows some of the best estimates of Recall available, still. And
their news was not good: on average retrievals captured only 20% of
Relevant documents!


 In short, methodologies for valid search engine
evaluations require much more sophistication and care than generally
appreciated. Careful experimental design [<A
HREF="bibrefs.html#Tague-Sutcliffe92">Tague-Sutcliffe92] ,
statistical analysis [Hull93] and
presentation [Keen92] are all
critical.




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.4 Basic measures





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Basic measures

Figure (figure) shows the relationship between relevant (\Rel)
and retrieved (\Ret) sets as a Venn Diagram, against the backdrop of the
universe $U$ of the rest of the documents of the corpus. Obviously our
focus should be on those documents that are in the intersection
of \Rel and \Ret, and making this intersection as large as possible.
Informally, we will be most happy with a \Rel set when it best overlaps
with the \Ret set, and therefore seek evaluation measures which reflect
this. The basic relations between the sizes of these sets can
also be captured in the CONTINGENCY TABLE of Figure
(figure) $ is the number of documents not retrieved,
$\mathname{NDoc}$ is the total number of documents, $\mathname{NRel}$ is
the number of relevant documents, $\mathname{NNRel}$ is the number of
irrelevant document.}


We know we want the intersection of and \Ret sets
to be large, but large relative to what?! As mentioned in Chapter 1, if
we are most focused on the \Rel set and use it as our standard of
comparison, we'd like to know what fraction of these we've retrieved.
This ratio is called RECALL : \beq \mathname{Recall}\equiv
{{\left| \mathname{Ret}\cap\mathname{Rel} \right| }
\over{\left|\mathname{Rel}\right|}} \eeq


Anticipating the probalistic
analysis of Section &sect;5.5 , we can think
of Recall as (an estimate of) the conditional probability that
a document will be retrieved, given that it is relevant:
$\mathname{Pr}(\mathname{Ret}|\mathname{Rel})$.


Conversely, if we instead
focus on the $$ set, we are most interested in what fraction of these
are relevant; this ratio is precision: \beq \mathname{Precision}\equiv
{{\left| \mathname{Ret}\cap\mathname{Rel} \right|}
\over{\left|\mathname{Ret}\right|}} \eeq Similarly, this is the
probability that a document will be relevant, given that it is
retrieved:$\Pr(\mathname{Rel}|\mathname{Ret})$. A closely related but
less common measure is called FALLOUT , where we (perversly!)
focus on the irrelevant documents and the fraction of them retrieved:
\beq \mathname{Fallout}\equiv
{\left|{\overline{\mathname{Ret}}\cap\mathname{Rel}}\right| \over
{\left|\overline{\mathname{Ret}}\right|}} \eeq This is
$\Pr(\mathname{Ret}|\overline\mathname{Rel})$. These two measures,
Recall and Precision, have remained the bedrock of search engine
evaluation since they were first introduced by Kent in 1955 [<A
HREF="bibrefs.html#Kent55">Kent55] [<A
HREF="bibrefs.html#Saracevic75">Saracevic75] .


The close relationship
between these three measures can be defined precisely, if the generality
$G$ of the query (cf. Sect. &sect;4.3.7 )
is known: \beq \mathname{Precision}= {{\mathname{Recall}\cdot
{G}}\over{{\mathname{Recall}\cdot {G}} + {\mathname{Fallout}\cdot
(1-{G})}}} \eeq By far the most common measures of search engine
performance are just the pair of measures, precision and recall.


Ideally,
of course, we'd like a system which has both high precision and
high recall: only relevant documents and all of them. But real-world,
practical systems must select documents based on features that are only
statistically useful indicators of relevance; we can never be sure. In
this case efforts made to improve recall must retrieve more documents,
and it is likely that precision will suffer as a consequence. The best
we can hope for is some balance.




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.5 Ordering the \Ret set





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Ordering the \Ret set

 Do not worry about large numbers of results: the best ones come
first! [AltaVista, 1998]


 The next step is to move beyond thinking
of as simply a set. We will suppose that retrieved documents are
returned {\em in some order} by the search engine, reflecting its
assessment of how well each document matches the query. Following
current Web vernacular, we will call this ordering of the \Ret set a
HITLIST and a retrieved document's position its HITLIST
RANK ${\bf \mathname{Rank}}(d_{i})$. This is a positive integer
assigned to each document in the \Ret set, in descending order of
similarity with respect to a the matching function ${\bf
\mathname{Match}}(q,d)$: {\bf \mathname{Match}}(q,d) \;\in \Re \nonumber
\\ {\bf \mathname{Rank}}(d) \;\in J^{+} \nonumber \\ {\bf
\mathname{Rank}}(d_{i}) < {\bf \mathname{Rank}}(d_{j})
\Longleftrightarrow {\bf \mathname{Match}}(q,d_{i}) > {\bf
\mathname{Match}}(q,d_{j})


 Sparck Jones [<A
HREF="bibrefs.html#SparckJones72">SparckJones72] and others have
historically referred to a document's rank in \Ret as its
COORDINATION LEVEL . Strictly speaking, coordination level refers
to the number of keywords shared by document and query. In Boolean
retrieval systems, sensitive only to the presence or absence of
keywords, ranking by coordination level may be the only measure on
document/query similarity availability.


For long queries, hitlist rank
and coordination level are likely to be similar, since it is unlikely
that different documents will match exactly the same number of words
from the query. But for short queries it is likely that coordination
level will only partially order the \Ret set. This is why
\vanR{161}, speaking of the Boolean systems typical at that time, says,
``Unfortunately, the ranking generated by a matching function is rarely
a simple ordering, but more commonly a weak ordering.'' Most modern
search engines, however, exploit keyword weightings and can
provide much more refined measures, thereby providing a TOTAL
ORDERING of the hitlist.


According to the Probability Ranking
Principle (cf. Section &sect;5.5.1 ), a
retrieval system is performing optimally just in case it retrieves
documents in order of decreasing probability of relevance. For
now we simply assume that there is a total ordering imposed over \Ret.
We will use the hitlist ranking to effectively define a series of
retrievals. Setting a very high threshold on this ordering would mean
retrieving a very small set, while setting a lower threshold will
retrieve a much larger one.


Now consider a particular query $q$ and the
set $Rel_{q}$ of relevant documents associated with it. Assuming that is
totally ordered makes it possible for us to define the fundamental
analytic tool for search engine performance: the RECALL/PRECISION
CURVE (Re/Pre Curve). The basic procedure is to consider each
retrieved document in hitlist rank order and ask the precision and
recall of a retrieval of all documents up to and including this one.


Consider
the first of the two hypothetical retrievals shown in Table
(FOAref) .


 With respect to this query, we will assume there are
exactly five relevant documents out of a total of 25 in the corpus. The
very first one retrieved is deemed relevant; if we stopped retrieval at
this point, our recall would be 0.2 (since we have retrieved 1 of 5
relevant documents) and our precision is perfect (the one retrieved
document is relevant). Our good luck continues as we consider the next
document, also relevant; this generates a second Re/Pre data point of
(0.4,1.0). We are not so lucky with the third document retrieved , and
precision drops to 2/3 while recall remains at 0.4. Proceeding down the
retrieval in Rank order, and plotting each and every point in this
fashion gives the Re/Pre curve shown in Figure (figure) .


At
this point we can already make several observations . Asymptotically, we
know that the final recall must go to one; once we have retrieved every
document we've also retrieved every relevant document. The precision
will be the ratio of the number of relevant documents to the total
corpus size. Ordinarily, unless we are interested in very general
queries, and/or very small sets of documents, this ratio will be very
close to zero.


The other end of the curve, however, turns out to be much
less stable. We would hope that a retrieval system's very first
candidate for retrieval, that document with hitlist rank = 1, will be
relevant but it may not. Figure (figure) shows a second pair of
hypothetical data points (in \blue), corresponding to the case that a
single irrelevant document is ranked higher than the relevant
ones. This relatively small change in assessment creates a fairly
dramatic effect on the curve, with real consequence once we need to
juxtapose multiple queries' curves (see Section <A
HREF="foa-4-3-7.html">&sect;4.3.7 , below). Such instability is an
inevitable consequence of the definitions of $\mathname{Precision}$ and
$\mathname{Recall}$: if the first retrieved document happens to be
relevant its Re/Pre coordinates will be $$,
otherwise it will be $$.


Figure (figure) puts this
particular retrieval in the context of the best and worst retrievals we
might imagine. The best possible retrieval would be to retrieve the five
relevant documents first, and then all other documents. This would
produce the upper, square Re/Pre curve. Alternatively, the worst
possible retrieval would retrieve all but the relevant documents before
returning these; this produces the lower line.




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.6 Normalized recall and precision





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Normalized recall and precision

The best/worst ``envelope'' surrounding an actual Re/Pre curve is
related to a similar comparison known as NORMALIZED RECALL [<A
HREF="bibrefs.html#Rocchio66">Rocchio66] . Imagine plotting the
fraction of relevant documents retrieved as a function of the
fraction of the total number of documents retrieved. Such a function is
plotted in Figure (figure) . Comparing the area between the
actual retrieval and worst-case (colored \yellow) to the total area
between best and worst cases (that above the \blue region) is very much
like the best-/worst-case envelope of Figure (FOAref) . We can
derive expressions for this area. Let $r_i$ be the hitlist rank of the
$i$-th relevant document. Then (if we define $r_0=0$): \beq
\mathname{Actual} =
\sum_{i=1}^{\mathname{NRel}}(r_i-r_{i-1}){i\over{\mathname{NRel}}} \eeq
In the best case $r_i=i$: \mathname{Best} &=&
\sum_{i=1}^{\mathname{NRel}} {(i-(i-1))\cdot i\over{\mathname{NRel}}}
\nonumber \\ &=&{1\over\mathname{NRel}}\sum_{i=1}^{\mathname{NRel}}i
\nonumber \\ &=&{\mathname{NRel}+1\over2} when $\mathname{NRel}
\rightarrow$. In the worst case $r_i=NDoc-NRel+i$: \mathname{Worst}&=&
\sum_{i=1}^{\mathname{NRel}} {{((\mathname{NDoc}-\mathname{NRel}+i) -
(\mathname{NDoc}-\mathname{NRel}+i-1))\cdot i}\over \hbox{NRel}}
\nonumber \\ &=& \hbox{NDoc}-\mathname{NRel}




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.7 Multiple retrievals across varying queries





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Multiple retrievals across varying queries

The next step of our construction is to go beyond a single query to
consider the performance of a system across a set of queries. It should
come as no surprise, given the wide range of activities in which FOA is
a crucial component that there is enormous variability among the kinds
of queries produced by users.


One obvious dimension to this variability
concerns the ``breadth'' of the query: How general is it? If the set for
a query is known, this can be quantified by GENERALITY ,
comparing the size of \Rel to the total number of documents in the
corpus: \beq \mathname{Generality}_q \equiv
{\left|\mathname{Rel}_q\right|\over\mathname{NDoc}} \eeq There are many
other ways in which queries can vary, and the fact that different
retrieval techniques seem to be much more effective on some types of
queries than others makes this a critical issue for further research.
For now, however, we will treat all queries interchangeably but consider
average performance across a set of them.


Figure (figure)
juxtaposes two Re/Pre curves, corresponding to two queries. Query 1 is
as before while Query 2 is a more specific query, as evidenced by its
lower asymptote. Even with these two queries, we can see that in general
there is no guarantee that we will have Re/Pre data points at the
desired recall level. This necessitates INTERPOLATION of data
points at required recall levels. The typical interpolation is done at
pre-specified recall levels, for example ${0, 0.25, 0.5, 0.75, 1.0}$. As
\vanR{152} discusses, a number of interpolation techniques are
available, each with their own biases. Since each new relevant document
added to our retrieved set will produce an increase in precision
(causing the saw-tooth pattern observed in the graph), simply using the
next available data point above a desired recall level will produce an
over-estimate, while using the prior data point will produce an
under-estimate.


With pre-established recall levels, we can now juxtapose
an arbitrary number of queries, and average over them at these levels.
For 30 years the most typical presentation of results within the IR
community is the 11-POINT AVERAGE curves, like those shown in
Figure (figure) [REF563] [<A
HREF="bibrefs.html#Salton68">Salton68] . (This data happens to show
performance on the ADI corpus of Boolean versus weighted retrieval
methods, include only the last 10 data points.)


It is not uncommon to see
research data reduced even further. For if queries are averaged at fixed
recall levels, and then all of these recall levels are averaged
together, we can produce a single number that measures retrieval system
performance. Note the even more serious bias this last averaging
produces, however. It says that we are as interested in how well the
system did at the 90% percent recall level as at 10\%!? Virtually all
users care more about the first screen full of hits they retrieve than
the last.


This motivates another way to use the same basic Re/Pre data.
Rather than measuring at fixed recall levels, statistics are collected
at the 10, 25, 50 document retrieval levels. Precision within the first
10 or 15 documents is arguably a much closer measure of standard browser
effectiveness than any other single number.


All such atempts to boil the
full Re/Pre plot are bound to introduce artifacts of their own. In most
cases the full Re/Pre curve picture is certainly worth a thousand words.
Plotting the entire curve is straight-forward and immediately
interpretable, and lets the viewer draw more of their own conclusions.


We
must guard against taking our intuitions based on this tiny example
(with only 25 documents in the entire corpus) too seriously when
considering results from standard corpora and queries. For example, our
first query had fully twenty percent of the corpus as relevant; even our
second query had eight percent. In a corpus of a million documents, this
would mean eighty thousand of them were relevant!? Much more typical are
queries with a tiny fraction, perhaps .001% relevant. This will mean
that the precision asymptote is very nearly zero. Also, we are likely to
have many, many more relevant documents, resulting in a much smoother
curve.




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.8.1 Sliding ratio





FOA Home

 | UP: One-parameter criteria



Sliding ratio

The fact that the set is ordered makes it useful to compare two rank
orderings directly. If the ``correct,'' idealized ranking is known (for
example, one corresponding to perfectly decreasing probability of
relevance), then an actual search engine's hitlist ranking can be
compared against this standard. More typically, the rankings of two
retrieval systems are compared to one another.


Given two rankings, we
will prefer the one which ranks relevant documents ahead of irrelevant
ones. If our relevance assesments are binary, with each document simply
marked as relevant or Irrelevant, the normalized recall measure
considered in Section &sect;4.3.6 (or the
expected search length measure to be described in Section <A
HREF="foa-4-3-10-1.html">&sect;4.3.10.1 ) is the best we can do in
distinguishing the two rankings.


But if we assume instead that it is
possible to impose a more refined measure $(d_i)$ than simply \Rel/\IRel
(e.g., recall the richer preference scale of Figure (FOAref) ),
more sophisticated measures are possible. In this case, we prefer a
ranking which ranks $d_i$ ahead of $d_j$ just in case
$\mathname{Rel}(d_i) > \mathname{Rel}(d_j)$. One way to quantify this
preference is to sum the $\mathname{Rel}(d_i)$ for the $\mathname{NRet}$
most highly ranked documents: \beq \sum_{i=1}^{NRet} Rel({d_i}) Ret}
Rel({d_i}) } Rel({d_i}) el({d_i}) {d_i}) i}) \eeq


The ratio of this
measure, computed for each of the two systems' rankings, is called the
SLIDING RATIO score [<A
HREF="bibrefs.html#Pollack68">Pollack68] : \beq
\frac{\sum_{i=1}^{{Rank_1}({d_i})({d_i})\ \preceq {NRet}} {Rel}({d_i})}
{\sum_{i=1}^{{Rank_2}({d_i})\ \preceq {NRet}} {Rel}({d_i})} \eeq As
$NRet$ increases, this ratio comes closer to unity: \beq \lim_{{NRet}\
\rightarrow {NDoc}} \frac{\sum_{i=1}^{{Rank_1}({d_i})\ \leq {NRet}}
{Rel}({d_i})} {\sum_{i=1}^{{Rank_2}({d_i})\ \leq {NRet}} {Rel}({d_i})}
=1 \eeq and so it is most useful for distinguishing between two rankings
when only a small $\mathname{NRet}$ is considered.




Top of Page

 | UP: One-parameter criteria

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.8.2 Point alienation





FOA Home

 | UP: One-parameter criteria



Point alienation

The sliding ratio measure provides a more discriminating measure but
depends entirely on the availability of metric
$\mathname{Rel}(d_i)$ measures for retrieved documents. As discussed in
Section &sect;4.1.1 , it is much easier to
derive non-metric assessments directly from $\mathname{RelFbk}$ data
given naturally as part of users' browsing: \beq \red{\ominus} \prec \#
\prec \blue{\oplus} \eeq


In an effort to exploit the nonmetric
preferences often provided by human subjects, Guttman [<A
HREF="bibrefs.html#Guttman78">Guttman78] has defined a measure known
as POINT ALIENATION . Bartell has pioneered a variation of it for
use with document rankings rated by $\mathname{RelFbk}$ [<A
HREF="bibrefs.html#REF1093">REF1093] . The basic idea is deceptively
simple: Compare the difference in rank between two differentially
preferred documents to the absolute difference of these ranks:
\beq J \equiv \sum_{d \succ d'}
\frac{Rank(d)-Rank(d')}{|Rank(d)-Rank(d')|} \eeq


If $d$ is really
preferred over $d'$ -- $(d d')$ -- (e.g., if some user has marked $d$ as
\Rel but said nothing about $d'$), we can hope that ${\bf
\mathname{Rank}}(d) < {\bf \mathname{Rank}}(d')$ ()$ {\em increases}
from most- to least-highly ranked document, so that the first element of
the hitlist has ${\bf \mathname{Rank}}=1$.} and so the numerator $({\bf
\mathname{Rank}}(d) - {\bf \mathname{Rank}}(d'))$ will be negative; if,
on the other hand, the two documents are incorrectly ordered by the
ranking, the numerator will be positive. Comparing this arithmetic
difference to its absolute value, and then summing over the rankings for
all pairs of documents $(d, d')$ which are differentially preferred $(d
\succ d')$ gives Equation (FOAref) .




Top of Page

 | UP: One-parameter criteria

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.8 One-parameter criteria





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



One-parameter criteria

This section began with Recall and Precision, the two most typical
measures of search engine performance. From that beginning, even richer,
more elaborate characterizations of how well the system is performing
have been considered. But even having the two measures of
Recall and Precision means that it is not a simple matter to decide
whether one system is better or worse than another. What are we to think
of a system that has good Recall but poor precision, relative to another
with the opposite bug/feature?


For example, if we wish to optimize a
search engine with respect to one or more design parameters (e.g., the
exact form of the query/document matching function, cf. Section <A
HREF="foa-5-3-1.html">&sect;5.3.1 ), effective optimization becomes
much more difficult in MULTI-CRITERIAL cases. Such thinking has
generated ``composite'' measures based on the basic components of recall
and precision.


For example, Nordine and van Rijsbergen [<A
HREF="bibrefs.html#Jardine71">Jardine71] [<A
HREF="bibrefs.html#vanR73">vanR73] originally proposed the
F-MEASURE for this purpose \beq F_{\beta} \equiv
\frac{(\beta^{2}+1) \cdot \mathname{Precision} \cdot \mathname{Recall} }
{\beta^{2} \mathname{Precision} + \mathname{Recall}} \eeq \vanR{174} has
since defined the closely related EFFECTIVENESS meaurure $E$
which uses $\alpha$ to smoothly vary the emphasis given to precision vs.
recall: \beq {E_{\alpha}} \equiv 1-
\left(\frac{\alpha}{\mathname{Precision}}+\frac{1-\alpha}{\mathname{Recall}}\right)^{-1}
\eeq The transform $\alpha = \frac{1}{\beta^2 + 1}$ converts easily
between the two formulations, with $E = 1 - F$. \vanR{174} also presents
an argument that a perfectly even-handed balance of precision against
recall at $\alpha = 0.5$ is most appropriate.


As discussed at some length
in Section &sect;7.4 , it is possible to view
retrieval as a type of classification task: given a set of features for
each document (e.g., the keywords it contains), classifiy it as either
\Rel or \IRel with respect to some query. Lewis and Gale [<A
HREF="bibrefs.html#Lewis94b">Lewis94b] have used the $F_{\beta}$
measure in the context of text classification tasks, and also recommend
a focus on the same $\beta=1.0$ balance. CLASSIFICATION ACCURACY
measures how often the classification is correct. If we associate the
choice to retrieve a document with classifiying it as \Rel, we can use
the variables defined in the contingency table of Figure
(FOAref) : \mathname{Accuracy} \equiv \frac{\mid Retr \cap Rel
\mid + \mid \overline{Retr} \cap \overline{Rel} \mid}{\mathname{NDoc}}

Subsections

	 4.3.8.1 Sliding ratio
	 4.3.8.2 Point alienation




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3.9 Test corpora





FOA Home

 | UP: INDIVIDUALS' assessment: search engine  performance



Test corpora

By TEST CORPORA we refer to collections of documents that also
have associated with them a series of queries for which RELEVANCE
ASSESMENTS are available. One of the earliest such test sets was a
collection of 1400 research papers on aerodynamics developed by C.
Cleverdon in the mid-1960's, known as the Cranfield corpus [<A
HREF="bibrefs.html#Cleverdon63">Cleverdon63] . For most of the
1980's, a set of corpora known as CACM, CISI, INSPEC, MED and NPL
(somtimes referred to as the CORNELL CORPORA ) were developed,
maintained and distributed by Gerald Salton and his students at Cornell
and became the {\em de facto} standard for testing within the IR
community. For some time the most influential test corpora have been the
TREC corpora associated
with the Text Retrieval Evaluation Conference meetings \cite{Harman95}


Table
(FOAref) gives a sample of statistics for a number of the most
widely-used corpora. One obvious trend is the increasing size of these
collections over time. The <A
HREF="http://www.research.att.com/~lewis/reuters21578.html">Reuters
corpus classification labels that are invaluable for training
classifiers (cf. Section &sect;7.4 ). With
our AIT corpus, the OSHMED [Hersh94]
is one of the few to provide multiple relevance assessments of the same
$\langle q,d \rangle$ pair.


 Table (FOAref) shows a sample query
from the TREC experiments.


 \footnotesize 1 > Science and Technology 2 >
AIDS treatments 3 > Document will mention a specific AIDS or ARC
treatment. 4 > To be r, a document must include a reference to at least
one specific potential Acquired Immune Deficiency Syndrome (AIDS or AIDS
Related Complex treatment. 5 > 1. Acquired Immune Deficiency Syndrome
(AIDS, AIDS Related Complex (ARC 6 > 2. treatment, drug, pharmaceutical
7 > 3. test, trials, study 8 > 4. AZT, TPA 9 > 5. Genentech,
Burroughs-Wellcome 10 > ARC - AIDS Related Complex 11 > . A set of
symptoms similar to AIDS. 12 > AZT - Azidothymidine, a drug for the
treatment of Acquired Immune Deficiency Syndrome, its related pneumonia,
and for severe AIDS Related Complex. 13 > TPA - Tissue Plasminogen
Activator - a blood clot-dissolving drug. 14 > treatment - any drug or
procedure used to reduce the debilitating effects of AIDS or ARC.
\caption{TREC query} For this query, and hundreds of others like it,
considerable manual effort has gone into assessing whether documents in
the TREC corpus should be considered ``relevant'' or not. Note
especially the way ``basic'' query (Line 2) has been embellished with
general and specific topical orientation (Lines 1,3), important terms
and abbreviations have been explicated, etc. This is much more
information than most users typically provide, but it also allows much
more refined assessments of systems' performance.


As the testing
procedures of the TREC participants have developed over the years,
multiple ``tracks'' have formed, corresponding to typical search engine
usage patterns. The task on which we have focused throughout this
section is termed AD HOC RETRIEVAL , in the sense that a constant
corpus is repeatedly searched with respect to a series of ad
hoc queries. This is distinguished from the ROUTING task,
which assumes a relatively constant, standing set of queries (for
example, corresponding to the interests of various employees of the same
corporation). Then,an on-going stream of documents is compared, with
relevant documents routed appropriate recipients.


More recently, a
special type of routing termed FILTERING has also been
considered. In the filtering task, the standing query is allowed to
adapt to the stream of RelFbk generated by the users as they
receive and evaluate routed documents (cf. Section <A
HREF="foa-7-3.html">&sect;7.3 ).




Top of Page

 | UP: INDIVIDUALS' assessment: search engine  performance

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.3 INDIVIDUALS' assessment: search engine  performance





FOA Home

 | UP: Assessing the retrieval



INDIVIDUALS' assessment: search engine  performance

The last section considered the assessment problem from the perspective
of a single individual, the browsing user. Now we would like to
generalize on this individual performance to attempt to obtain
statistically significant observations.

Subsections

	 4.3.1 Underlying assumptions
	 4.3.2 Consensual relevance
	 4.3.3 Traditional evaluation methodologies
	 4.3.4 Basic measures
	 4.3.5 Ordering the \Ret set
	 4.3.6 Normalized recall and precision
	 4.3.7 Multiple retrievals across varying queries
	 4.3.8 One-parameter criteria
	 4.3.9 Test corpora
	 4.3.10 Other measures




Top of Page

 | UP: Assessing the retrieval

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.4.1 RAVeUnion





FOA Home

 | UP: RAVe: A Relevance Assessment VEhicle



RAVeUnion

It would be most useful if, for every query, the relevance of every
document could be assessed. However, the collection of this many
assesments, for a corpus large enough to provide a real retrieval test,
quickly becomes much too expensive. But if the evaluation goal is
relaxed to being the relative comparison of one retrieval
system with one or more alternative systems, assesments can be
constrained to only those documents retrieved by one of the systems.


We
therefore follow the POOLING procedure used by many other
evaluators, viz., using the proposed retrieval methods themselves as
procedures for identifying documents worth assessing.


The first step in
constructing a RAVe experiment is to combine the ranked retrieval lists
of the two or more retrieval methods, creating a single list of
documents ordered according to how interested we are in having them
assessed by a subject. We call this function RAVeUnion.



{First, the assessment of a document whose ranked order is highly
correlated across retrieval methods provides little information about
differences between the methods. Said another way, we can potentially
learn most from those documents whose rank order is most different, and
hence a measure of the difference in ranked orders of a particular
document might be used to favor ``controversial'' documents. This factor
has the unfortunate consequence, however, of being sensitive to what we
would expect to be the least germane documents, those documents ranked
low by any of the methods under consideration. A second factor that
could be considered is a ``sanity check,'' including near the top of our
list a random sample. While we might learn a great deal from these if
users agree that these randomly selected documents are in fact relevant,
we expect that in general the retrieval performance of the systems
should not depend on random documents.}


RAVeUnion produces
the most straight-forward ``zipper'' merge of the lists, beginning with
the most highly ranked and alternating. The output of
RAVeUnion is a file of (query, document) pairs along with a
field which indicates if the pair was uniquely suggested by only one of
the methods. This last information can be used to compare the average
relevance scores of documents suggested by one method alone to those
retrieved by more than one.




Top of Page

 | UP: RAVe: A Relevance Assessment VEhicle

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.4.2 RAVePlan





FOA Home

 | UP: RAVe: A Relevance Assessment VEhicle



RAVePlan

A second challenge in the desired density or redundancy of sample
points. That is, for each document that we believe may be relevant to a
query, how many subjects should evaluate it? The answer will vary
depending on such factors as the number of participants, their
expertise, their motivation to produce quality judgments, how long each
will spend rating documents, etc. A higher density means that less
documents will be evaluated, but also that the inter-subject, cumulative
assesment is likely to be more statistically stable. This can be
especially important when RelFbk is to be used to train the system.


The
trade-off between the most important of these factors is captured in the
following formula: x = \frac{N R}{S T Q} where x & = & number of
documents to be evaluated for each query \\ N & = & number of subjects
\\ R & = & expected subject efficiency (votes/user/time) \\ T & = & time
spent by subjects \\ S & = & desired density (votes/document) \\ Q & = &
number of queries to be evaluated \\


 Note that this formula ignores the
overlap between queries that occurs when users see a document that may
be relevant to two or more of the queries in the their list. Care must
be taken, therefore, to minize expected overlap between the topical
areas of the queries. We have also found that the assessment densitities
constructed using this formula to be unfortunately uneven. The main
source of these is variability in $R$, the rate at which subjects are
able to produce relevance assesments.


RAVePLAN takes as
input a list of $Q$ query specifications, a list of $N$ subject logins,
the desired density $S$, and the number of documents $R*T$ that should
be allocated to each subject. The query specifications indicate which
queries can go in which fields, and which queries should not be shown
together. This allows us to limit possible interactions between queries
about similar topics. { Ideally, RAVePLAN could be folded
into the interactive RAVe facility (described below), so that documents
are allocated dynamically and incrementally, maintaining a
nearly-consistent density on all queries. The experimenters could then
push the experiment alternately in the directions of higher density or
larger candidate-document sets. In its current implementation, however,
RAVePLAN requires that we predetermine the list of
documents that each subject will see. }


RAVePLAN outputs two
files. The plan file, which is an input to the RAVe
interactive application, lists each subject id along with the queries
and document numbers which have been selected for that subject. The
assigments is a list of document-query pairs, which tells
us which query we expect the document to be relevant to.
RAVeCompile uses this file after data collection is
completed to generate true and false-positive measures.




Top of Page

 | UP: RAVe: A Relevance Assessment VEhicle

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.4.3 Interactive RAVe





FOA Home

 | UP: RAVe: A Relevance Assessment VEhicle



Interactive RAVe

The interactive portion of RAVe is an HTML document, as shown in Figure
(figure) . The top of RAVes window displays the three queries
against which the subject is to judge each document. Two queries are
short sentences or phrases, in this case REASONING ABOUT
UNCERTAINTY and LEGAL APPLICATIONS and the third is
a scrolling pane containing the text of the long, \RelFbk-defined
document (in this example, the thesis A PLANNING MODEL WITH
PROBLEM ANALSIS \ldots). While the subject is asked to judge the
documents shown to him or her for being about the two short
queries, the task associated with the \RelFbk-doument query is to find
``documents like this.''


Below each query the RAVe window contains four
radio-buttons labeled ``Not (relevant)'', ``Possibly (relevant)'',
``Relevant'', and ``Critically (relevant)''. Since we asked our subjects
to spend two hours each, but could not assume their participation would
necessarily be continuous, there is a ``QUIT'' button which allows the
subject to suspend the session; when the subject launches RAVe again,
the session will be resumed where he or she left off. Finally, the
``Next'' button is pressed after the subject has read and recorded his
or her relevance assessments for a document.




Top of Page

 | UP: RAVe: A Relevance Assessment VEhicle

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.4.4 RAVeCompile





FOA Home

 | UP: RAVe: A Relevance Assessment VEhicle



RAVeCompile

RAVeCompile processes the votes which the interactive RAVe
records, creating a relevance assessment file and some statistics about
each query and subject. The first step is to map the four-valued
relevance assessments into the Boolean relevant/non-relevant
discriminations typically used by standard IR evaluation techniques.
RAVe Compile lets the experimenter configure a simple predicate of the
following form: .


where


In one set of experiments [<A
HREF="bibrefs.html#REF1133">REF1133] , the data used two predicates
constructed in this fashion. These are:


 \item[Permissive:] if (two or
more POSSIBLE votes) or (at least one RELEVANT vote);


 if (two or more
RELEVANT votes) or (at least one CRITICAL vote)




Top of Page

 | UP: RAVe: A Relevance Assessment VEhicle

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4.4 RAVe: A Relevance Assessment VEhicle





FOA Home

 | UP: Assessing the retrieval



RAVe: A Relevance Assessment VEhicle

Section &sect;4.3.2 argued that the
opinions of many users concerning the relevance of a document to a query
provides a more robust characterization than any single expert. It
seems, however, that our move from omniscent to consentual relevance has
only made the problem of evaluation that much more difficult: Test
corpora must be large enough to provide robust tests for retrieval
methods, and multiple queries are necessary in order to evaluate the
overall performance of an IR system. Getting even a single person's
opinion about the relevance of a document to a particular query is hard,
and we are now interested in getting many!


This section describes RAVe, a
Relevance Assessment VEhicle that demonstrates it is possible to
operationally define relevance in the manner we suggest. RAVe is a suite
of software routines that allow an IR experimenter to effectively
collect large numbers of relevance assessments for an arbitrary document
corpus. It has been used with a number of different classes of students
to collect the relevance assessments used for evaluation with respect to
the AIT corpus; your teacher may be having you participate in a similar
experiment. It can also be used to collect assessments for other
document corpora and query sets.


In this chapter we began by making some
assumptions about users of an search engine in order to figure out just
how well the system is doing at satisfying users' information needs. We
focused on two separate notions of assessment: first, assessing the
relevance of documents retrieved by the system in response to a
particular query, and second, assessing the search engine's overall
utility through aggregating relevance judgements provided by many users
performing many queries.


Section 4.1 discussed both metric and non-metric
relevance feedback, and the difficulties in getting users to provide
relevance judgements for documents in the retrieved set. We saw,
however, that relevance feedback could be used to suggest query
refinements to the users and/or be used to modify the underlying
document representations to improve future system performance.


The
concept of consentual relevance introduced in Section 4.2 addresses an
issue raised in Chapter 1 in which we asked what success criteria can be
used in evaluating a search engine. Consentual relevance tells us that
relevant documents are those documents that many users find to be
useful. We can ask how useful a particular search engine is, or compare
one search engine with another, by posing the question: How useful
(relevant) do users find the documents retrieved in response to queries?


To
answer that question we quantified several measures of system
performance. The generality of a query is a measure of what fraction of
documents in the corpus are relevant to the query. Fallout measures the
fraction of irrelevant documents found in the retrieved set of a given
query. The key notions of recall, the fraction of relevant documents in
the retrieved set, and precision, the fraction of retrieved documents
that are relevant, allow us to make direct comparisons between two
search engines' performances on any query. Other methods of comparison
include sliding ratio, point alienation, expected search length, and
operating characteristic curves.

Subsections

	 4.4.1 RAVeUnion
	 4.4.2 RAVePlan
	 4.4.3 Interactive RAVe
	 4.4.4 RAVeCompile




Top of Page

 | UP: Assessing the retrieval

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 4 Assessing the retrieval





FOA Home

 | UP: foa-book.tex



Assessing the retrieval

We've come a long way since Chapter 1 first sketched the full range of
activities we might consider FOA. As Chapter 2 considered the various
ways of breaking text into indexable features, and Chapter 3 the various
ways of weighting combinations of these features to identify the best
matches to a query, I hope you have been aware how many
alternatives have been mentioned! That is, rarely has there
been a single method which is proveably optimal to all others. IR has
traditionally been driven by empirical demonstrations, and the range of
commercial competitors now trying to provide the ``best'' search of the
WWW makes it likely this performance orientation will continue. Whether
we are scientists interested in objectively assessing the results of one
particular technique, or a consumer of WWW search engine technology
interested in buying/using the best, a solid basis of performance
assessment is critical.


Several perspectives on assessment are
possible. In the first chapter FOA was viewed as a personal
activity, adopting the users' points of view. Section <A
HREF="foa-4-1.html">&sect;4.1 will continue in this theme,
considering how users assess the results of their retrievals and how
they can express their opinions using RELEVANCE FEEDBACK
(\RelFbk). Oddy is credited with first identifying this important stream
of data, naturally provided by users as a part of their FOA browsing [<A
HREF="bibrefs.html#REF179">REF179] [<A
HREF="bibrefs.html#Belkin82">Belkin82] .


But in this book we are also
concerned with FOA from the IR system builder's point of view.
Ideally, we would like to construct a search engine that finds the
``right" documents, those that are most relevant for each query, and for
each user. The second section of this chapter discusses performance
measures of statistical properties that are reliable across
large numbers of users and their highly variable queries. The key to
these measures is having some insight into which documents
should have been retrieved, typically because some
``omniscent'' expert has determined (within a specially-constructed
experimental situation) that certain documents ``should'' have been
retrieved. Alternatively the RelFbk of many users can be combined
to form a CONSENTUAL opinion of relevance, as described in
Section &sect;4.4 .


A concrete notion of
RELEVANCE would seem fundamental precondition for understanding
either an indivudal's RelFbk or how this can be used to assess a
search engine. But in this respect information retrieval generally and
RelFbk in particular is like many other academic areas of study
(artificial intelligence, genetics, information retrieval come to mind)
in that the lack of a fully satisfactory definition of the core concept
(information, intelligence, genes, respectively) has not entirely
stopped progress. That is, a great deal can be done by
``operationalizing'' RelFbk to be simply the production of certain
RELEVANCE ASSESSMENT behaviors as part of a FOA dialog. This
operational simplification will hold us until fundamental issues of
language and communication are again addressed in Section <A
HREF="foa-8-2-1.html">&sect;8.2.1 .

Subsections

	 4.1 INDIVIDUAL'S assessment of relevance
	 4.2 Extending the dialog with \RelFbk
	 4.3 INDIVIDUALS' assessment: search engine  performance
	 4.4 RAVe: A Relevance Assessment VEhicle




Top of Page

 | UP: foa-book.tex

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1.1 Bibliometric analysis of science





FOA Home

 | UP: Citation: inter-document links



Bibliometric analysis of science

Most extensive analysis of citation has been in science. Long before
even Newton [Newton1776]
appreciated ``standing on the shoulders of the giants that came
before,'' scientists have realized that they need one another to
advance. In some cases the reference is to arguments on which a new
author builds; in other cases there is disagreement about hypotheses,
data, etc.


The field of BIBLIOMETRICS has found a great deal of
interesting structure in graphs created by bibliographic citation links.
That is, imagine each document in a corpus is represented by a node in a
graph, and a directed edge is drawn from document $d_{j}$ to $d_{i}$
just in case $d_{j}$ refers to $d_{i}$ in its bibliography.


Figure
(figure) [Price86] shows
this citation structure when the references are ordered by a natural,
temporal feature. In any subject area, papers can be indexed
chronologically, and a dot placed at location $\langle i,j \rangle$ just
in case document $d_{i}$ cites document $d_{j}$. Since citations can run
only backward in time, this graph is upper triangular. phenomena
investigated in the early 1900's. N-rays were a form of radiation first
hypothesized to exist in 1904. After an extended period of
investigation, the community of physicists investigating the question
determined that in fact there were no such things as N-rays! This means
the corpus of documents has a convenient, cleanly defined time period.
The example also provides insight into the larger scientific process:
This is what Science looks like when this engine is entirely divorced
from any underlying phenomena. In general we can, with Plato, imagine
that there is indeed an underlying reality, as well as a social process
of science attempting to describe that reality. We can hope that in most
cases any particular scientist's activities, or that of the community in
which he participates is governed by both influences, that of the
physical reality and of the social process. }


As with many fields, this
one begins with a small number of highly cross-linked papers in the
upper left hand corner. Strong horizontal and vertical stripes can also
be seen against a more uncorrelated background. Horizontal lines
correspond to CLASSIC PAPERS citations: chestnuts that everyone
includes in their bibliography. Vertical stripes are papers that have
much more extensive bibliographies, and stretch much farther back in
time than typical; these are often referred to as REVIEW ARTICLES
. Note how these semantic deterimations can be derived from patterns in
the syntactic facts of citation. Other inferences are also possible.


Perhaps
the most common use of citation graphs is IMPACT ANALYSIS . In
terms of the bibliographic graph, a document's importance, its affect on
a field, is proportional to its IN-DEGREE : the number of
citation links pointing into a document node. Price provides motivation
for this measure: Flagrant violations there may be, but on the whole
there is, whether we like it or not, a reasonably good correlation
between the eminence of a scientist and his productivity of papers. It
takes persistence and perseverence to be a good scientist, and these are
frequently reflected in a sustained production of scholarly writing. [<A
HREF="bibrefs.html#Price86">Price86]


 This suggests a simple
heuristic, widely used by university deans who must quickly evaluate
faculty up for promotion: important authors are those with higher impact
than their peers! The Institute for Scientific Information (ISI) has
made an entire industry of collating bibliographic citations and
inverting them. Its Web of
Science product now makes hypertext navigation of this valuable
information straight-forward. Similar arguments can be extended to
identify important academic departments, universities, even countries.
This mode of analysis, used to evaluate individuals, scientific
institutions and disciplines, consistently makes news when data and
politics cross paths [May97] .


Finally,
as mentioned in &sect;5.2.5 ,
CO-CITATION can be used as a basis for inter-document similarity:
two documents are similar to the extent that their bibliographies
overlap. Bar-Hillel has been credited with the first suggestion of using
co-citation as a similarity metric between documents [<A
HREF="bibrefs.html#BarHillel57">BarHillel57] [<A
HREF="bibrefs.html#Swanson88">Swanson88] ; Henry Small, Eugene
Garfield and others have provided some of the first empirical support
for this hypothesis [Small73] [<A
HREF="bibrefs.html#REF616">REF616] [<A
HREF="bibrefs.html#REF620">REF620] [<A
HREF="bibrefs.html#REF596">REF596] .


So-called INVISIBLE
COLLEGES [REF621] connecting
cliques of self-referential colleagues who are relatively independent of
the rest of science have been identified. Beyond fully isolated cliques,
higher order structure over sets of documents can also be analyzed. we
can imagine that the documents of one discipline have much higher
connectivity among themselves than they do with papers in other
disciplines. A new paper, whose bibliography cites papers coming from
more than one discipline can therefore be imagined to be a new, cutting
edge synthesis!?


Bibliometrics has also made clear many dangers in using
citation data. What we might call the NORM OF SCHOLARSHIP , the
average number of citations in a document, seems to be about 10 to 20
[Price86] . Some scientific
disciplines rely on much longer bibliographies than others; within
discipline, idiosyncratic author variations in bibliography length are
also common.




Top of Page

 | UP: Citation: inter-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1.2 Time-scale





FOA Home

 | UP: Citation: inter-document links



Time-scale

Obviously bibliographic reference can only go back in time, but just how
far back in time a citing author reaches also tells us something. For
typical journal publications, the typical time scale of references is on
the order of years. A large fraction of this time has been, to date,
sensitive to the production schedules of scientific journals. As the
distance between author's words and reader's browsing has shrunk (e.g.,
in fields like physics where the <A
HREF="http://xxx.lanl.gov/cmp-lg/">Los Alamos reprint server has
become a dominant mechanism of dissemination), much of the time now is
due exclusively to delays associated with peer review, revision, etc.
This delay between the time an author is finished with a document and
when it reaches its public obviously provides a lower-bound for
CITATION LAG , the time between a document's publication date and
its' reference by another paper's bibliography.


But independent of
production schedules, there are wide varieties in how far back citations
typically reach. Price sees a deep connection between the social
processes underlying various scientific disciplines. First, he
distinguishes between two gross types of citation, NORMAL AGING
and the IMMEDIACY EFFECT : ``a special hyperactivity of the
rather recent literature'' (p. 164). He defined PRICE'S INDEX to
be the fraction of documents published which are cited {\em within five
years}.


He then goes on to use the width of this research field to
distinguish between ``hard'' and ``soft'' sciences, even
``non-sciences,'' all based on the width of this RESEARCH FRONT
within which most citation is typically made. He sees his Price's Index
as ``corresponding very well with what we intuit as hard science, soft
science and non-science as we descend the scale'' (p. 168). Using
biological metaphors, with different disciplines compared to different
kinds of organisms: Such pathological cases apart, it would seem that
the [Price] index provides a good diagnostic for the extent to which a
subject is attempting, so to speak, to grow from the skin rather than
from the body. With a low index one has a humanistic type of metabolism
which the scholar has to digest all that has gone before, let it mature
gently in the cellar of his wisdom, and then distill forth new words of
wisdom about the same sorts of questions. In hard science the
positiveness of the knowledge and its short term permanence enable one
to move through the packed down past while still a student and then to
emerge at the research front where interaction with one's peers is as
important as the store- house of conventional wisdom. The thinner the
skin of science the more orderly and crysline the growth and the more
rapid the process. (p. 177-178)


 He also infers prescriptions from these
statistics for editors of journals who are in a controlling position to
influence a field: I regard the value of this work as being not only
diagnostic, but also prescriptive, so let us look in closing at what
suggestions it makes for the technology of scientific information. At
the personal level of the scientific writer and the editor and publisher
of journals, it tells that one might well be as careful about references
as we are about titles, authorships, and proper presentation of data....
For a research paper it should be exceptional to allow an author to
eschew interaction with col- leagues by citing too few references, and
if he cited too many, perhaps he is writing a review paper rather than a
research con- tribution. Similarly, if you want to make the field firm
and tight and hard and crystalline you have to play with your peers and
keep on the ball by citing their recent work. (p. 178)




Top of Page

 | UP: Citation: inter-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1.3 Legal citation





FOA Home

 | UP: Citation: inter-document links



Legal citation

The use of citation in legal documents is interesting for a number of
reasons [REF1113] . First, within the
common law tradition adjudicating legal behavior is based on arguments
of stare decis: Stand by previous decision. The ability to
reference prior judicial opinions provides the core of many forms of
documents, including the judicial opinions themselves, briefs, even
legislation. It is no wonder, then, that legal prose has developed an
extensive system of conventions for representing how judicial opinions
relate to one another .


As with scientific papers, the fact of citation 
reference by one judge to the opinion of another  is never in doubt.
But when referring to prior cases, the relevance of prior
decisions often depends on the judges interpretation of the
relation holding between the two opinions.


In fact, an entire
industry exists within legal publishing to do nothing but elaborate the
syntactic fact of reference to a prior ruling with an interpretation of
the purpose for which the citation is made. This process is performed
especially well by Shepherd/McGraw Hill. So critical are the arguments
captured by these citations that the process of checking a prior ruling
a lawyer wishes to reference, to be sure that it hasn't been overruled
or otherwise rendered obsolete, is known as SHEPHERDIZING a case.



Figure (FOAref) shows the entire range of Shepard citation
labels. These are broken into two categories, the first dealing with the
history of a case and the second with its treatment.


To make sense of
this distinction a brief digression into the purpose of legal citation
is necessary. Cases, as they proceed from lower to higher courts, have
basically a binary outcome: they are either won or lost. There is never
ambiguity as to whether a higher court agrees with or overrules the
opinion of a lower court. These unambiguous statements are what are
captured as the history of a case.


But in a common law tradition, a much
larger number of citations refer to cases and decisions in those cases
by other judges. The relationship between these cases and the one before
the author-judge is less clear. But as Figure ?? (figure) makes
explicit, the two cases (citing and cited) have at least two important
dimensions along which they may be similar or dissimilar. First, the set
of facts associated with one case may be very close to those in the
other, or they may be much different facts. Second, the rules of law
which are to be applied may be consistent between one judge and the
other, or they may be contrary. Figure(FOAref) shows these two
dimensions and orders Shepard's treatment labels roughly along
dimensions of this two dimensional similarity space.




Top of Page

 | UP: Citation: inter-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1.4.1 Citations and arguments





FOA Home

 | UP: Discussion



Citations and arguments

Expository documents are arguments: Attempts to convince an audience
that some set of proposition are true. Depending on the type of
document, and the type of audience, the argument may be more or less
formally structured.


Perhaps the most structured kinds of documents are
mathematical papers. Mathematical arguments typically depend on, indeed
are defined by, theorem proving which connects new propositions to
previously established results. Even in the case of mathematical papers
however, it is interesting to observe the crucial role natural language
text continues to play in providing a context for formal theoretical
results. Readers can be persuaded that assumptions are reasonable, that
mathematical definitions capture intuitive relationships, etc. Citations
in a mathematical paper typically point to papers containing theorems
that were used as part of the current proof.


In a scientific paper, the
form of the argument often has to do with the discipline. There is often
data to present, methodology to describe, etc. Many of the citations are
typically gathered in a section relating the current piece of work to
prior literature. This prior literature may be referenced for at least
two, very different purposes. Most typically the prior work is used much
as theorems in mathematical papers, to establish a result from which the
current work proceeds. The current author points to the work of a prior
author as providing validation for a proposition they both believe to be
true. Another potential reason for citation is quite the opposite. In
this case, the current author is interested in debating a position put
forward by the cited work.


This polarity, reflecting whether the cited
work and its conclusions are positively or negatively correlated with
those in the citing work will be termed the POLARITY of the
citation. Here legal documents provide a good example for what might be
possible in scientific writing. Every law student learns proper legal
syntax for codifying references as they learn to write LEGAL
BRIEFS , the structured memos provided to judges and often
incorporated into the ultimate opinion [<A
HREF="bibrefs.html#BlueBook">BlueBook] . One feature of their
writing is the special syntax used to refer to statuatory law (e.g.,
{\tt 17 U.S.C 101} refers to a section of the U.S. Code) or case law
(e.g., {\tt West Pub. Co. v. Mead Data Cent., Inc., 616 F.Supp 1571}) in
a conventional fashion. A more interesting aspect of legal brief style
is the explicit syntactic marking of the relation of the cited case to
the argument being made in the brief: reference to a supporting legal
precident is marked by {\tt cf.}, while potentially antagonistic
argument is marked with {\tt but cf.}!


Note the importance of the
syntactic localization of both the source and destination of a citation
pointer to the two document's semantic purposes. Localization helps to
anchor the author's purpose in using the citation to the document's
larger argument. Most papers make many points and attempt to establish a
number of propositions. The ability to point to particular conclusions
within the cited paper is therefore important. The citing paper's
argument structure is simultaneously being extended, and so refenence to
prior argument at a particular location in the current argument is can
be more persuasive than if all cites aren't localized.


 within the
opinion, as reported in a particular court reporter publication, makes
it far easier to find the relevant legal issue. Goldberg summarizes the
legal issues:


 Given today's technology, access to court opinions should
be easier than ever. The law of the states and the federal law is in the
public domain and is frequently reported electronically by the
courts.... Small legal publishers are attempting to take advantage of
this access and package less expensive computer assisted legal research
tools than Westlaw and Lexis services; for example, many want to package
state law on CD-ROM. Given the raw material is available these
publishers should have a green light. The problem is that West
Publishing, who publishes many of the State and Regional reporters and
has control over the Federal Reporters, claims that their citation form,
specifically the pagination of the reporters, is protected by copyright.
As a result, those who wish to become legal publishers must either
receive a license from the official reporters to use their pagination
and citation form or petition the court for recognition of their
citation form. Publishers are free to request copies of opinions from
the courts or, for those that are available, download them from
electronic bulletin boards and then print them in whatever form they
choose. These materials are useless, however, to many potential
customers as there is no recognized way to cite to them. In turn, the
fee these publishers can fetch is a fraction of that of those using
West's pagination. In 1991, the Supreme Court held in Feist Publication
v. Rural Telephone Service that in order to deserve a copyright a work
must have a ``creative spark" since a copyright is to reward
originality. This decision has left some to doubt whether West is
deserving of copyright protection for its pagination. In the 1980s, West
sued Mead Data's Lexis over the use of its pagination but the two
settled out of court. Under the terms of the license agreement Lexis
received, Lexis can not relitigate so the issue is not yet settled.
Efforts to get legislation on the issue passed have failed allegedly in
part due to West's strong legislative ties. [<A
HREF="bibrefs.html#Goldberg95">Goldberg95]


 The American Bar
Association has recently issued a <A
HREF="http://www.uscourts.gov/aba/abarep.htm">Special Report on Citation
Issues including page numbering. Like most of electronic publishing,
the underlying issues remains in flux. }


One reason citation has been
less-exploited in FOA applications than it might is due to the expense
of obtaining this data. In the context of the WWW, however, it turns out
that the indices built by Web crawlers can be quite easily extended to
capture cited pages information which can also be easily
inverted to maintain citing pages as well (cf. Section <A
HREF="foa-8-1.html">&sect;8.1 ).




Top of Page

 | UP: Discussion

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1.4 Discussion





FOA Home

 | UP: Citation: inter-document links



Discussion
Subsections

	 6.1.4.1 Citations and arguments




Top of Page

 | UP: Citation: inter-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1.5 Analyzing WWW adjacency





FOA Home

 | UP: Citation: inter-document links



Analyzing WWW adjacency

To a computer scientist, the WWW looks much like the directed graphs
(digraph) we have studied in data structure classes for decades. It's
big, it's dynamic, we have special questions about it, etc., but many of
the same analyses we would apply to any digraph are good starting points
for the WWW. A useful first step is to define the ADJACENCY
MATRIX $A$ connecting all the documents $d_{i}$ of the WWW: A_{ij}
&=&1 A_{ij} &=&1\ \textstyle{if}\ d_{i}\ \textstyle{cites}\ d_{j} \\
&=&0\textstyle{ otherwise} \end{eqnarray}


As an example, the <A
HREF="http://www.google.com/">Google search engine imagines the WWW
graph as the basis of a \defn{Markov process}, where the probability of
jumping from one page is uniform across all of its anchor/citations.


If
$is defined to be this (small) probability, the STATIONARY
DISTRIBUTION of this Markov process provides some insight into how
likely a browsing user is to find themselves at a particular page. NEC's
CiteSeer is another example
of useful citation information can be as part of a tool for searching
computer science literature.


A more extensive analysis [<A
HREF="bibrefs.html#Chakrabarti98a">Chakrabarti98a] [<A
HREF="bibrefs.html#Kleinberg98">Kleinberg98] has analyzed the WWW,
looking especially for methods that extract AUTHORITATIVE pages
from the vast numbers of other pages the WWW also contains:). The first
component of their method corresponds approximately to the notion of
IMPACT discussed in section <A
HREF="foa-6-1-1.html">&sect;6.1.1 . ... the fundamental difficulty
lies in what could be called the Abundance Problem: The number of pages
that could reasonably be returned as ``relevant" is far too large for a
human user to digest. Thus, to provide effective methods for automated
search under these constraints, one does not necessarily need stronger
versions of classical information retrieval notions such as relevance;
rather one needs a method of providing a user, from a large set of
relevant pages, a small collection of the most "authoritative" or
"definitive" ones.... Unfortunately, ``authority" is perhaps an even
more nebulous concept than ``relevance," again highly subject to human
judgment.... We claim that an environment such as the WWW is explicitly
annotated with precisely the type of human judgment that we need in
order to formulate a notion of authority. Specifically, the creation of
a link in the WWW represents a concrete indication of the following type
of judgment: the creator of page $p$, by including a link to page $q$,
has in some measure conferred authority on $q$. [<A
HREF="bibrefs.html#Chakrabarti98a">Chakrabarti98a] Second, they
define HUB documents to be ones which are particularly exhaustive
in their reference to other pages. This is roughly analogous to review
papers also mentioned in Section <A
HREF="foa-6-1-1.html">&sect;6.1.1 . To a first approximation,
authoritative pages are ones with high in degree while hubs are ones
with high out degree. But Kleinberg imposes an additional, important
constraint: a COMMUNITY of hubs and authority pages must be
mutually self-referential. The thinking underlying Kleinberg's method is
provacative: Authoritative pages relevant to the initial query should
not only have large in-degree; since they are all authorities on a
common topic, there should also be considerable overlap in the sets of
pages that point to them. Thus, in addition to highly authoritative
pages, we expect to find what could be called hub pages: these are pages
that have links to multiple relevant authoritative pages. It is these
hub pages that ``pull together" authorities on a common topic, and allow
us to throw out unrelated pages of large in-degree. Hubs and authorities
exhibit what could be called a mutually reinforcing relationship: a good
hub is a page that points to many good authorities; a good authority is
a page that is pointed to by many good hubs. (p.4)


 Two quantities, $$
and $\mathbf{x}$, are associated with each document, corresponding to
how good an authority or hub, resp., the document is, based on the
adjacency matrix $A$. x_{i} &\equiv &\mathname{Authority}(d_{i}) \\
y_{i} &\equiv &\mathname{Hub-ness}(d_{i}) \mathbf{x} &\equiv &\;
\\ \mathbf{y} &\equiv &\;


 $$ and $\mathbf{y}$ values are
iteritively updated by pre-multiplication with the adjacency matrix or
its transpose: \mathbf{x}^{t+1} &=&A^{T} \; \mathbf{y}^{t} \\
\mathbf{y}^{t+1} &=&A \; \mathbf{x}^{t} It is also important to
re-normalize these vectors to unit length after each update.


Under
reasonable assumptions, this update procedure is guaranteed to converge
on values with $^{*}$ being the principle eigen vector of $A^{T}A$ and
$\mathbf{y}^{*}$ the principle eigen vector of $AA^{T}$. \mathbf{x}^{*}
&=&\omega _{1}(A^{T}A) \\ \mathbf{y}^{*} &=&\omega _{1}(AA^{T})


 Using
this notation, similarity of documents $d_{i}$ and $d_{j}$ can be
conveniently measured in terms of co-citation as the $$ entry of
$AA^{T}$. [Kessler63] }.


While we
can conceive of applying these techniques to the graph
corresponding to the entire WWW, the computational time and space
required still makes such an analysis intractable. Kleinberg et al.
typically recommend applying this adjacency analysis to a subset of
pages pulled together by a query against some search engine. In their
experiments they augment this initial hit list with documents either
pointing to, or pointed to from, documents in the hit list itself, as
shown in Figure (figure)


 Note that the values for authority and
hub on which this analysis converges corresponds to the first, primary
eigen vector. Using these values to identify high hub and anchor nodes,
gives rise to the first, primary community of documents in the graph.
Another interesting application of adjacency analysis is to consider
communities other than the one corresponding to the first, largest eigen
value. A particularly striking application of this analysis concerns
bi-modal queries: consider results arising from a query
ABORTION, shown in Figure (FOAref) . After first
identifying a community of pages extensively citing both pro-choice and
pro-life documents, the second eigen vector $\omega _{2}$ clearly
separates pages associated with pro-choice organizations (with
relatively high positive values and pro-life with negative values!



Another important feature of this analysis is that it depends on only
first order adjacency information. That is, while it is always easy to
find all of the documents pointed to by a target document simply by
inspecting the document for its anchors, the in-neighborhood of a
document can be identified through direct inspection of other documents.
This means, for example, that search engine crawlers which look at every
single documents can, as part of their normal search, simultaneously
collect this adjacency data.




Top of Page

 | UP: Citation: inter-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.1 Citation: inter-document links





FOA Home

 | UP: Inference beyond the \Index



Citation: inter-document links

The bibliography at the end of a scientific publication, links from one
World Wide Web page to another, references in a legal brief to prior
judicial opinions, and CONVERSATIONAL THREADS connecting postings
to one another within a common Usenet group may seem completely
unrelated. In each case, however, the author of one document has found
it useful to cite another document. Perhaps it is because the author
wishes to extend a prior scientific or legal argument, or perhaps it is
to attack it. It may be to pull together disjointed Web pages into a
single ``home page'' theme. Or the citation may be designed to quiet a
bunch of ``newby'' Newsgroup discussion by alerting them to a FAQ
(frequently asked question) answer. In all cases, a new piece of text is
being woven into the larger fabric of other texts, uniting one author's
contribution into the legacy of many others'. The value citations can
offer in supporting the FOA activity has been recognized by many [<A
HREF="bibrefs.html#REF1072">REF1072] [<A
HREF="bibrefs.html#Salton79">Salton79] , and lead to methods that
allow users to capture and organize their own bibliographic materials
[REF407] . As more and more scientific
publishing moves to open, electronic repositories, efforts such as the
Open Citation
Project are leading the way towards new standards for the exchange
of this important information.


At its core a citation is a pointer, from
a document to a document. (figure) A. We typically think of one
a citation pointing from one document to another document of the same
type: Scientific papers cite other journal articles, Email messages
refer to prior messages, HTML pages point to one another. But in today's
quickly changing scene, it is not uncommon to find heterogeneous forms
of citation, from one document type to another as shown in Figure
(FOAref) B. For many publications, citations are collected at
the very end of a document, in its bibliography. Often the real locus of
a citation, however, is a someplace earlier in the paper, and many
compositional styles insert an expicit bibliographic citation there. We
will be interested in the CITATION RESOLUTION of both ends of the
pointer: how accurately do we know the location of the citation in the
citing paper, and how precisely is its pointer into the cited paper?
Does it point to a particular paragraph, page, section, or just the
entire document.


The application of very similar citation mechanism has
been exploited in much different ways in different contexts. Figure
(figure) summarizes a number of dimensions across several
contexts. Here we consider citation as exemplified in two particular
classes of documents, generated by Science and by Law, which have
supported social activities for a very long time. Section <A
HREF="foa-6-1-5.html">&sect;6.1.5 then reports on new analyses of
citation patterns observed on the WWW.

Subsections

	 6.1.1 Bibliometric analysis of science
	 6.1.2 Time-scale
	 6.1.3 Legal citation
	 6.1.4 Discussion
	 6.1.5 Analyzing WWW adjacency




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.2.1 Footnotes, hyper-footnotes and \emph{cf.





FOA Home

 | UP: Hypertext, intra-document links



Footnotes, hyper-footnotes and \emph{cf.

Footnote text embellishes a primary text. They provide a more detailed
treatment of terms or concepts used in the priamry text. A lexical
token, typically smaller in size, creates a correspondence between the
primary text and the annotation on it.


Perhaps the most direct claim of
hypertext authors is that the standard linear presentation of text does
violence to the more networked way in which we naturally conceive of the
concepts being discussed. At the same time, the conventional sequential
flow through text mandated by printed media does a great deal to support
RHETORICAL ARGUMMENT . For this reason, this FOA text was written
first as a traditional book, assumeing a basically linear flow. On this
linear spine, two hypertext extensions have been added (cf. Section <A
HREF="foa-8-2-1.html">&sect;8.2.1 ).


For example, the primary purpose
of last paragraph was to move (the primary, linear course of the
textbook) from a discussion of the syntactic conventions of footnoting
to a consideration of communication's ultimate purposes. Since these
issues are covered in more depth in Section <A
HREF="foa-8-2-1.html">&sect;8.2.1 , a parenthetical cf.
reference to this other section is made. The cf. relation is
bested viewed as an opportunity (offered by the author to a reader) to
compare the two passages. } {cf. for conferre, Latin
for TO COMPARE, BRING TOGETHER, CONTRIBUTE, CONSULT. [<A
HREF="bibrefs.html#MW3">MW3] }


In this FOA text I have also chosen to
distinguish between standard footnotes, and HYPER-FOOTNOTES used
to capture more extended digressions. The last paragraph allows readers
with access to the CDROM version of FOA who arem interested in the Latin
etymology of the cf. token to poke into that. Hyper-footnotes
include a caption, providing more indication (than a simple number) as
to what additional information lies on the other end of the hyper-jump.




Top of Page

 | UP: Hypertext, intra-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.2.2 Hierarchic containment





FOA Home

 | UP: Hypertext, intra-document links



Hierarchic containment

One of the most common features of all document types is that as larger
and larger portions are aggregated, explicit hierarchic structure is
used to organize the text. For example, this textbook is broken into
chapters, which are broken into sections and subsections, etc. This is
basically a CONTAINMENT relationship, with shorter passages
aggregated to form larger ones, but with short textual RUBRICS
providing useful summaries of the themes of the smaller units. Typical
atomic units of text range from a few sentences [<A
HREF="bibrefs.html#Salton93">Salton93] [<A
HREF="bibrefs.html#Salton94">Salton94] but typically are paragraphs
[Hearst93] .


A provocative picture of
how the containment relation can be exploited is shown from Salton et
al's analysis of encyclopedia text in Figure (figure) . This
shows the encyclopedia text ordered sequentially around a ring. In the
top figure, individual pages of the encyclopedia are treated as separate
documents, while in the bottom figure larger sections are aggregated. In
both cases, links between documents are created when their similarity
(as measured according to their vector space representations; cf Section
&sect;3.4 ) exceeds a threshold. As Chapter 3
explored in detail, a vector space representation of document content is
very sensitive to length. Salton investigates the use of inclusion to
aggregate text into larger units (in the bottom of Figure
(FOAref) ) and various ``global'' vs. ``local'' weighting
schemes to manipulate the effects of length normalization. Note also how
co-reference to similar topical themes can be seen at different scales
within the containment hierarchy.


When considering constituent
PASSAGES of the same document, it becomes possible to ask how
much the TOPIC of the prose changes as it goes from one passage
to the next. Hearst analyzes how <A
HREF="http://www.sims.berkeley.edu/~hearst/tb-example.html">similarity
(using tf-idf weighting and cosine similarity) varies across
passages. [Hearst93] The result
is the wave of Figure (figure) . Also shown on this figure are
vertical bars where a human judge has determined that a topical shift
has occurred.


Having isolated individual passages as part of retrieval,
it becomes important to show the user this additional level of analysis
as part of their ``documents'' retrieval. Figure (figure) .
shows TOPICAL TILING , another visualization technique Hearst has
developed to highlight shifts in topical focus from one passage to the
next. By increasing the resolution of document analysis, users can see
just which passages match particular keywords or combinations of
keywords in their query.


Containment relations among documents may also
be useful in supporting queries of widely varying generality (cf.
Section &sect;4.3.4 ). If a user issues a
very broad query, it may mean they seek documents with an equally broad,
overview LEVEL OF TREATMENT . Then one reasonable hypothesis is
that broader queries should correspond to the retrieval of large
sections while narrower queries to smaller sections, but only if the
assumption that documents in general obey something like a uniform level
of treatment. Such an assumption would not be unreasonable for the
encyclopedia text considered by Salton et al., since we expect (and
encyclopedia editors attempt to ensure) that there is some document
about every topic.


Figure (figure) contrasts the
encyclopedia's top-down, coherent TOPICAL TILING with another
(hypothetical) document distribution, generated by bottom-up, organic
case law. People don't go to court unless they disagree, and so what
judges must write about are contentious legal issues defining the
borders of legal disputes. As courts work out a legal issue
(for example, the intellectual property status of software, or whether
genes can be patented) we can expect many opinions to cover very similar
topical ground.




Top of Page

 | UP: Hypertext, intra-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.2.3 Argument relations





FOA Home

 | UP: Hypertext, intra-document links



Argument relations

A central purpose of many documents is to persuade. Especially
interesting, then, are relations among documents that reveal the
arguments relating among the documents [<A
HREF="bibrefs.html#Sitter92">Sitter92] . The law provides several
interesting examples, as shown in Figure (figure) (taken from ,
from Rose Fig. 7.4). Rose enumerates four relations often found in legal
documents, from simple reference to more elaborate logical relations
such as exception.


While less explicit than within legal documents,
educational materials also have implicit or explicit PREREQUISITE
relationships assumed by a text's author. Figure (figure) shows
dependencies amongh this book's chapters. In interdisciplinary papers,
it is common to have several introductory sections, providing background
for experts in one field who may not have requisite background in
another. When taken across many texts, such a prerequisite structure
imposes a lattice, which can be exploited to help a reader/browsing user
find just those components of the document that are most important to
them.




Top of Page

 | UP: Hypertext, intra-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.2.4 Intra- vs.  inter-document relations





FOA Home

 | UP: Hypertext, intra-document links



Intra- vs.  inter-document relations

It is interesting to see how similar many conventions for relating
textual passages within a single document resemble those used
to traditionally refer across documents. In part this reflect
circumstantial features of a document's production --- whether it was
printed on newsprint, archival paper, in a journal or a book, or a WWW
page --- will disappear as a common technology underlies them all. And
at the same time integrating new technologies into the publication
process makes it clear how certain features will remain true onlyt
within a single document.


This can be described in terms of a
``membrane'' that functionally defines what we will continue to consider
the document. The defining membrane in this case has to do with a notion
of AUTHORSHIP . The words expressed are the creation/opinion of a
single author or multiple authors, each who have signed their names. For
example, when one paragraph of a legal document refers to another
passage within the same document: Except as provided in the case of
certain unauthorized signatures (\S8-205), lack of genuineness of a
certificated security or an initial transaction statement is a complete
defense ... we can be assured that the document's authors were thinking
about both passages and commit to a particular relation between them.
Inter-documentment citations, on the other hand, reflect a new opinion
about some pre-existing document.




Top of Page

 | UP: Hypertext, intra-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.2.5 Beyond unary $\mathname{About





FOA Home

 | UP: Hypertext, intra-document links



Beyond unary $\mathname{About

Our discussion of FOA has generally assumed that we are attempting to
find the topic or topics that a document is about. In the language of
logic, we can talk about this as applying a unary predicate $(k)$. That
is, the determination that a document is about keyword $k$ depends on
only one argument, a single keyword. But no piece of writing deals with
only a single topic, at least for more than a moment. Good writing
connects topics. It discusses a relation among those topics.


Within
the paragraph-size textual passages that have been our focus, this
relational aspect of language has not been too troubling. But it should
make us especially interested in the transitions in topical focus as we
move from one paragraph to the next within the same document. Passage-
level analysis like this was mentioned above (cf. Section <A
HREF="foa-6-2-2.html">&sect;6.2.2 ).


As we move to larger documents,
an additional source of information is citations from one to another.
Assuming that we have characterized the topical area of each document
independently, an analysis of the citations, placed by one author
relating another document to the authors topic, are explicit
indications of the relationships between these topics.


All three levels
of analysis, within paragraph relations among topics, paragraph to
paragraph topical progression, and interdocument relations reflected by
citations, are some of the clues we can exploit when suggesting
potential browsing directions for users trying to FOA.




Top of Page

 | UP: Hypertext, intra-document links

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.2 Hypertext, intra-document links





FOA Home

 | UP: Inference beyond the \Index



Hypertext, intra-document links

Since the very first papyrus scrolls were first used to capture written
language, it has become most natural to conceive of text as a single,
linear and continuous thread, authored and then read as a single stream.
But as books became longer Tables of Contents were prepended, Indices
were appended and the opportunities for traversing the text in
fundamentallly nonlinear ways became more and more common. As
we become interested in other kinds of documents, many bring their own
special structures and writing conventions, for example the abstract
paragraph, introductions and conclusions of longer papers, the
``Methods'' sections in scientific papers. In news reporting SPIRAL
EXPOSITION is often used: a news item is summarized in the first
paragraph, then treated in more detail in the paragraphs that fit on
Page 1 or ``above the fold'' of the newspaper, and in more detail still
in the body of the article.


The attempt to analyze, support, and create
such nonlinear, HYPERTEXT relations among documents began long
before the WWW made hypertext links commonplace; <A
HREF="http://www.theatlantic.com/unbound/flashbks/computer/bushf.htm">Vanavar
Bush's ``As We May Think'' article (published in 1945!) [<A
HREF="bibrefs.html#REF701">REF701] and Ted Nelson's revolutionary <A
HREF="http://jefferson.village.virginia.edu/elab/hfl0155.html">
Xanadu Xanadu project [Nelson87]
are often mentioned as seminal works. Mice and graphical interfaces made
clicking on one textual passage, so as to jump to another, second
nature. Hypertext conferences focused on these new issues began in the
mid-1980s, and taken on new energy as the HTTP protocols and HTML
authoring languages made it easy to support many kinds of intra- and
inter-document relations [REF700] [<A
HREF="bibrefs.html#REF728">REF728] [<A
HREF="bibrefs.html#Agosti92a">Agosti92a] [<A
HREF="bibrefs.html#Agosti92b">Agosti92b] [<A
HREF="bibrefs.html#Bruza92">Bruza92] [<A
HREF="bibrefs.html#Egan91">Egan91] . In the process, many types of
linkage between documents have been proposed. The following sections
mention some of the most common and useful, sometimes using this FOA
text (self-referentially!) as examples.

Subsections

	 6.2.1 Footnotes, hyper-footnotes and \emph{cf.
	 6.2.2 Hierarchic containment
	 6.2.3 Argument relations
	 6.2.4 Intra- vs.  inter-document relations
	 6.2.5 Beyond unary $\mathname{About




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.3.1 Automatic thesaurus construction





FOA Home

 | UP: Keyword structures



Automatic thesaurus construction

Before going on to consider WordNet,another elaborate thesaurus, it is
useful to relate these , manually-constructed representations with
automatic, statistically-derived analogs. Such comparisons have been
part of IR research since its beginnings [<A
HREF="bibrefs.html#Joyce58">Joyce58] [<A
HREF="bibrefs.html#Dennis64">Dennis64] [<A
HREF="bibrefs.html#Soergel74">Soergel74] . Section <A
HREF="foa-5-2-5.html">&sect;5.2.5 has discussed how the same
information used to cluster documents can be used with keywords as well.
The semantics of relations based strictly on co-occurrence frequencies
are not obvious [vanR77] , but seem to
provide evidence for the \textbf{RT} (related term, or synonymy
relation) discussed above.


Using this information to construct hierarchic
relations among keywords corresponds to (hierarchic) clustering
techniques. Thesaurus-specific techniques generally exploit a heuristic
that high frequency keywords correspond to broad, general terms while
low frequency keywords correspond to narrow, specific ones [<A
HREF="bibrefs.html#Srinivasan92">Srinivasan92] . This heuristic can
be used to organize keywords into levels of a taxonomy, with the
hierarchic parent/child relation formed between those keywords with
similar document distributions. RelFbk can also be used to provide
theaurus structure [Guntzer89] .
Whether constructed manually or automatically, thesuarus structures
support many new forms of naviagation [<A
HREF="bibrefs.html#REF1077">REF1077] [<A
HREF="bibrefs.html#REF1079">REF1079] .




Top of Page

 | UP: Keyword structures

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.3.2 Corpus-based linguistics and WordNet





FOA Home

 | UP: Keyword structures



Corpus-based linguistics and WordNet

Linguistics has traditionally focused on the phenomena of
spoken language, and since Chomsky [<A
HREF="bibrefs.html#REF716">REF716] [<A
HREF="bibrefs.html#REF725">REF725] further focused on
syntactic rules describing the generation and understanding of
individual sentences. But as more large samples of written text have
become increasingly available, CORPUS-BASED LINGUISTICS has
become an increasingly active area of research . . D. D. Lewis and E. D.
Liddy have collected a useful <A
HREF="http://ftp://ciir-ftp.cs.umass.edu/pub/papers/lewis/nlirbib93.ps.Z">bibliography
and resource list on NLP for IR , and R. Futrelle and X. Zhang have
collected Large-Scale Persistent Object Systems For <A
HREF="http://atg1.wustl.edu/DL94/paper/futrelle.html">Corpus Linguistics
and Information Retrieval . Stuart Shieber maintains the <A
HREF="http://xxx.lanl.gov/cmp-lg/">The Computation and Language
Archive as part of the LANL reprint server.


The sophistication of
computational linguistics syntactic analysis provides a striking
contrast to IR's typical BAG-OF-WORDS approach which aggressively
ignores any ordering effects. Conversely, IR's central concerns with
semantic issues of meaning and the ultimate pragmatics of using
language to find relevant documents goes beyond the myopic concern with
isolated sentences that is typical of linguistics. The range of
potential interactions between these perspectives is only beginning to
be explored, but includes the introduction of parsing techniques with IR
retrieval systems [Smeaton92] [<A
HREF="bibrefs.html#Strzalkowski94">Strzalkowski94] , as well as
using statistical methods to identify PHRASES that are a first
step from a simple bag-of-words to sytactically well-formed sentences
[LEWIS92b] [<A
HREF="bibrefs.html#Krovetz93">Krovetz93] [<A
HREF="bibrefs.html#Church89">Church89] [<A
HREF="bibrefs.html#REF1112">REF1112] [<A
HREF="bibrefs.html#REF866">REF866] . Another important direction of
interaction is the use of IR methods across multi-lingual corpora, for
example arising from the integration of the European Community [<A
HREF="bibrefs.html#Hull96a">Hull96a] [<A
HREF="bibrefs.html#Sheridan96">Sheridan96] .


From a syntactic
perspective, the only way to get issues of real meaning into
language is via the LEXICON : a dictionary of all words and their
meanings. Our present concern, inter-keyword structures, them becomes an
issue of LEXICAL SEMANTICS [<A
HREF="bibrefs.html#REF641">REF641] , and it is no surprise then that
linguists have also developed representational systems for inter-word
relationships. An influential and widely used example of a keyword
thesaurus is the <A
HREF="http://www.cogsci.princeton.edu/~wn/">WordNet {developed by
George Miller and colleagues} [<A
HREF="bibrefs.html#Fellbaum98">Fellbaum98] . {This is the same
George Miller whose analysis of Zipf's Law was mentioned in Section <A
HREF="foa-3-2.html">&sect;3.2 . He is perhaps most famous for his
``human information processing'' analyses of cognition, such as the
limit of $7\pm2$ on the number of ``chunks'' that can be retained in
short-term memory [Miller56] . The
same information theoretic motivation underlies all these wide-ranging
efforts.}


One obvious distinction of WordNet is simply the size of its
vocabulary: it contains almost 100,000 distinct word forms, divided into
lexical categories as shown in Figure (FOAref) . Central to the
lexical approach to semantics is distinguishing between lexical items
and the ``concepts'' they are meant to invoke. ``Word form'' will be
used here to refer to the physical utterance or inscription and ``word
meaning'' to refer to the lexicalized concept that a form can be used to
express. Then the starting point for lexical semantics can be said to be
the mapping between forms and meanings. [<A
HREF="bibrefs.html#Fellbaum98">Fellbaum98]


 The relations connecting
words in WordNet ar similar to those used within thesauri, but not
identical. The first and most important relation is SYNONYMY .
This has a special role in WordNet, pulling multiple word forms together
into a SYNONYM SET which, by definition, all have the same
meaning. According to one definition (usually attributed to Leibniz) two
expressions are synonymous if the substitution of one for the other
never changes the truth value of a sentence in which the substitution is
made\ldots. A weakened version of this definition would make synonymy
relative to a context: two expressions are synonymous in a linguistic
context C if the substitution of one for the other in C does not alter
the truth value. [Fellbaum98]


 The
BT/NT relation in standard thesauri is refined in WordNet into two types
of relations, HYPERNYMY and MERONYMY . The former relation
plays a dominant role, allowing INHERITANCE of various properties
of parent words by their children. Much attention has been devoted to
hyponymy/hypernymy (variously called subordination/superordination,
subset/superset, or the ISA relation)\ldots. A hyponym inherits all the
features of the more generic concept and adds at least one feature that
distinguishes it from its superordinate and from any other hyponyms of
that superordinate. This convention provides the central organizing
principle for the nouns in WordNet. [<A
HREF="bibrefs.html#Fellbaum98">Fellbaum98] This hypernymy relation
connects virtually all the words into a forest of trees rooted on a very
restricted set of ``unique beginners.'' In the case of nouns, the
top-level categories are those shown in Figure (FOAref) , and
for verbs in Figure (FOAref)


 The final category of STATIVE
VERBS is used to capture the distinction between the majority of
ACTIVE VERBS and those (e.g., SUFFICE, BELONG,
RESEMBLE) reflecting state characteristics.


WordNet also
represents roughly the opposite of the synonym relation with the
ANTONYMY relation. Defining this logically proves more difficult,
and Miller is forced to simply equate it with human subjects' typical
responses: Antonymy is a lexical relation between word forms, not a
semantic relation between word meanings\ldots. The strongest
psycholinguistic indication that two words are antonyms is that each is
given on a word association test as the most common response to the
other. For example, if people are asked for the first word they think of
(other than the probe word itself) when they hear VICTORY,
most will respond DEFEAT; when they hear
DEFEAT most will respond VICTORY. [<A
HREF="bibrefs.html#Fellbaum98">Fellbaum98]


 The use of the antonymy
relation in WordNet is particularly interesting when applied to
adjectives. The semantic organization of descriptive adjectives is
entirely different from that of nouns. Nothing like the hyponymic
relation that generates nominal hierarchies is available for
adjectives\ldots. The semantic organization of adjectives is more
naturally thought of as an abstract hyperspace of N dimensions rather
than as a hierarchical tree. [<A
HREF="bibrefs.html#Fellbaum98">Fellbaum98] First, WordNet
distinguishes the bulk of adjectives, which are called DESCRIPTIVE
ADJECTIVES (such as BIG, INTERESTING, POSSIBLE) from
RELATIONAL ADJECTIVES (PRESIDENTIAL, NUCLEAR) and
REFERENCE-MODIFYING ADJECTIVES (FORMER, ALLEGED).
They then find: All descriptive adjectives have antonyms; those lacking
direct antonyms have indirect antonyms, i.e., are synonyms of adjectives
that have direct antonyms. (p. 28) An example of the resulting
dumbbell-shaped ``bi-polar'' organization is shown in Figure
(figure) .


Voorhees has been one of the first to explore how
WordNet data can be harnassed as part of a search engine [<A
HREF="bibrefs.html#Voorhees93">Voorhees93] .




Top of Page

 | UP: Keyword structures

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.3.3 Taxonomies





FOA Home

 | UP: Keyword structures



Taxonomies

As their central role in WordNet suggests, the hierarchic BT/NT or
hypernymy relations are especially important. Because of the constant
analytic pressure of the Western intellectual tradition, topics continue
to be refined into smaller topics, which the next generation of scholars
immediately refine further. This has led to a wide range of
classification taxonomies associated with various social groups of
scholars and scientists.


Figure (figure) shows several different
sources from which indexing information might be obtained. The most
important single source of subject indexing is the Library of Congress
(LoC). This indexing system is the basis of most large libraries because
its indexing scheme covers all disciplines. For exactly the same reason,
however, the indexers at LoC have far too much to do, and the resulting
indices are admittedly crude. Partly as a response to the lack of
adequate indexing structures, various professional groups have developed
their own taxonomies to help organize the information within their
particular technical specialty. For example, the Association of
Computing Machinery has developed an <A
HREF="http://ls6-www.informatik.uni-dortmund.de/CRclass.html">ACM
Computing Reviews Classification for use by its {\em Computing
Reviews} publication [REF396] . This
taxonomy is much more specific, and therefore more widely used by
computer specialists. It lacks, however, many of the advantages of the
LoC system. In general, the keywords are assigned by the authors rather
than by trained librarians. The system is rarely if ever integrated into
the operations of libraries. And while the indexing structure is much
more refined than that of the LoC, it is still too crude for most
research currently going on in any one sub-specialty. This has caused
some practitioners in various sub-specialties to develop their own
extensions. For example, David Waltz was commissioned by Scientific
Data-Link to extend the ACM's {\em Computing Reviews} taxonomy for the
sub-specialty of artificial intelligence (AI) [<A
HREF="bibrefs.html#REF331">REF331] . Waltz's extension is extremely
refined and helpful to AI practitioners. At the same time, it is even
more {\em ad hoc}, its ``sponsoring institution'' has less impact, and
consequently it is even less well accepted within libraries.


All three of
these indexing systems are examples of TOP-DOWN KNOWLEDGE
STRUCTURES. That is, they are developed by various social
institutions as prescriptive languages used to represent the consensus
opinion as to how information is to be organized. Such ``consensual''
knowledge structures are critical if individuals are to {\em share}
information. Each indexing system represents a compromise between
increased scope and diminished resolution. Increased scope brings along
with it broader acceptance and adherence. These advantages are bought at
the expense of acceptance by users actively involved in technical
specialties.


The central role of hierarchic BT/NT relations in organizing
keyword vocabularies should make us especially concerned with a precise
semantics for this relationship. Most would agree that if {A} is a
broader-term than {\tt B}, then {\tt B} ``is a'' {\tt A}. But as
knowledge engineers within AI have known for a long time, the ubiquitous
{\tt IS\_A} relation admits a number of interpretations which can
support much different types of inference [<A
HREF="bibrefs.html#REF55">REF55] [<A
HREF="bibrefs.html#REF97">REF97] . In general, the BT/NT relation
seems to correspond most closely to the ``a kind of'' implication
relating predicates {\tt A} and {\tt B}: (\forall x) B(x) \rightarrow
A(x)


 Earlier generations of Internet users participated in the
construciton of the extensive <A
HREF="http://ftp://rtfm.mit.edu/pub/usenet-by-hierarchy/">UseNet
hierarchy of discussion boards, on topics from {\tt
ALT.SEX.FETISH.ROBOTS} to {\tt COMP.SYS.MAC.OOP.TCL}; see Section <A
HREF="foa-7-4-5.html">&sect;7.4.5 for an example of the use of this
hierarchy in text classification tasks. These days the most widely know
taxonomies are WWW DIRECTORIES such as <A
HREF="http://www.yahoo.com">Yahoo! , where employees of this company
have constructed a hierarchy, primarily of places to spend money. One of
the most exciting recent developments is the development of
\defn{collaborative classification} efforts such as the <A
HREF="http://dmoz.org/about.html">Open Directory Project (DMOZ)
which involve large communities of experts, each working on making sense
within their own are of expertise.




Top of Page

 | UP: Keyword structures

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.3 Keyword structures





FOA Home

 | UP: Inference beyond the \Index



Keyword structures

Most of what we have said about the Index relation (e.g., as
part of the vector space model) assumes that keywords are simply a set
of features. But beyond simply providing access to the retrieval of
documents, the fact that keywords are \rikmeaning-ful objects in their
own right means that we can analyze relationships among keywords
directly.


THESAURI are structured representations of relations
among keywords. Common relations represented in thesauri include:


 \item
\textbf{Broader term/narrower term (BT/NT)}; these capture hierarchic
relations, generally between a kind of semantics, sometimes part whole
relation. \item \textbf{Related term (RT)} capturing synonym or
quasi-synonym relationships. \item \textbf{Use for (UF)} capturing a
preferred, conventional or authoritative term over possible
alternatives.


 One of the most extensive examples of such a
representation is the MESH (MEdical Subject Headings) thesaurus, part of
the National Library of Medicine's extensive PubMed system. FIgure
(figure) shows the term {\tt LYMPHOMA} within the MESH
thesaurus. expanded in the second two tree locations.} example
Hierarchic BT/NT relations are shown as indentation. Because this
thesaurus allows a single keyword to fit in multiple places in the
hierarchy (e.g., treating {\tt LYMPHOMA} as a kind of {\tt NEOPLASM} as
well as a kind of {\tt IMMUNOLOGIC DISEASE} ) this browser shows the
term as part of three separate paths; note that the children (narrower
terms) of {\tt LYMPHOMA} are repeated at each location.

Subsections

	 6.3.1 Automatic thesaurus construction
	 6.3.2 Corpus-based linguistics and WordNet
	 6.3.3 Taxonomies




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.4.1 A.I. Geneology





FOA Home

 | UP: Social relations among authors



A.I. Geneology

The AI Geneology project (AIG) is an attempt to collect information
relating authors in artificial intelligence to one another through
shared advisors. In analogy to geneological family trees, we can treat
the advisor as parent to advisee. Students of students become
grandchildren, etc. A subset of this data is shown in Figure
(FOAref) . As an example, of how this additional information
about authors can be exploited to understand more about the words used
in documents, Steier has analyzed word-pair co-occurrance data as a
means of finding potential index phrases [<A
HREF="bibrefs.html#REF1097">REF1097] . Take, for example, the phrase
{\tt CASE-BASED REASONING}. This phrase has a high degree of phrasal
information content (using mutual information statistics to identify
statistically-dependent word pairs) when these statistics are collcted
across the entire AIT document corpus.


 \small McCulloch, W. S. ----
Minsky, Marvin ---- ---- Winston, Patrick H. ---- 1973 Waltz, David L.
---- 1980 Finin, Timothy W. THE SEMANTIC INTERPRETATION... 1988 Kass,
Robert John ACQUIRING A MODEL OF THE... 1989 Klein, David A. SEE CO-ADV
SHORTLIFFE, TED 1987 Pollack, Jordan B. ON CONNECTIONST MODELS OF...
1993 Angeline, Peter EVOLUTIONARY ALGORITHMS AND... 1994 Kolen, John
COMPUTATION IN RECURRENT...


Mey, Jacob 1969 Schank, Roger C. A CONCEPTUAL
DEPENDENCY... ---- Kolodner, Janet L. ---- 1989 Shinn, Hong Shik A
UNIFIED APPROACH TO... 1989 Turner, Roy Marvin A SCHEMA-BASED MODEL
OF... 1991 Hinrichs, Thomas Ryland PROBLEM-SOLVING IN OPEN... 1992
Redmond, Michael Albert LEARNING BY OBSERVING AND... ---- Dejong, Gerald
Francis ---- 1987 Segre, Alberto Maria EXPLANATION-BASED LEARNING...
1988 Shavlik, Jude William GENERALIZING THE STRUCTURE... 1991 Towell,
Geoffrey Gilmer SYMBOLIC KNOWLEDGE AND... 1988 Mooney, Raymond Joseph A
GENERAL EXPLANATION-BASED... 1992 Ng, Hwee Tou A GENERAL ABDUCTIVE
SYSTEM... 1989 Rajamoney, Shankar Anandsubramaniam EXPLANATION-BASED
THEORY... ---- Lehnert, Wendy G. ---- 1983 Dyer, Michael G. IN-DEPTH
UNDERSTANDING: A... 1987 Mueller, Eric DAYDREAMING AND COMPUTATION:...
1987 Zernik, Uri STRATEGIES IN LANGUAGE... 1988 Pazzani, Michael John
LEARNING CAUSAL... 1988 Gasser, Michael SEE CO-ADV HATCH, EVELYN 1989
Dolan, Charles Patrick TENSOR MANIPULATION... 1989 Alvarado, Sergio Jose
UNDERSTANDING EDITORIAL... 1989 Dolan, Charles THE USE AND ACQUISITION
OF... 1991 Lee, Geunbae DISTRIBUTED SEMANTIC... 1991 Reeves, John
Fairbanks COMPUTATIONAL MORALITY: A... 1991 Nenov, Valeriy Iliev
PERCEPTUALLY GROUNDED... 1991 Quilici, Alexander Eric THE CORRECTION
MACHINE: A... 1993 Turner, Scott R. MINSTREL: A COMPUTER MODEL... 1990
Williams, Robert Stuart LEARNING PLAN SCHEMAS FROM... 1976 Meehan, James
R. THE METANOVEL: WRITING... 1978 Wilensky, Robert UNDERSTANDING
GOAL-BASED... 1985 Jacobs, Paul A KNOWLEDGE-BASED APPROACH... 1986
Norvig, Peter A UNIFIED THEORY OF... 1986 Arens, Yigal CLUSTER: AN
APPORACH TO... 1987 Chin, David Ngi INTELLIGENT AGENTS AS A... 1992 Wu,
Dekai AUTOMATIC INFERENCE: A... 1978 Carbonell, Jaime G. SUBJECTIVE
UNDERSTANDING:... 1988 Minton, Steven LEARNING EFFECTIVE SEARCH... 1989
Lehman, Jill Fain ADAPTIVE PARSING:... 1991 Perlin, Mark W. AUTOMATING
THE CONSTRUCTION... 1991 Hauptmann, Alexander Georg MEANING FROM
STRUCTURE IN... 1992 Veloso, Manuela M. LEARNING BY ANALOGICAL... 1980
Lebowitz, Michael GENERALIZATION AND MEMORY IN... 1987 Hovy, Eduard
Hendrik GENERATING NATURAL LANUGAGE... 1988 Hunter, Lawrence E. GAINING
EXPERTISE THROUGH... 1989 Ram, Ashwin QUESTION-DRIVEN... 1989 Dehn,
Natalie Jane COMPUTER STORY-WRITING: THE... 1990 Leake, David Browder
EVALUATING EXPLANATIONS 1992 Domeshek, Eric Andrew DO THE RIGHT THING:
A... 1993 Edelson, Daniel Choy LEARNING FROM STORIES:... \caption{A
sample of the AIGenealogical record}


 But a statistically-significant
different distribution is observed within dissertations coming from a
particular set of universities: Yale, Georgia Tech. and the University
of Massachusetts. Within these particular university contexts, the
constituent concepts of { CASE-BASED} and {\tt REASONING} are examined
in detail and independently. Not only do these words occur together as
{\tt CASE-BASED REASONING}, but they often occur separately (e.g. {\tt
CASE-BASED PLANNING, CASE-BASED ARGUMENT, CASE-BASED PROBLEM,
SCHEMA-BASED REASONING, ANALOGICAL REASONING, COMMONSENSE REASONING}).
Within the limited context of these sub-corpora, the keywords occur more
independently and hence are considered less phrase-like. As this phrase
is ``exported'' into the general AI vocabulary, the semantic nuances are
left behind, and the dominant use of the constituent words is as part of
the phrase.


But what is it that makes language use so much different at
Yale, Georgia Tech. and the University of Massachusetts? Perhaps it is
merely coincidence, but our hypothesis is that it is the common lineage
traced to Roger Schank! Work across these geographically distant
research institutions is pulled together by an intellectual tradition
captured by the AIG data.


Part of what is interesting about the AIT
corpus is its demonstration within a single corpus of many of the
representations discussed in this chapter. The AI genealogy captures
some of the intellectual linkage relations arising from the Ph.D.
advisor/advisee relationship. David Waltz' taxonomy of AI provides an
excellent initial thesaurus over keywords [<A
HREF="bibrefs.html#REF331">REF331] .




Top of Page

 | UP: Social relations among authors

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.4.2 An emprical foundation for a philosophy of Science





FOA Home

 | UP: Social relations among authors



An emprical foundation for a philosophy of Science

One advantage of studying a focused corpus like the A.I.T. is that we
have an especially good chance of understanding some of these social
relations. A history of A.I. often begins with a seminal conference that
took place at Dartmouth in 1956 [<A
HREF="bibrefs.html#McCorduck85">McCorduck85] [<A
HREF="bibrefs.html#Russell95">Russell95] . If we treat the attendees
at that meeting as ``founding fathers'' (in a population genetic
sense!), we can attempt to track their ``genetic'' impact on the current
field of A.I.


In an attempt to capture other significant intellectual
influences beyond the advisor, the AIG questionnaire asks for committee
members other than the chairman. The role of committee members varies a
great deal from department to department, and campus to campus. But all
of these numbers can be expected to exert some intellectual influence as
well. Asking for committee members is a step towards other, non-advisor
influencors, and it is also a matter of record. Research institutions
are another way to capture intellectual interactions among
collaborators.


Even if/when the AI-PhD family tree is completed, it will
certainly not have captured all of what we mean by ``artificial
intelligence research.'' For example: \item The Dartmouth ``founding
fathers'' probably provide (direct) lineage for a {\em minority} of AI
PhD's currently working in the field.


PhD theses, themselves, are
probably some of the {\em worst} examples of research, in AI or
elsewhere. By definition, we are talking about students who are doing
some of their first work. They had {\em better} improve with time! \item
PhD's account for only a fraction of AI research.


 Nevertheless, Science
is primarily concerned with {accumulating} knowledge, at least as much
as it is about its initial discovery. A primary argument for interest in
the AIG is that traditional academic relationships, as embodied by PhD
genealogies, form a sort of ``skeleton'' around which the rest of
scientific knowledge coalesces. Certainly individuals can pursue their
own research program, and corporations can fund extended investigations
into areas that are not represented in academia whatsoever. But it is
hard to imagine extended research programs (like that ``fathered'' by
Roger Schank, for example) that do not involve multiple ``generations''
of investigators; academia is almost certainly the most successful
system for such propagation. Kuhnian and post-Kuhnian [<A
HREF="bibrefs.html#REF623">REF623] [<A
HREF="bibrefs.html#REF588">REF588] analyses highlight the importance
of {\em social} aspects of Science. ``Paradigms'' are extremely
appealing constructs, but they're also amorphous. For all of its faults,
the AI-PhD tree represents incontrovertible {\em facts}, just as word
frequencies do.




Top of Page

 | UP: Social relations among authors

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.4 Social relations among authors





FOA Home

 | UP: Inference beyond the \Index



Social relations among authors

Another potential source of information about documents that we can use
to augment statistial keywords is ``cultural'' information, capturing
some features of the social relationships among authors in a
field. Most of our discussion of documents has been as if they were
completely dead artifacts. But documents are written by people,
authors who write from a particular perspective. When their writing is
in science, or the law, or any other tradition within which they
participate, we can make reasonable guesses about some aspects of what
it is they are trying to say.


An author's education, in particular,
offers clues as to how we can interpret their words. Work and writing in
many fields requires extensive, graduate education. Students are soon
moved from common ``core'' curricula to more advanced material. Kuhn and
others have analyzed the central role textbooks play as part of the
social process of codifying a discipline [<A
HREF="bibrefs.html#REF162">REF162] . As students move beyond common
textbooks to the specialized training, their approach to the problem
often resembles that of their teachers (at least as long as they are
around the teacher). By knowing something about the author's education,
and especially about their dissertation advisor, we may have a basis for
interpreting their writing. The importance of dissertations as an
academic resource was recognized as early as 1940 by University
Microfilms Inc. (UMI) as copies of virtually every dissertation
published by many universities was microfilmed. UMI (now the Information
and Learning division of Bell \& Howell) makes its <A
HREF="http://wwwlib.umi.com/dissertations">Dissertation Abstracts
corpus available for WWW searching.

Subsections

	 6.4.1 A.I. Geneology
	 6.4.2 An emprical foundation for a philosophy of Science




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.1 Theorem-proving models for relevance





FOA Home

 | UP: Modes of inference



Theorem-proving models for relevance

Imagine the current state of knowledge possessed by a browsing user as a
set of axioms $ Then we can model their information need as a question:
can we infer that a theorem $\tau$ is true or false from our current
knowledge? \Sigma \stackrel{?}{\models} \tau The fact that they have an
information need can be taken as an assumption that there must be
additional knowledge, contained in some documents, which together with
their current knowledge base does allow $\tau$ (or $\neg \tau$) to be
proven. Relevant documents are exactly those which support such an
inference. \mathname{Rel} \equiv \left\{ d \mid \Sigma \cup d \models
\tau \right\} Practically speaking of course, this model is impossible.
It would demand a complete and consistent logical description
of the user's cognitive state. It also requires that the full set of
all possible logical facts contained in each and every
document be similarly encoded. (And then, of course, there is the
minor problem of searching for the minimal set of documents which
satisfy this inference!) of language, for example as documents are
passed around a business organization [<A
HREF="bibrefs.html#Winograd86">Winograd86] .}


Still, this basic
conception of existing knowledge being extended by new facts, and new
inferences being possible when new facts become known, is a very
provocative metaphor at the least. Sperber and Wilson's notion of
connected information corresponds exactly to such new inferences (cf.
Section &sect;8.2.2 ).


van Reisbergen has
talked about this strong notion of relevance as OBJECTIVE
RELEVANCE \vanR{147}. In more recent work, van Reisbergen has
extended this basic model to a full RELEVANCE LOGIC , based on a
four-valued semantics over $2^{\{T,F\}}$ [<A
HREF="bibrefs.html#vanR86">vanR86] .




Top of Page

 | UP: Modes of inference

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.2.1 Basic representation





FOA Home

 | UP: Spreading activation search



Basic representation

In AIR, each new document first causes a corresponding document node to
be generated. An author node is then generated (if it doesn't already
exist) for each author of the document. Two links are then created
between the document and each of its keywords (one in each direction),
and two more between the document and each of its authors. Weights are
assigned to these links according to an inverse frequency weighting
scheme: the sum of the weights on all links going out of a node is
forced to be a constant; in our system that constant is one. Figure
(figure) shows the subnet corresponding to the book
Parallel Models of Associative Memory, by J. A. Anderson and G.
E. Hinton [REF35] .


The initial network
is constructed from the super-position of many such documents'
representations. Most of the experiments to be described in this report
used a network constructed from 1600 documents, forming a network of
approximately 5,000 nodes. This is a trivial corpus, and used relatively
crude lexical analysis and keyword weighting ideas. However, AIR
requires only that the initial automatic indexing assign some
weighted set of tentative keywords to each document.


There is one
property of the inverse weighting scheme on which AIR does depend,
however. A network built using this keyword weighting scheme, together
with similar constraints on the weights assigned author links, has the
satisfying property of {conserving activity}. That is, if a unit of
activity is put into a node and the total outgoing associativity from
that node is one, the amount of activity in the system will neither
increase nor diminish. This is helpful in controlling the spreading
activation dynamics of the network during querying.




Top of Page

 | UP: Spreading activation search

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.2.2 Querying and retrieval





FOA Home

 | UP: Spreading activation search



Querying and retrieval

Users begin a session with AIR by describing their information need,
using a very simple query language. An initial query is composed of one
or more clauses. Each clause can refer to one of the three types of
``features'' represented in AIR's network: keywords, documents or
authors, and all but the first clause can be negated. This query causes
activity to be placed on nodes in AIR's network corresponding to the
features named in the query. This activity is allowed to propagate
throughout the network and the system's response is the set of nodes
that become most active during this propagation.


The traditional result
of a query is only documents. AIR also provides keywords and authors.
Keywords retrieved in this manner are considered RELATED TERMS
that users may use to pursue their searches. Retrieved authors are
considered to be closely linked to the subject of interest. There are
many ways in which a user might find related terms and centrally
involved authors a valuable information product in their own right.


Figure
(figure) shows AIR's response to a typical query: \small {\tt
((:TERM ``ASSOCIATIVE'')(:AUTH ``ANDERSON,J.A.''))} This is the network
of keywords, documents and author's considered relevant to this query.
The nodes are drawn as a tri-partite graph, with keywords on the top
level, documents in the middle and authors on the bottom. Associative
links that helped to cause a node to become retrieved (and only those
links) are also displayed. Heavier lines imply stronger associative
weights. AIR uses directed links, and this directionality is represented
by the concavity of the arcs; a clockwise convention is used. For
example, a link from a document node (in the middle level) to a keyword
node (in the top level) goes clockwise, around to the left.


Actually,
this is only a picture of the final state of the system's retrieval. The
network is actually drawn {incrementally}, with the first nodes to
become significantly active being drawn first and in the middle of the
pane. As additional nodes become active at significant levels, they are
drawn farther out along the three horizontal axes and the links through
which they became active are drawn as well. This dynamic display has at
least two real advantages. First, the fact that AIR provides the first
part of its retrieval almost immediately means that the user is not
impatiently waiting for the retrieval to complete (typically 5-10
seconds in this implementation). Second, displaying the query's dynamics
helps to give the user a tangible feeling of ``direct manipulation'' [<A
HREF="bibrefs.html#REF654">REF654] ; the user ``prods'' the network
in a certain place, and then watches as waves of activity flow outward
from that place.




Top of Page

 | UP: Spreading activation search

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.2.3 \RelFbk in AIR





FOA Home

 | UP: Spreading activation search



\RelFbk in AIR

Queries subsequent to the first are performed much differently. After
AIR is done retrieving the network of features, the user responds with
indicating which features are considered (by that user) relevant to the
query and which are not. Using a mouse, the user marks features with the
symbols: {\bf ++, +, --} and {\bf -- --}, indicating that the feature
was {\tt Very Relevant, Relevant, Irrelevant}, or {\tt Very Irrelevant},
respectively. Not all features need be commented upon.


The system
constructs a new query directly from this feedback. First, terms from
the previous query are retained. Positively marked features are added to
this query, as are the negated versions of features marked negatively.
Equal weight is placed on each of these features, except that features
marked {Very Relevant} or {\tt Very Irrelevant} are counted double.


From
the perspective of retrieval, this RelFbk becomes a form of {\em
browsing}: positively marked features are directions which the user
wants to pursue, and negatively marked features are directions which
should be pruned from the search. From the perspective of learning,
thisRelFbk is exactly the training signal AIR needs to modify its
representations through learning. This unification of learning (i.e.,
changing representations) and doing (i.e., browsing) was a central
component of AIR's design. It mean that the collection of feedback is
not an onerous, additional task for the user, but a natural part of the
retrieval process.




Top of Page

 | UP: Spreading activation search

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.2.4 Learning in AIR





FOA Home

 | UP: Spreading activation search



Learning in AIR

Nodes marked by the user with positive or negative feedback act as
sources of a signal that then propagates backwards along the weighted
links. A local learning rule then modifies the weight on links directly
or indirectly involved in the query process. Several learning rules were
investigated; the experiments reported here used a learning rule that
correlated the activity of the PRE-SYNAPTIC $node_i$ with the
feedback signal experienced by the POST-SYNAPTIC $node_j$: w_{ij}
& \propto & Corr(n_{i}& Corr(n_{i}\ active, n_{j}\ relevant) \\ & = &
\frac{\mu_{a_{i} \cdot r_{j}} - \mu_{a_{i}} \cdot
\mu{r_{j}}}{\sigma_{a_{i}} \cdot \sigma_{r_{j}}} \\ & = & \frac{\Sigma
(a_{i} \cdot r_{j}) - \frac{\Sigma a_{i} \Sigma r_{j}}{N}}{\sqrt{\Sigma
a_{i}^{2} - \frac{(\Sigma a_{i})^{2}}{N}} \sqrt{\Sigma r_{j}^{2} -
\frac{(\Sigma r_{j})^{2}}{N}}} \end{eqnarray*}


AIR makes a most-direct
correspondence between the connectionist notion of {activity} and the IR
notion of $\Pr(\mathname{Rel})$ (cf. Section <A
HREF="foa-5-5.html">&sect;5.5 ): The activity level of nodes at the
end of the propagation phase is considered to be a prediction of the
probability that this node will be judged relevant to the query
presented by the user. This interpretation constrained the AIR system
design in several ways (e.g., activity is a real number bounded between
zero and one, query nodes are activated fully). AIR also allows negative
activity, which is interpreted as the probability that a node is {\em
not} relevant. The next step of the argument is to consider a link
weight $w_{AB}$ to be the conditional probability that $Node_{B}$ is
relevant given that $Node_{A}$ is relevant. Next, this definition must
be extended inductively to include indirect, transitive paths that AIR
uses extensively for its retrievals.


The system's interactions with users
are then considered experiments. Given a query, AIR predicts which nodes
will be considered relevant and the user confirms or disconfirms this
prediction. These results update the system's weights (conditional
probabilities) so as reflect the system's updated estimates. Thus, AIR's
representation results from the combination of two completely different
sources of evidence: the word frequency statistics underlying its
initial indexing; and the opinions of its users.


A straight-forward
mechanism exists for incrementally introducing new documents into AIR's
database. Links are established from the new document to all of its
initial keywords and to its authors; new keyword and author nodes are
created as necessary. The weights on these links are distributed evenly
so that they sum to a constant. Because the sum of the (outgoing)
weights for all nodes is to remain constant, any associative weight to
the new document must come from existing link weights. a new parameter
({*CONSERVATIVE*}) is introduced to control the weight given these new
links at the expense of existing ones. If the network is untrained by
users, this parameter can be set to zero so as to make the effect of an
incremental addition exactly the same as if the new document had been
part of the initial collection. In a trained network, setting {\tt
*CONSERVATIVE*} near unity insures that the system's experience
incorporated in existing link weights is not sacrificed to make the new
connections. Also, note that the computation required to place the new
document is strictly local: only the links directly adjacent to the new
documents immediate neighbors need be changed. The major observation
about the inclusion of new documents, however, is that there is an
immediate ``place'' for new documents in AIR's existing representation.


A
second source of new information to the AIR system comes from users'
queries. If a query contains a term unknown to AIR, this term is held in
abeyance and AIR executes the query based on the remaining terms. Then,
after the user has designated which of AIR's responses are relevant to
this query, a new node corresponding to the new query term is created
and becomes subjected to exactly the same learning rule used for all
other nodes.


While easily incorporating new documents and new query terms
are valuable properties for any IR system, from the perspective of
machine learning these are both examples of simple rote learning, and
necessarily dependant on the specifics of the IR task domain. The main
focus of the AIR system is the use of general purpose connectionist
learning techniques that, once the initial document network is
constructed, are quite independent from the IR task.




Top of Page

 | UP: Spreading activation search

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.2.5 Generalized representation





FOA Home

 | UP: Spreading activation search



Generalized representation

The standard, inter-keyword and inter-document associations typically
evaluated as part of keyword and document clustering are part of the
broader context of the associative representation used by AIR. The
system extends these pairwise clustering relations and the fundamental
keyword-document Index relation to include higher-order {\em
transitive} relations as well. That is, if node A is associated with
node B and node B is associated with node C, then node A is also
considered to be associated with node C, but to a lesser extent.


Obviously,
this transitive assumption is not always valid, and this may be why most
IR research has not considered this extension. But AIR was is an
adaptive system, and one of the critical problems facing any learning
system is the generation of {plausible hypotheses}; i.e., theories which
stand a better than average chance of being correct. Transitivity is
considered a default assumption, the consequences of which will be
subjected to adaptation which favor appropriate transitivities and cull
out inappropriate ones.


It is interesting to contrast the adaptive
changes made by AIR in response to RelFbk with the documet
modification strategies of Salton with Brauen et al. [<A
HREF="bibrefs.html#REF566">REF566] [<A
HREF="bibrefs.html#REF567">REF567] mentioned in Section <A
HREF="foa-4-2-2.html">&sect;4.2.2 (and returned to again in Section
&sect;7.3 ). The query-document matches used
as the basis of their changes considers only direct, keyword-to-document
associations while AIR makes use of a much wider web of indirect
associations as well. To a first approximation the changes made by AIR
to direct keyword-to-document associations are not unlike those proposed
by Salton and Brauen (if I'd only known!). But AIR makes other changes,
to more indirect associations as well.


Salton and Buckley have analyzed
the spreading activation search used in some of these systems and
concluded that it is inferior to more traditional retrieval methods [<A
HREF="bibrefs.html#REF668">REF668] [<A
HREF="bibrefs.html#Salton88b">Salton88b] . They point out: ... the
relationships between terms or documents are specified by {\em labeled}
links between the nodes .... the effectiveness of the procedure is
crucially dependent on the {\em availability} of a representative node
association map (p. 4,5) [Emphasis added]


 In a weighted, associative
representation the semantics of indexing,inter-document and
inter-keyword clustering links are dropped in favor of a single,
homogeneous ASSOCIATIVE RELATION. AIR treats all three types of
weighted links equally. For example, if inter-document citation data had
been available this information could naturally be included a swell;
again the semantics of these relations would have been dropped in favor
of a simple associative weight. The contrast between the use of use of
spreading activation search in {\em connectionist} networks with its use
in {\em semantic} networks is admittedly a subtle one, but it is also
critically important [REF672] . One
clear difference is that semantic networks typically make logical,
deterministic use of labeled links, while connectionist networks like
AIR rely on weighted links for probabilisitic computations.




Top of Page

 | UP: Spreading activation search

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.2 Spreading activation search





FOA Home

 | UP: Modes of inference



Spreading activation search

The fact keywords can be associated with documents, that keywords can be
associated with one another, documents become associated by citations,
etc. lead early IR pioneers such as Doyle and Stiles to adopt simple
association as a unifying relation connecting many objects of
the FOA inquiry [REF317] [<A
HREF="bibrefs.html#Doyle61">Doyle61] [<A
HREF="bibrefs.html#Doyle62">Doyle62] [<A
HREF="bibrefs.html#REF322">REF322] [<A
HREF="bibrefs.html#Maron82">Maron82] [<A
HREF="bibrefs.html#Giuliano63">Giuliano63] [<A
HREF="bibrefs.html#Findler79">Findler79] . Integrating information
across SEMANTIC NETWORKS of such associative relations has been
an important example of knowledge representation within AI since the
memory models of Collins and Qullian [<A
HREF="bibrefs.html#REF170">REF170] . In its simplest form, a simple
quantity known as ACTIVITY is allowed to propagate through a
network like that shown in Figure (figure) from several
sources. SPREADING ACTIVATION SEARCH is the name for a broad
range techniques that find solutions (for example, a path from Start to
Goal in Figure (FOAref) ) by controlling the propagation of
activity through associative networks like this [<A
HREF="bibrefs.html#REF321">REF321] [<A
HREF="bibrefs.html#REF668">REF668] [<A
HREF="bibrefs.html#REF667">REF667] .


The Adaptive Information
Retrieval (AIR) system was a prototype search engine built as part of my
dissertation at the University of Michigan in the mid-1980s [<A
HREF="bibrefs.html#REF222">REF222] [<A
HREF="bibrefs.html#REF622">REF622] . This research was part one of
several systems applying CONNECTIONIST (neural network) learning
methods to the IR search engine problem [<A
HREF="bibrefs.html#REF568">REF568] [<A
HREF="bibrefs.html#REF732">REF732] [<A
HREF="bibrefs.html#REF773">REF773] [<A
HREF="bibrefs.html#REF430">REF430] .


Figure (figure) shows
how many of the features discussed here can interact as part of a single
retrieval system. This figure comes from Dan Rose's SCALIR (Symbolic and
Connectionist Approach to Legal Information Retrieval) system, built to
investigate the use of both logical, ``symbolic'' modes of inference and
probabalitic, ``subsymbolic'' ones. This figure shows containment
relations between document elements, (like those shown in more detail in
(FOAref) ) topical connections between keywords, and
inter-document citations, all mixed and used as part of spreading
activation-based inference.

Subsections

	 6.5.2.1 Basic representation
	 6.5.2.2 Querying and retrieval
	 6.5.2.3 \RelFbk in AIR
	 6.5.2.4 Learning in AIR
	 6.5.2.5 Generalized representation




Top of Page

 | UP: Modes of inference

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5.3 Discovering latent knowledge within a corpus





FOA Home

 | UP: Modes of inference



Discovering latent knowledge within a corpus

Nothing is more frustrating than spending many, many hours on a
technical problem, unless it is then finding out that someone else had
{previously solved} this same problem! One of the motivations shared by
many working on search engine technology is the hope that we can reduce
the number of times the same wheel is reinvented.


D.R. Swanson has
concentrated on the scientific and medical literature, and viewed it as
``a potential source of new knowledge.'' [<A
HREF="bibrefs.html#Swanson90">Swanson90] [<A
HREF="bibrefs.html#Swanson97">Swanson97] . Each new medical report
contains new knowledge about some particular disease or treatment. But
Swanson has gone the next step, to imagine what modes of inference might
be most appropriate across a {\em network} of such papers each
describing potential causal relations: Each scientific article
contributes to a web of logical connections that interlace the
literature of science. Some of these connections are made explicit
through references from one article to another, citatipns that reflect,
among other things, authors' perceptions of how their own work is
related to that of others and how it fits into the scheme of existing
knowledge. However, there may exist many implicit logical interarticle
connections that are unintended and not marked by citations; such
implicit links are the focus of this paper. (The word ``logical'' here
is used informally; a ``logical connection'' is formed by statements
that are related by any process of scientific reasoning or argument.)
Scientific articles can be seen as clustering into more or less
independent sets or ``literatures." Within each set, common probiems are
addressed, common arguments are advanced, and articles ``interact'' by
citing one another. Distinct literatures that are essentially unrelated
are in general "noninteractive" in that they do not cite or refer
directly to each other, have no articles in common, and are not cited
together by other articles. On the other hand, if two literatures are
linked by arguments that they respectively put forward -- that is, are
``logically'' related or connected -- one would expect them to cite each
other. If they do not, then the logical connections between them would
be of great interest, for such connections may be unintended, unnoticed,
and unknown-therefore potential sources of new knowledge.... The number
of possible pairs of literatures (that might be connected) increases
very nearly as the square of the number of literatures; the number of
possible connections increases at even a greater rate if triple or
higher combinations are taken into account rather than just pairs. From
this perspective, the significance of the "information explosion" may
lie not in an explosion of quantity per se, but in an incalculably
greater combinatorial explosion of unnoticed {\em logical} connections.
(p. 29,35)


 Swanson's first and most well-known example of new knowledge
discovered in this fashion identified fish oil as a treatment for
Raynaud's syndrome, a circulatory problem resulting in poor blood supply
to the extremities. Figure (figure) shows a second example of
Swanson's method. Beginning with a syndrome like migraine headaches,
Swanson searches the literature for features mentioned in the context of
migraines which also have been mentioned in a second, disjoint,
``mutually oblivious'' literature. Working backwards from the syndrome
to be explained, a query is formed against a medical corpus like
MEDLINE. The resulting set of (in this case 63) documents is then
analyzed and clustered into related sets; it is in this clustering that
Swanson's {\em manual}, intellectual effort is most obvious. The
clusters are then used to suggest additional, new queries. Finally, the
single common ``cause'' of MAGNESIUM is identified. Fig.
(FOAref) also shows an attempt to classify the relationship
between common phrases in the retrieved literature and the relation to
the syndrome. Further, the relationship between these same phrases, or
lexical variations on them! and the common cause magnesium is also
forced into structured relationships.


It is hard to imagine a more
exciting prospect for the analysis of all literature. The identification
of such ``undiscovered public knowledge'' is almost certainly possible
in many other situations. The question becomes how we might
algorithmically search for all of them. Note especially the liberties
taken in interpreting the literature and phrases used consistently
within it as they have been transformed into the {structured relations}
of Fig. (FOAref) . These relations are meant to suggest more
formal and powerful modes of inference between causes and effects
mentioned within each paper. Some of the arrowheads show suggest causal
relationships; some relationships (e.g., {\tt associated\_with}) are
neutral with respect to correlation versus causation; others ({\tt
type\_of}) suggest hierarchic relations between classes.


Swanson is
well-aware of these difficulties: ... [the] form and structure of
logically connected arguments are in general recognizable by scientists
irrespective of their specialty, a point that may have implications for
research on futuristic, more fully automated systems. However, the
simple structure of the syllogistic model does not in many respects
reflect the depth or range of actual problems that would be encountered
if one tried to build a database of logical connections\ldots. The
objective, moreover, is not simply to draw mechanistic logical
inferences, but rather to deter- mine whether certain plausible
connections or hy- potheses appear to be worth testing. Most articles
harbor, either explicitly or implicitly, an enormous number of logical
connections. Which connections are relevant and important can be de-
termined only in the light of a specific context, hy- pothesis, problem,
or question; but such contexts and questions are always changing. The
degree to which one can hope to encode logical connections in any form
suitable for all future uses may therefore be quite limited. (p. 35)



Recognizing that even if fully automatic discovery of new facts is
currently too hard, Gordon and Lindsey have investigated forms of
``discovery support'' [Gordon96] .
Gordon and Dumais have also explored the use of LSI techniques (cf.
Section &sect;5.2.3 ) as part of the
literature-based discovery process [<A
HREF="bibrefs.html#Gordon98">Gordon98] . The formulization of SVD,
eigenstructure analysis of the relationship between a query and
documents (step 2) and then the analysis of these intermediate
literatures and ultimate causes (step 3) is an important extension
beyond the manual investigations originally proposed by Swanson.




Top of Page

 | UP: Modes of inference

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.5 Modes of inference





FOA Home

 | UP: Inference beyond the \Index



Modes of inference

If deduction is the process of proceeding from a set of rules to the
implications of those rules, and if induction is the process of forming
rules based on patterns across many examples, and abduction the process
of forming hypotheses worthy of asking, FOA can also be viewed as an
inference process: the process of forming questions that illicit
desirable answers.

Subsections

	 6.5.1 Theorem-proving models for relevance
	 6.5.2 Spreading activation search
	 6.5.3 Discovering latent knowledge within a corpus




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.6.1 Geographical hitlists





FOA Home

 | UP: Deep interfaces



Geographical hitlists

SINGULAR tokens are those proper nouns -- people, places, events
-- that have a unique reference in the world. They are distinguised from
GENERAL terms which refer to CATEGORIES of objects in the
world. Distinctions like this have been a part of linguistic analysis
since the beginning [REF54] , and many
with a background in AI will recall Ron Brachman's {\tt
CLYDE\_THE\_ELEPHANT} example [REF336]
[REF669] .


In FOA the distinction
initially arose out of practical considerations. The basic morphological
procesing of folding case, Porter's stemmer and similar tools is
designed primarily to deal with what we could, in the present context,
call GENERAL TERMS only. Conversly, proper names (family and
place names, etc.) rarely observe morphological transformation names.
The capitalization which often flags singular proper nouns is thrown
away, rather than actually helping to ease the task of automatically
identifying and parsing them. Names like {\tt JOHNSON} purportedly began
as names to describe sons of John. Suggest rules like those used in
Porter's stemmer to exploit systematic variations in family names such
as this.


 It is no wonder, then, that NAME-TAGGING techniques
which deal intelligently with singular tokens was an early area of
search engine development [Rau88] [<A
HREF="bibrefs.html#Jacobs87">Jacobs87] . Identifying the sub-class
of people singulars is an especially active area. Relatively
small dictionaries of the ``movers and shakers'' of the modern world --
politicians, captains of industry, artists, etc. -- can provide an
especially informative and commercially valuable set of additional
indexing tokens in applications such as financial news services.


Chris
Needham has proposed an interesting strategy for progressively applying
stronger models of representation based on various classes of singulars
(personal communication). Working on a representation for editors, the
procedure Needham and his group hit upon was to \item first describe
places in the world; \item then people who live (are
born, travel through and then die) in these places; and finally \item
events involving people at locations. Specification of one
layer of terminology provided a concrete frame of reference for the
next: Events involve people, which are associated with places. This
suggests one argument for focusing on place-related singulars first. But
modeling even this ``simplest'' class of propoer names quickly required
even tighter focuse onto PHYSICAL PLACES about which it was quite
easy to give very concrete reference and distinguished from POLITICAL
PLACES whose names and extents can vary dramatically. As editors of
the \EB, these designers were especially aware of how historically and
culturally sensitive resolving political place names could be.


But at
least for physical locations, the emergence of GLOBAL POSITIONING
SYSTEM (GPS) technologies that allow users to know their position
within a single, reconciled geographic frame has helped to drive a
growing market for GEOGRAPHICAL INFORMATION SYSTEMS (GIS)
software. and the development of world-wide AUTHORITY LISTS of
place names (e.g., The
U. S. Board on Geographic Names (BGS) and the earlier <A
HREF="http://164.214.2.59/gns/html/FIPS/FIP10-4.html">Federal
Information Processing Standards (FIPS) ``Countries, Dependencies,
Areas Of Special Sovereignty, And Their Principal Administrative
Divisions'' list). Like people's names, place data is an important
information commodity.


Further, human cognition has evolved to live in a
three-dimensional world. We each have deep psychological commitments to
basic features of our physical space and orientation with respect to a
spatial frame of reference [REF47] [<A
HREF="bibrefs.html#Kosslyn80">Kosslyn80] . In contrast to all the
other abstract, disembodied dimensions along which information often
barrages a user's screen, place information is special. Our experience
of time is the other important experiential dimension, as demonstrated
by representations like the TIME LINE . The orientation provided
by such concrete frames can be critical.


Consider, for example, the query
{CIVIL WAR BATTLE} and its conventional retrieval, as shown in Figure
(figure) Instead we should be able to see these retrieved items
in the geographical frame they naturally suggest, as shown in Figure
(figure)


 Note the steps this required: First the textual
hitlist was parsed for geographical tokens. Next, the map coordinates
for each of these WiW entries are collected, and a CONVEX HULL
(bounding polygon) for at least a majority of them is computed. Finally
the map which best contains this region is identified, zoomed and
shifted to best fit them.


Within this same frame, a user also immediately
knows how to DRAW QUERIES , for example restricting search to
only those battles near the East Coast, or along a particular river.
With modern graphical techniques, animation of these battles as a
time-line slider is slid back and forth is almost trivial. But the
additional power of visualization and DIRECT MANIPULATION
interface techiques [REF654] such as
these to browsing users is enormous. The important thing is that this
additional functionality is not at the expense of a much more complex,
complicated interface of commands or even menu items. People already
know what space \means, how to interpret it and how to work within it.




Top of Page

 | UP: Deep interfaces

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.6 Deep interfaces





FOA Home

 | UP: Inference beyond the \Index



Deep interfaces

Probably because interface technology did not support rich graphical
presentations and pointing devices until late in the development of
computer science, the human-computer interface (HCI) is often an
after-thought in software engineering. This bias remains today in search
engine designs which assume the indexing and match algorithms can be
separated from the presentation of result [<A
HREF="bibrefs.html#Harman92b">Harman92b] [<A
HREF="bibrefs.html#REF654">REF654] .


Sometimes interface design is
approached as a data visualization task [<A
HREF="bibrefs.html#Veerasamy96">Veerasamy96] . Korfage VIBE
presentation [Korfhage95]
highlights a user and author's perspectives on topical areas. Rather
than assuming that there is any absolute, preferred perspective on
keywords, Korfage considers what the words look like from the
perspective of users and authors, respectively.


In terms of the vector
space model, we can think of these as projections. The huge dimensional
space of keywords, or even the still large reduced dimension
representation, is still far more than we can visualize on the two
dimensional computer screen. We can try to impose other dimensions (e.g.
color, size) but still we must pick some projected subspace of the
larger data set. Norman and Schneiderman have written extensively on the
design of interfaces which are deeply connected to the user's underlying
task [Norman88] [<A
HREF="bibrefs.html#Shneiderman92">Shneiderman92] ; Marchianoni has
focused particularly on interfaces for the ``information seeking'' task
[Marchionini95] . As part of
the Xerox PARC group, Hearst has explored a number of visualization
techniques (cf. Figure (FOAref) ); she has also recently
provided an extensive survey of interface technologies [<A
HREF="bibrefs.html#Hearst99">Hearst99] .

Subsections

	 6.6.1 Geographical hitlists




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.7 FOA(The Law)





FOA Home

 | UP: Inference beyond the \Index



FOA(The Law)

The careful reader will have noticed that there have been an unusually
large number of examples drawn from the legal domain. There are a number
of things it can teach us about general principals of FOA [<A
HREF="bibrefs.html#REF398">REF398] . The Common Law tradition is
very old, and has been connected to an organized corpus of documents in
free text for a very long time. As the Doonesbury cartoon (part of Gary
Trudeau's Bicentennial series) in Figure (figure) suggests,
legal documents help to demonstrate just how long prose can live beyond
its drafting. Hafner was one of the first to recognize this, manually
representing a wide range of attributes for a small number of documents
in the legal domain [REF131] . This
work has now become a part of a larger effort within AI to model the
legal reasoning process (e.g., reasoning by analogy [<A
HREF="bibrefs.html#Ashley90">Ashley90] ). Here we are most concerned
with what can we learn from lawyers who have FOA the Law which might
generalizes to other corpora and searchers.


First, the fact that judicial
opinions have been written for so long and in such a particular
``voice'' has meant that it is also possibel to consider special
linguistic characteristics of this legal genre [<A
HREF="bibrefs.html#Goodrich87">Goodrich87] [<A
HREF="bibrefs.html#Levi82">Levi82] .


Second, notions of citation are
especially well-used within this corpus. Simple concepts like impact
have been described [REF1074] , and
theories of legal citation proposed [<A
HREF="bibrefs.html#REF128">REF128] [<A
HREF="bibrefs.html#REF1069">REF1069] [<A
HREF="bibrefs.html#REF1071">REF1071] . Obviously, manipulation of
access to the legal printed record (for example by controlling which
judges' opinions are made available!) has enormous political
ramifications [Brenner92] . This
become even more true as recent consolidation of the media industry
means that one or two corporations effectively control the entire
process of legal publication.


Third, the backbone of the legal process is
an adversarial argument. This dialectic is often explicit, for example
as marked by the { cf., but cf.} citation conventions (cf. Section <A
HREF="foa-6-1.html">&sect;6.1 ). The presense of such syntactic
marker makes it conceivable to analyze polazrization across an entire
legal literature. Of course the arguments contained in briefs and
opinions contain a great deal more structure than simple opposition. The
analysis of legal ARGUMENT STRUCTURES , in conjunction with the
textual foundations of Common Law, is perhaps the most important feature
of the legal domain to FOA. One the one hand, it is possible to model
individual documents and their {\em logical} features so as to reason
about them [REF131] [<A
HREF="bibrefs.html#REF124">REF124] . Special DEONTIC LOGICS
have been developed especially to deal with the concepts of ``rights''
and ``obligations'' that are at the heart of many legal relations [<A
HREF="bibrefs.html#REF730">REF730] .


{Statistical} analyses of large
document corpora may seem contrary to {\em logical} analyses of the
arguments contained in each of them, and in fact this chasm runs very
deep. Not only does it suggest quite different technology bases from the
arsenal of (roughly inductive vs. deductive) AI techniques, but reflects
a tension within the law itself. Rose [<A
HREF="bibrefs.html#REF1113">REF1113] refers to a spectrum of legal
philosophies ranging from ``formalism'' to ``realism.'' On the one hand,
many legal documents certainly seem to function logically, with careful
definitions and reasoning that Langdell [<A
HREF="bibrefs.html#Langdell87">Langdell87] has idealized as
``mechanical jurisprudence.'' At the same time analyses such as the
Critical Legal Studies [Unger83] have
helped to demonstrate that the Law is just another social process.


It is
exactly this dual nature of the Law and hence legal texts that make it
especially interesting as an example of FOA [<A
HREF="bibrefs.html#Nerhot91">Nerhot91] . Individual applications
include LITIGATION SUPPORT systems which allow lawyers to search
through the truckloads of documents involved in extended trials. On a
much larger scale, systems like West Group's <A
HREF="http://www.westlaw.com/">WestLaw and Reed Elsevier's <A
HREF="http://www.lexis.com">LEXIS systems provide access to the bulk
of statuatory and case law to all practicing lawyers [<A
HREF="bibrefs.html#REF1064">REF1064] [<A
HREF="bibrefs.html#REF733">REF733] .




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.8 FOA(Evolution)





FOA Home

 | UP: Inference beyond the \Index



FOA(Evolution)

For some time the study of evolution has demanded an especially
interdisciplinary approach, and so it is no wonder that profound
difficulties in communication arise as scientists trained in paradigms
as varied as biology, psychology and even computer science [<A
HREF="bibrefs.html#REF1123">REF1123] attempt to communicate with one
another. Now, of course, theories of evolution are increasingly informed
by huge volumes of concrete data, generated by the Human Genome Project
and related efforts. Serendipitously, the field of molecular biology is
also one of the first (but quite certainly not the last) disciplines to
undergo a qualitative change because of the WWW. The nearly simultaneous
growth of the WWW and genomic databases has meant that computational
biology as a science has grown up with a very advanced notion of
publication. Beyond formal publication channels, even beyond informal
email and discussion groups, the genomic databases at the heart of
molecular biology today may point to forms of communication among
scientists which are arguably, like the image-based WWW traffic,
POST-VERBAL .


The flood of biological sequence data -- nucleic
acid, proteins, and now gene expression networks, metabolic pathways --
into sequence databases, with the related flood of molecular biology
literature, represents an unprecedented opportunity to investigate how
concepts learned automatically from various data sets relate to the
words and phrases used by scientists to describe them. Learning this
linkage -- between molecular biology concepts and the
genomic data relating to them -- can be described as
annotating the data. It is now possible to learn many of these
correspondences automatically, guided by the RelFbk of practicing
scientists, as a natural by-product of their browsing through genome
data and publications related to them. RelFbk provides a key
additional piece of information to learning algorithms, beyond the
statistical correlations that may exist within the genome data or
textual corpora treated independently: It captures the fact that a
scientist who understands both the sequence data and the journal
articles deeply does (or does not) believe that a particular sequence
and particular keyword/concept share a common referent. Sequences are
posted, annotations are often automatically constructed based on
HOMOLOGOUS relations to other sequences found in the databases. A
different variety of ``sequence search engines,'' specially developed to
look for similarities among sequences rather than among documents,
become the basis for retrievals. These retrievals can and often do
connect the work of one scientist to that of another without a single
verbal expression passing.


Figure (figure) sketches the basic
relations. On the bottom are the most fundamental classes of molecular
data, namely gene and protein sequences. On the top is a set of
scientific documents, such as those found in MEDLINE. The primary
relation connecting between the raw genetic data and textual corpora are
ANNOTATION links that scientists have (manually) established
between articles and sequences are both significant and useful. They are
significant because they help to establish the construction of the
genome as a piece of the scientific enterprise, linking it to the
traditions of academic publication. They are also useful to many
scientists who, for example, are interested in a particular gene or
protein and want to find out all that others might know about it. But
annotation is not done consistently by all participating scientists, nor
has a precise semantics for what exactly an annotation should mean been
established. The Entrez
interface to MEDLINE makes it convenient for a user with a particular
sequence in mind to find its corresponding publication, and vice versa.
Together with the MESH thesaurus of medical terms (cf. Section <A
HREF="foa-6-3.html">&sect;6.3 ), these features make the National
Library of Medicine's resource one of the most advanced on the WWW.


In
addition to expediting the searches of scientists and doctors, the
identification of significant patterns in one modality (i.e., in text or
in sequence data) can be used to suggest hypotheses in the other
(similar to suggestions made by Swanson (cf. Section <A
HREF="foa-6-5-3.html">&sect;6.5.3 ). Also shown in the figure are
$\mathcal{S}im$ arcs relating ``similar'' data. In the case of genetic
or protein sequence data, these similarity measures are typically based
on a notion of ``edit distance'' generated by string-matching tools such
as BLAST and FastA, but the investigation of new methods for this
problem is one of the most active areas within machine learning (cf. [<A
HREF="bibrefs.html#Glasgow96">Glasgow96] ). The investigation of
inter-document similarities has been an important problem within the
field of information retrieval (IR) for many decades. Most document
similarity measures are based on correlations between ``keywords''
contained by pairs of documents, but other methods (e.g., based on a
bibliometric analysis of shared entries in the documents'
bibliographies) have also received considerable attention.




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.9.1 Grounding symbols in texts





FOA Home

 | UP: Text-based intelligence



Grounding symbols in texts

According to Harnad's grounding hypothesis, if computers are ever to
understand natural language as fully as humans, they must have an
equally vast corpus of experience from which to draw [<A
HREF="bibrefs.html#REF425">REF425] . We propose that the huge
volumes of natural language text managed by hypertext systems provide
exactly the corpus of ``experience'' needed for such understanding. Each
word in every document in a hypertext system constitutes a separate
experiential ``data point'' about what that word \means. The exciting
prospect of using search engines as a basis for natural language
understanding systems is that their understanding of words, and then
concepts built from these words, will reflect the richness of this huge
base of textual ``experience.'' Their are of course differences between
the text-base ``experience'' and first-person, human experience, and
these imply fundamental limits on language understanding derived from
this source.


In this view, the computer's experience of the world is
second-hand, via documents written by people about the world and
subsequently through users' queries of the system. The ``trick'' used is
to learn what words mean by interacting with users who already know what
the words mean, with the documents of the textual corpus forming the
common referential base of experience.


The hypertext itself is in fact
only the first source of information, viz., how authors use and
juxtapose words. The second, ongoing source of experience is the
subsequent interactions with users, a new popualtion of people who use
these same words and then react positively or negatively to the system's
interpretation of those words. Both the original authors and the
browsing users function as the text-based intelligent system's ``eyes''
into the real world and how it looks to humans. That insight is
something no video camera will ever give any robot.




Top of Page

 | UP: Text-based intelligence

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6.9 Text-based intelligence





FOA Home

 | UP: Inference beyond the \Index



Text-based intelligence

Knowledge representation has always been a central issue for AI, and as
a sub-discipline within Computer Science it's primary contribution is
probably the beginnings of a computational theory of knowledge. While it
is still too early to speak of such a theory, some key aspects of good
knowledge representation are becoming clear [<A
HREF="bibrefs.html#REF438">REF438] .


The text captured in document
corpora was not entered with the { intention} of being part of a
knowledge base. These are documents written by someone as part of a
natural communication process, and any search engine technology simply
gives this document added life. Alternatively, we can say that the
document {\em was} intended to become part of a ``knowledge base,'' but
one that pre-dates (at least the AI) use of that term: people publish
their documents with the explicit hope that their ideas can become part
of our collective wisdom and used by others.


Note the ease with which
author-as-knowledge engineer can express their knowledge. Hypertext
knowledge bases are accessible to every writer. In this view, hypertext
solves the key AI problem of the KNOWLEDGE ACQUISITION BOTTLENECK
, providing a knowledge representation language with the ease,
flexibility and expressiveness of natural language --- by actually using
natural language! The cost paid is the weakness of the inferences that
can be made from a textual foundation: contrast the strong
theorem-proving notions of inference of Section <A
HREF="foa-6-5-1.html">&sect;6.5.1 with the many confounded
associations which arise in Swanson's analysis of latent knowledge in
Section &sect;6.5.3 .

Subsections

	 6.9.1 Grounding symbols in texts




Top of Page

 | UP: Inference beyond the \Index

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21





FOA: 6 Inference beyond the \Index





FOA Home

 | UP: foa-book.tex



Inference beyond the \Index

The that critical mapping between documents and descriptive keywords,
has dominated our approach to FOA in all the preceeding chapters. But
there is of course a larger context of available information: FOA can be
accomplished by showing a user relations among keywords, by
acquainting him or her with important authors, by pointing to important
journals where relevant documents are often published, etc. Retrieval of
all these information resources, especially when structured in
meaningful interface, can tell a user much more than simply listing
relevant documents.


This chapter is concerned with exploiting a variety
of other clues we might have about documents (and keywords, authors,
etc.), above and beyond the statistical, word-frequency information that
has been at the heart of the Index relation. In all cases,
these techniques identify some new source of data, represent it
efficiently, and then perform some kind of inference over the
representation.


AI is a subdiscipline of computer science centrally
concerned with questions of knowledge representation and inference over
those representations, especially when these algorithms arguably lead to
``intelligent'' behaviors. (In many ways the best characterization of
the AI domain is extensional one provided by the corpus of Ph.D.
dissertations so classified by U.M.I.) We could expect, therefore, that
there would be a great deal of cross-fertilization between AI methods
and IR methods, both growing up within computer science during the same
period. But for complicated reasons, until recently there has been very
little interaction.


 By and large, AI has defined its notions of
inference in logical terms, based originally on automatic
theorem proving results. The last chapter has discussed IR's
probabilistic foundations and so one immediate axis of difference
between AI and IR is the distinction between primarily logical and
primarily probabilistic modes of inference. Nevertheless, some in IR
perceived early on the advantages that AI's KNOWLEDGE
REPRESENTATIONS [REF328] and
expert systems [Fox87] [<A
HREF="bibrefs.html#McCune85">McCune85] [<A
HREF="bibrefs.html#Fidel86">Fidel86] techniques offered.


Today, the
fields of AI and IR align much more closely. For example, both machine
learning and natural language processing have always been considered
central issues within AI. The next chapter discusses at length machine
learning techniques as they hve been applied to document corpora.
Section &sect;8.2 can only sketch another
large intersection, corpus-based linguistics, where natural language
issues and IR techniques also merge.


The advantages of applying AI
knowledge representation techniques become especially obvious when
additional structured attributes are associated with documents,
keywords, and authors. Early on, Kochen [<A
HREF="bibrefs.html#REF320">REF320] considered a broad range of these
forms of information as shown in Figure (figure) Even more
inclusive lists have been proposed [<A
HREF="bibrefs.html#Katzer82">Katzer82] [<A
HREF="bibrefs.html#REF436">REF436]


 This shows the primary
Index relation in the larger context of other information we
might also have available. What all of these additional forms of
information have in common is their ability to shed new light on the
semantic questions of what the documents are about. Information about
the publication details of documents, for example the journal date, even
page numbers of documents, can help provide a context within which
individual documents can be better understood. Much of this
data-about-data document is now referred to as META-DATA . This
additional modeling of document structure in languages like XML and
codified in standards like the <A
HREF="http://www-diglib.stanford.edu/diglib/pub/dublin.html">Dublin
Core are one of the most important ways in which database and IR
technologies now interact. This constructively blurs many of the
database/search engine differences mentioned before (cf. Section <A
HREF="foa-1-6.html">&sect;1.6 ). Techniques for performing FACT
EXTRACTION -- building database relations from analysis of textual
WWW pages [Craven98] -- suggest a
broad range of new ways that structured attributes may enter into the
retrieval task.


Section &sect;6.1 discusses
one of the most important ways in which documents can be understood
independent of their keywords. In science, in the common law tradition,
and much more recently in email newsgroups and now with HTML hyperlinks,
the ability to link one document to another can provide vital
information about how the arguments of one document relate to those
contained in another.


Section &sect;6.3 will
discuss some of the special representation techniques which have been
used to organize keywords in the vocabulary. It is also possible to
reason about authors of documents. Section <A
HREF="foa-6-4-1.html">&sect;6.4.1 discusses experiments exploiting
Ph.D. ``genealogies'' in which dissertation authors are related to one
another by shared advisors. Co-authorship and membership in the same
research institution have also been proposed as ways to provide context
on a particular author's words. In some cases characterizations of
expertise of the authors, and independent of the documents themselves,
are available.


The chapter concludes with several suggestions of just how
these varied information sources can become integrated as part of
next-generation FOA tools. Section &sect;6.5
considers several ``modes of inference'' by which new conclusion about
keywords and documents can be reached from elementary facts. Section <A
HREF="foa-6-6.html">&sect;6.6 suggests a few of the new interface
techniques that become available as richer data streams are provided by
and presented back to a user. Sections <A
HREF="foa-6-7.html">&sect;6.7 and <A
HREF="foa-6-8.html">&sect;6.8 look at two domains of discourse in
particular -- the law and science surrounding molecular genetics -- as
examples of how such techniques can be marshalled towards particular FOA
purposes. After considering all these ways that the methods of AI can be
used to help with FOA, Section &sect;6.9
concludes with a speculation of how the problem of intelligence itself
might be changed as we take seriously the prospect of basing it on
textual representations.

Subsections

	 6.1 Citation: inter-document links
	 6.2 Hypertext, intra-document links
	 6.3 Keyword structures
	 6.4 Social relations among authors
	 6.5 Modes of inference
	 6.6 Deep interfaces
	 6.7 FOA(The Law)
	 6.8 FOA(Evolution)
	 6.9 Text-based intelligence




Top of Page

 | UP: foa-book.tex

 | ,FOA Home 





FOA &copy; R. K. Belew - 00-09-21



